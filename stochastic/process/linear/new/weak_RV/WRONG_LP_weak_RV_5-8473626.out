
The following have been reloaded with a version change:
  1) mii/1.1.1 => mii/1.1.2

/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Compiler/intel2020/openmpi/4.0.3/bin/mpiexec
/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Compiler/intel2020/openmpi/4.0.3/bin/mpirun
Number of Vertices for coarse grid  90601
Number of Vertices for fine mesh is  361201
1	
	-111111	
number of random variable  5
order of input  2
order of output  3
Number of input PC   21
Number of output PC  56
Mean of Gaussian  0
Sd of Gaussian  0.3
Size of linear System  2.02273e+07
sindex is 25 2	
	   1   1
	   1   2
	   2   1
	   2   2
	   1   3
	   3   1
	   2   3
	   3   2
	   1   4
	   4   1
	   3   3
	   2   4
	   4   2
	   3   4
	   4   3
	   1   5
	   5   1
	   2   5
	   5   2
	   4   4
	   3   5
	   5   3
	   1   6
	   6   1
	   2   6
	
mindex for file ./kledata/multiIndex00030005.dat is 56 5	
	   0   0   0   0   0
	   1   0   0   0   0
	   0   1   0   0   0
	   0   0   1   0   0
	   0   0   0   1   0
	   0   0   0   0   1
	   2   0   0   0   0
	   1   1   0   0   0
	   1   0   1   0   0
	   1   0   0   1   0
	   1   0   0   0   1
	   0   2   0   0   0
	   0   1   1   0   0
	   0   1   0   1   0
	   0   1   0   0   1
	   0   0   2   0   0
	   0   0   1   1   0
	   0   0   1   0   1
	   0   0   0   2   0
	   0   0   0   1   1
	   0   0   0   0   2
	   3   0   0   0   0
	   2   1   0   0   0
	   2   0   1   0   0
	   2   0   0   1   0
	   2   0   0   0   1
	   1   2   0   0   0
	   1   1   1   0   0
	   1   1   0   1   0
	   1   1   0   0   1
	   1   0   2   0   0
	   1   0   1   1   0
	   1   0   1   0   1
	   1   0   0   2   0
	   1   0   0   1   1
	   1   0   0   0   2
	   0   3   0   0   0
	   0   2   1   0   0
	   0   2   0   1   0
	   0   2   0   0   1
	   0   1   2   0   0
	   0   1   1   1   0
	   0   1   1   0   1
	   0   1   0   2   0
	   0   1   0   1   1
	   0   1   0   0   2
	   0   0   3   0   0
	   0   0   2   1   0
	   0   0   2   0   1
	   0   0   1   2   0
	   0   0   1   1   1
	   0   0   1   0   2
	   0   0   0   3   0
	   0   0   0   2   1
	   0   0   0   1   2
	   0   0   0   0   3
	
Finished generating log normal input coefficients for rank 0
Assembling matrices for level 0 and rank 0
Assembling matrices for level 1 and rank 0
Exited Assembling 0
Starting solve for 0
  0 KSP Residual norm 1.663888888889e-03 
      Residual norms for mg_coarse_ solve.
      0 KSP Residual norm 3.395839444915e+00 
      1 KSP Residual norm 2.020675294419e+00 
      2 KSP Residual norm 4.518699980128e-01 
      3 KSP Residual norm 8.318978938968e-02 
      4 KSP Residual norm 1.706635116825e-02 
      5 KSP Residual norm 3.424539061140e-03 
      6 KSP Residual norm 5.850880436422e-04 
      7 KSP Residual norm 1.025674886409e-04 
      8 KSP Residual norm 1.755528804339e-05 
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 8
  1 KSP Residual norm 9.387967647279e-04 
      Residual norms for mg_coarse_ solve.
      0 KSP Residual norm 2.873404052383e-01 
      1 KSP Residual norm 3.456175962227e-03 
      2 KSP Residual norm 6.874337976008e-04 
      3 KSP Residual norm 1.128816463294e-04 
      4 KSP Residual norm 2.267356341554e-05 
      5 KSP Residual norm 4.719089139237e-06 
      6 KSP Residual norm 8.239115292894e-07 
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 6
  2 KSP Residual norm 9.748626865314e-05 
      Residual norms for mg_coarse_ solve.
      0 KSP Residual norm 2.943567978655e-01 
      1 KSP Residual norm 3.056443302935e-03 
      2 KSP Residual norm 3.712473216374e-04 
      3 KSP Residual norm 6.944501840924e-05 
      4 KSP Residual norm 1.350278848042e-05 
      5 KSP Residual norm 2.737080155457e-06 
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 5
  3 KSP Residual norm 6.249507486885e-06 
      Residual norms for mg_coarse_ solve.
      0 KSP Residual norm 3.915694660117e-01 
      1 KSP Residual norm 9.612323549511e-03 
      2 KSP Residual norm 3.484924462913e-04 
      3 KSP Residual norm 4.876959654182e-05 
      4 KSP Residual norm 9.765386789546e-06 
      5 KSP Residual norm 1.939355173139e-06 
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 5
  4 KSP Residual norm 1.318209531399e-06 
      Residual norms for mg_coarse_ solve.
      0 KSP Residual norm 4.057970891217e-01 
      1 KSP Residual norm 1.836946703007e-02 
      2 KSP Residual norm 1.955496812690e-03 
      3 KSP Residual norm 3.382678123005e-04 
      4 KSP Residual norm 7.049742779628e-05 
      5 KSP Residual norm 1.421978213310e-05 
      6 KSP Residual norm 2.460023569495e-06 
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 6
  5 KSP Residual norm 1.892414420885e-07 
      Residual norms for mg_coarse_ solve.
      0 KSP Residual norm 3.387297631664e-01 
      1 KSP Residual norm 2.146995213621e-02 
      2 KSP Residual norm 2.443999671884e-03 
      3 KSP Residual norm 3.988133225767e-04 
      4 KSP Residual norm 8.127805706828e-05 
      5 KSP Residual norm 1.713879999914e-05 
      6 KSP Residual norm 3.030296369930e-06 
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 6
  6 KSP Residual norm 3.914334246245e-08 
      Residual norms for mg_coarse_ solve.
      0 KSP Residual norm 4.273596295492e-01 
      1 KSP Residual norm 2.766600347371e-02 
      2 KSP Residual norm 6.158706023355e-04 
      3 KSP Residual norm 1.426288253135e-04 
      4 KSP Residual norm 2.477454867264e-05 
      5 KSP Residual norm 4.987574243034e-06 
      6 KSP Residual norm 9.995194004025e-07 
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 6
  7 KSP Residual norm 4.542179334497e-09 
time taken for solve is44.8863
************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

/home/a/asarkar/sudhipv/software/FreeFem-sources/src/mpi/FreeFem++-mpi on a  named nia0012.scinet.local with 800 processors, by sudhipv Wed Nov 23 17:26:08 2022
Using Petsc Release Version 3.15.0, Mar 30, 2021 

                         Max       Max/Min     Avg       Total
Time (sec):           4.518e+01     1.000   4.518e+01
Objects:              1.440e+02     1.000   1.440e+02
Flop:                 5.757e+12     1.910   4.374e+12  3.499e+15
Flop/sec:             1.274e+11     1.910   9.682e+10  7.746e+13
MPI Messages:         1.560e+03     8.000   1.113e+03  8.905e+05
MPI Message Lengths:  3.883e+07     3.036   2.858e+04  2.545e+10
MPI Reductions:       1.970e+02     1.000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flop
                            and VecAXPY() for complex vectors of length N --> 8N flop

Summary of Stages:   ----- Time ------  ----- Flop ------  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total    Count   %Total     Avg         %Total    Count   %Total
 0:      Main Stage: 4.5176e+01 100.0%  3.4992e+15 100.0%  8.905e+05 100.0%  2.858e+04      100.0%  1.790e+02  90.9%

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flop: Max - maximum over all processors
                  Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   AvgLen: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flop in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flop over all processors)/(max time over all processors)
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flop                              --- Global ---  --- Stage ----  Total
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   AvgLen  Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s
------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

BuildTwoSided          6 1.0 1.0734e+0092.4 0.00e+00 0.0 1.4e+04 4.0e+00 5.0e+00  1  0  2  0  3   1  0  2  0  3     0
BuildTwoSidedF         2 1.0 4.6996e-0316.6 0.00e+00 0.0 0.0e+00 0.0e+00 2.0e+00  0  0  0  0  1   0  0  0  0  1     0
MatMult               94 1.0 2.7399e+00 1.2 1.27e+09 1.4 4.5e+05 5.6e+03 2.0e+00  6  0 50 10  1   6  0 50 10  1 320297
MatMultAdd             7 1.0 4.4687e-02 8.9 4.00e+05 1.3 3.2e+04 1.3e+04 0.0e+00  0  0  4  2  0   0  0  4  2  0  6337
MatMultTranspose       7 1.0 1.5730e-01 3.9 0.00e+00 0.0 3.2e+04 4.2e+04 0.0e+00  0  0  4  5  0   0  0  4  5  0     0
MatSolve              39 1.0 3.4882e+00 1.3 5.76e+12 1.9 0.0e+00 0.0e+00 0.0e+00  7100  0  0  0   7100  0  0  0 1002894461
MatLUFactorSym         1 1.0 6.0291e-01 1.9 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0
MatLUFactorNum         1 1.0 3.3697e+00 1.4 9.39e+07 1.6 0.0e+00 0.0e+00 0.0e+00  6  0  0  0  0   6  0  0  0  0 17701
MatConvert             1 1.0 1.2709e-01 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 4.0e+00  0  0  0  0  2   0  0  0  0  2     0
MatResidual            7 1.0 3.4329e-01 1.3 1.57e+08 1.3 3.2e+04 6.9e+03 0.0e+00  1  0  4  1  0   1  0  4  1  0 322752
MatAssemblyBegin       6 1.0 4.8325e-03 2.2 0.00e+00 0.0 0.0e+00 0.0e+00 2.0e+00  0  0  0  0  1   0  0  0  0  1     0
MatAssemblyEnd         6 1.0 4.6917e+0024.1 0.00e+00 0.0 0.0e+00 0.0e+00 1.2e+01  4  0  0  0  6   4  0  0  0  7     0
MatGetRowIJ            3 1.0 9.5389e-03 1.8 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatCreateSubMats       1 1.0 6.5273e-01 1.7 0.00e+00 0.0 2.3e+04 8.1e+05 1.0e+00  1  0  3 73  1   1  0  3 73  1     0
MatGetOrdering         1 1.0 2.4440e-02 2.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatIncreaseOvrlp       1 1.0 5.0003e-02 2.2 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  1   0  0  0  0  1     0
KSPSetUp               3 1.0 6.2677e+00 1.0 1.62e+12 1.9 1.9e+05 1.0e+05 3.8e+01 14 28 21 77 19  14 28 21 77 21 157460785
KSPSolve               1 1.0 1.1757e+01 1.0 4.13e+12 1.9 6.8e+05 7.8e+03 1.1e+02 26 72 76 21 54  26 72 76 21 60 213688657
KSPGMRESOrthog        59 1.0 9.8072e-01 7.0 1.42e+07 1.4 0.0e+00 0.0e+00 5.9e+01  1  0  0  0 30   1  0  0  0 33  9951
PCSetUp                1 1.0 1.4790e+01 1.0 1.62e+12 1.9 1.9e+05 1.0e+05 4.2e+01 33 28 21 77 21  33 28 21 77 23 66728868
PCSetUpOnBlocks       15 1.0 3.8967e+00 1.4 9.39e+07 1.6 0.0e+00 0.0e+00 0.0e+00  7  0  0  0  0   7  0  0  0  0 15307
PCApply                7 1.0 1.1433e+01 1.0 4.13e+12 1.9 6.5e+05 7.8e+03 9.2e+01 25 72 73 20 47  25 72 73 20 51 219741464
PCApplyOnBlocks       39 1.0 3.4909e+00 1.3 5.76e+12 1.9 0.0e+00 0.0e+00 0.0e+00  7100  0  0  0   7100  0  0  0 1002130551
VecMDot               59 1.0 9.7767e-01 7.1 7.11e+06 1.4 0.0e+00 0.0e+00 5.9e+01  1  0  0  0 30   1  0  0  0 33  4991
VecNorm               68 1.0 1.7995e-01 3.9 1.86e+06 1.4 0.0e+00 0.0e+00 6.8e+01  0  0  0  0 35   0  0  0  0 38  7035
VecScale              82 1.0 7.5976e-04 2.6 9.30e+05 1.4 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0 833065
VecCopy               83 1.0 2.3915e-03 1.9 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecSet               204 1.0 8.9033e-03 1.8 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecAXPY               16 1.0 9.0005e-04 2.6 6.25e+05 1.4 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0 483443
VecAYPX               42 1.0 2.8588e-03 2.0 1.60e+06 1.3 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0 396218
VecAXPBYCZ            14 1.0 1.7129e-03 2.9 2.00e+06 1.3 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0 826609
VecMAXPY              68 1.0 3.7446e-03 1.5 8.74e+06 1.4 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0 1600642
VecScatterBegin      243 1.0 1.0877e+0035.6 0.00e+00 0.0 7.8e+05 5.8e+03 3.0e+00  1  0 88 18  2   1  0 88 18  2     0
VecScatterEnd        243 1.0 1.0649e+00186.4 2.36e+05 3.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0   144
VecNormalize          60 1.0 1.7951e-01 3.9 2.10e+06 1.5 0.0e+00 0.0e+00 6.0e+01  0  0  0  0 30   0  0  0  0 34  7873
SFSetGraph             4 1.0 5.2270e-05 1.9 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
SFSetUp                4 1.0 1.0730e+0086.4 0.00e+00 0.0 2.7e+04 1.4e+03 3.0e+00  1  0  3  0  2   1  0  3  0  2     0
SFPack               243 1.0 2.6487e-03 3.4 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
SFUnpack             243 1.0 1.6190e-03 5.6 2.36e+05 3.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0 94879
------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix    12             12    248919996     0.
       Krylov Solver     5              5        72264     0.
      Preconditioner     5              5         6024     0.
              Viewer     2              1          848     0.
              Vector    85             85     11317688     0.
           Index Set    15             15       772208     0.
   IS L to G Mapping     1              1       320552     0.
   Star Forest Graph    10             10        11376     0.
    Distributed Mesh     3              3        15144     0.
     Discrete System     3              3         2712     0.
           Weak Form     3              3         2472     0.
========================================================================================================================
Average time to get PetscTime(): 6.86e-08
Average time for MPI_Barrier(): 7.97294e-05
Average time for zero size MPI_Send(): 1.27598e-06
#PETSc Option Table entries:
-ksp_max_it 200
-ksp_monitor
-ksp_pc_side right
-ksp_type fgmres
-log_view
-m 300
-mg_coarse_ksp_converged_reason
-mg_coarse_ksp_monitor
-mg_coarse_ksp_pc_side right
-mg_coarse_ksp_type gmres
-mg_coarse_pc_type hypre
-mg_levels_pc_asm_type basic
-mg_levels_pc_type asm
-mg_levels_sub_pc_factor_mat_solver_type mumps
-mg_levels_sub_pc_type lu
-n 300
-nw
-pc_mg_type multiplicative
-pc_type mg
#End of PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: --prefix=/home/a/asarkar/sudhipv/software/FreeFem-sources/install/ff-petsc/r MAKEFLAGS= --with-debugging=0 COPTFLAGS="-O3 -mtune=native" CXXOPTFLAGS="-O3 -mtune=native" FOPTFLAGS="-O3 -mtune=native" --with-cxx-dialect=C++11 --with-ssl=0 --with-x=0 --with-fortran-bindings=0 --with-cudac=0 --with-cc=/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Compiler/intel2020/openmpi/4.0.3/bin/mpicc --with-cxx=/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Compiler/intel2020/openmpi/4.0.3/bin/mpic++ --with-fc=/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Compiler/intel2020/openmpi/4.0.3/bin/mpif90 --with-scalar-type=real --with-blaslapack-include=/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/imkl/2020.1.217/mkl/include --with-blaslapack-lib="-Wl,-rpath,/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/imkl/2020.1.217/mkl/lib/intel64 -L/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/imkl/2020.1.217/mkl/lib/intel64 -lmkl_rt -lmkl_intel_thread -lmkl_core  -liomp5 -lpthread" --download-metis --download-ptscotch --download-hypre --download-parmetis --download-mmg --download-parmmg --download-superlu --download-suitesparse --download-tetgen --download-slepc --download-hpddm --download-scalapack --download-mumps --download-slepc-configure-arguments=--download-arpack=https://github.com/prj-/arpack-ng/archive/b64dccb.tar.gz PETSC_ARCH=fr
-----------------------------------------
Libraries compiled on 2021-05-02 15:39:49 on nia-login03.scinet.local 
Machine characteristics: Linux-3.10.0-957.27.2.el7.x86_64-x86_64-with-centos-7.6.1810-Core
Using PETSc directory: /home/a/asarkar/sudhipv/software/FreeFem-sources/install/ff-petsc/r
Using PETSc arch: 
-----------------------------------------

Using C compiler: /cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Compiler/intel2020/openmpi/4.0.3/bin/mpicc  -fPIC -wd1572 -O3 -mtune=native   
Using Fortran compiler: /cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Compiler/intel2020/openmpi/4.0.3/bin/mpif90  -fPIC -O3 -mtune=native     
-----------------------------------------

Using include paths: -I/home/a/asarkar/sudhipv/software/FreeFem-sources/install/ff-petsc/r/include -I/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/imkl/2020.1.217/mkl/include
-----------------------------------------

Using C linker: /cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Compiler/intel2020/openmpi/4.0.3/bin/mpicc
Using Fortran linker: /cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Compiler/intel2020/openmpi/4.0.3/bin/mpif90
Using libraries: -Wl,-rpath,/home/a/asarkar/sudhipv/software/FreeFem-sources/install/ff-petsc/r/lib -L/home/a/asarkar/sudhipv/software/FreeFem-sources/install/ff-petsc/r/lib -lpetsc -Wl,-rpath,/home/a/asarkar/sudhipv/software/FreeFem-sources/install/ff-petsc/r/lib -L/home/a/asarkar/sudhipv/software/FreeFem-sources/install/ff-petsc/r/lib -Wl,-rpath,/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/imkl/2020.1.217/mkl/lib/intel64 -L/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/imkl/2020.1.217/mkl/lib/intel64 -Wl,-rpath,/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Compiler/intel2020/openmpi/4.0.3/lib -L/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Compiler/intel2020/openmpi/4.0.3/lib -Wl,-rpath,/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/libfabric/1.10.1/lib -L/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/libfabric/1.10.1/lib -Wl,-rpath,/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/ucx/1.8.0/lib -L/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/ucx/1.8.0/lib -Wl,-rpath,/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/imkl/2020.1.217/lib/intel64 -L/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/imkl/2020.1.217/lib/intel64 -Wl,-rpath,/cvmfs/restricted.computecanada.ca/easybuild/software/2020/Core/intel/2020.1.217/compilers_and_libraries_2020.1.217/linux/tbb/lib/intel64/gcc4.8 -L/cvmfs/restricted.computecanada.ca/easybuild/software/2020/Core/intel/2020.1.217/compilers_and_libraries_2020.1.217/linux/tbb/lib/intel64/gcc4.8 -Wl,-rpath,/cvmfs/restricted.computecanada.ca/easybuild/software/2020/Core/intel/2020.1.217/compilers_and_libraries_2020.1.217/linux/compiler/lib/intel64_lin -L/cvmfs/restricted.computecanada.ca/easybuild/software/2020/Core/intel/2020.1.217/compilers_and_libraries_2020.1.217/linux/compiler/lib/intel64_lin -Wl,-rpath,/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Compiler/intel2020/openmpi/4.0.3/lib64 -L/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Compiler/intel2020/openmpi/4.0.3/lib64 -Wl,-rpath,/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/gcccore/9.3.0/lib/gcc/x86_64-pc-linux-gnu/9.3.0 -L/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/gcccore/9.3.0/lib/gcc/x86_64-pc-linux-gnu/9.3.0 -Wl,-rpath,/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/gcccore/9.3.0/lib64 -L/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/gcccore/9.3.0/lib64 -Wl,-rpath,/cvmfs/soft.computecanada.ca/gentoo/2020/lib64 -L/cvmfs/soft.computecanada.ca/gentoo/2020/lib64 -Wl,-rpath,/cvmfs/soft.computecanada.ca/gentoo/2020/usr/lib64 -L/cvmfs/soft.computecanada.ca/gentoo/2020/usr/lib64 -Wl,-rpath,/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/gcccore/9.3.0/lib -L/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/gcccore/9.3.0/lib -Wl,-rpath,/cvmfs/soft.computecanada.ca/gentoo/2020/lib -L/cvmfs/soft.computecanada.ca/gentoo/2020/lib -Wl,-rpath,/cvmfs/soft.computecanada.ca/gentoo/2020/usr/lib -L/cvmfs/soft.computecanada.ca/gentoo/2020/usr/lib -lHYPRE -lcmumps -ldmumps -lsmumps -lzmumps -lmumps_common -lpord -lscalapack -lumfpack -lklu -lcholmod -lbtf -lccolamd -lcolamd -lcamd -lamd -lsuitesparseconfig -lsuperlu -lmkl_rt -lmkl_intel_thread -lmkl_core -liomp5 -lpthread -lparmmg -lmmg -lmmg3d -lptesmumps -lptscotchparmetis -lptscotch -lptscotcherr -lesmumps -lscotch -lscotcherr -lpthread -lparmetis -lmetis -ltet -lstdc++ -ldl -lmpi_usempif08 -lmpi_usempi_ignore_tkr -lmpi_mpifh -lmpi -lifport -lifcoremt_pic -limf -lsvml -lm -lipgo -lirc -lpthread -lgcc_s -lirc_s -lrt -lquadmath -lstdc++ -ldl
-----------------------------------------


scontrol show jobid 8473626
JobId=8473626 JobName=LP_weak_RV_5
   UserId=sudhipv(3061414) GroupId=asarkar(6001189) MCS_label=N/A
   Priority=1740108 Nice=0 Account=def-asarkar QOS=normal
   JobState=COMPLETED Reason=None Dependency=(null)
   Requeue=0 Restarts=0 BatchFlag=1 Reboot=0 ExitCode=0:0
   RunTime=00:01:09 TimeLimit=00:30:00 TimeMin=N/A
   SubmitTime=2022-11-23T17:24:52 EligibleTime=2022-11-23T17:24:52
   AccrueTime=2022-11-23T17:24:52
   StartTime=2022-11-23T17:25:00 EndTime=2022-11-23T17:26:09 Deadline=N/A
   SuspendTime=None SecsPreSuspend=0 LastSchedEval=2022-11-23T17:25:00 Scheduler=Main
   Partition=compute AllocNode:Sid=nia-login03:98134
   ReqNodeList=(null) ExcNodeList=(null)
   NodeList=nia[0012-0013,0065,0152,0176,0601-0602,0610-0611,0641-0642,0750-0752,0763-0764,0783-0784,0836-0837]
   BatchHost=nia0012
   NumNodes=20 NumCPUs=1600 NumTasks=800 CPUs/Task=1 ReqB:S:C:T=0:0:*:*
   TRES=cpu=1600,mem=3500000M,node=20,billing=800
   Socks/Node=* NtasksPerN:B:S:C=40:0:*:* CoreSpec=*
   MinCPUsNode=40 MinMemoryNode=175000M MinTmpDiskNode=0
   Features=[skylake|cascade] DelayBoot=00:00:00
   OverSubscribe=NO Contiguous=0 Licenses=(null) Network=(null)
   Command=/gpfs/fs0/scratch/a/asarkar/sudhipv/poisson_hpc/stochastic/process_L/poisson_weak_sto/runff_niagara.sh
   WorkDir=/gpfs/fs0/scratch/a/asarkar/sudhipv/poisson_hpc/stochastic/process_L/poisson_weak_sto
   Comment=/opt/slurm/bin/sbatch --export=NONE runff_niagara.sh 
   StdErr=/gpfs/fs0/scratch/a/asarkar/sudhipv/poisson_hpc/stochastic/process_L/poisson_weak_sto/LP_weak_RV_5-8473626.out
   StdIn=/dev/null
   StdOut=/gpfs/fs0/scratch/a/asarkar/sudhipv/poisson_hpc/stochastic/process_L/poisson_weak_sto/LP_weak_RV_5-8473626.out
   Power=
   MailUser=sudhipv@cmail.carleton.ca MailType=INVALID_DEPEND,BEGIN,END,FAIL,REQUEUE,STAGE_OUT
   

sacct -j 8473626
JobID           JobName    Account    Elapsed  MaxVMSize     MaxRSS  SystemCPU    UserCPU ExitCode 
------------ ---------- ---------- ---------- ---------- ---------- ---------- ---------- -------- 
8473626      LP_weak_R+ def-asark+   00:01:09                        24:40.427   09:38:00      0:0 
8473626.bat+      batch def-asark+   00:01:09  76894284K  49028252K  01:12.616  28:54.427      0:0 
8473626.ext+     extern def-asark+   00:01:12    142384K      1132K  00:00.010  00:00.006      0:0 
8473626.0         orted def-asark+   00:01:04  76768980K  50436936K  23:27.801   09:09:06      0:0 
