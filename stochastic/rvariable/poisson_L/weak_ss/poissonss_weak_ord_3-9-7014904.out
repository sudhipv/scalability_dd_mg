
Due to MODULEPATH changes, the following have been reloaded:
  1) mii/1.1.1

/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Compiler/intel2020/openmpi/4.0.3/bin/mpiexec
/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Compiler/intel2020/openmpi/4.0.3/bin/mpirun
Number of Vertices for coarse grid  4004001
Number of Vertices for fine mesh in  1.6008e+07
2	
	-111111	  4	
order of input  2
order of output  3
Mean of Gaussian  0
Sd of Gaussian  0.3
Size of linear System  6.4032e+07
6	
	1.04602786	0.313808358	0.06656880537	0.01153005531	0.001729508297
	0.0002320378871	
  0 KSP Residual norm 2.499375000000e-04 
      Residual norms for mg_coarse_ solve.
      0 KSP Residual norm 2.397727961622e+05 
      1 KSP Residual norm 2.506887577552e+04 
      2 KSP Residual norm 3.817817696612e+03 
      3 KSP Residual norm 6.783468472956e+02 
      4 KSP Residual norm 8.053176666076e+01 
      5 KSP Residual norm 1.639877807195e+01 
      6 KSP Residual norm 2.387558774300e+00 
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 6
  1 KSP Residual norm 2.477485372960e-04 
      Residual norms for mg_coarse_ solve.
      0 KSP Residual norm 3.328388649035e+00 
      1 KSP Residual norm 3.418726700842e-01 
      2 KSP Residual norm 5.367310113109e-02 
      3 KSP Residual norm 9.906933943859e-03 
      4 KSP Residual norm 1.856371699365e-03 
      5 KSP Residual norm 3.505141714544e-04 
      6 KSP Residual norm 6.122219081719e-05 
      7 KSP Residual norm 9.931447163783e-06 
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 7
  2 KSP Residual norm 3.936123506612e-05 
      Residual norms for mg_coarse_ solve.
      0 KSP Residual norm 8.022718831976e+00 
      1 KSP Residual norm 1.349771366037e+00 
      2 KSP Residual norm 2.513607491430e-01 
      3 KSP Residual norm 6.273859990819e-02 
      4 KSP Residual norm 1.289686110212e-02 
      5 KSP Residual norm 2.692010290145e-03 
      6 KSP Residual norm 4.013834603605e-04 
      7 KSP Residual norm 6.331574394681e-05 
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 7
  3 KSP Residual norm 2.308378111773e-06 
      Residual norms for mg_coarse_ solve.
      0 KSP Residual norm 8.185429712347e+01 
      1 KSP Residual norm 1.078415855489e+01 
      2 KSP Residual norm 1.919381751843e+00 
      3 KSP Residual norm 4.873381353534e-01 
      4 KSP Residual norm 6.271316376155e-02 
      5 KSP Residual norm 1.206084287182e-02 
      6 KSP Residual norm 2.464702911939e-03 
      7 KSP Residual norm 3.778630024058e-04 
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 7
  4 KSP Residual norm 4.109826519301e-07 
      Residual norms for mg_coarse_ solve.
      0 KSP Residual norm 1.561309418535e+01 
      1 KSP Residual norm 3.403734200147e+00 
      2 KSP Residual norm 7.694418725996e-01 
      3 KSP Residual norm 1.498665490438e-01 
      4 KSP Residual norm 3.382203809231e-02 
      5 KSP Residual norm 7.962986219284e-03 
      6 KSP Residual norm 1.596985087459e-03 
      7 KSP Residual norm 3.447392752490e-04 
      8 KSP Residual norm 2.669556225122e-05 
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 8
  5 KSP Residual norm 1.026523892086e-07 
      Residual norms for mg_coarse_ solve.
      0 KSP Residual norm 3.400439128282e+01 
      1 KSP Residual norm 3.812371962241e+00 
      2 KSP Residual norm 6.282860993228e-01 
      3 KSP Residual norm 1.521004610280e-01 
      4 KSP Residual norm 1.829880991722e-02 
      5 KSP Residual norm 2.913819058836e-03 
      6 KSP Residual norm 5.960371880891e-04 
      7 KSP Residual norm 8.281624925056e-05 
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 7
  6 KSP Residual norm 2.650269564301e-08 
      Residual norms for mg_coarse_ solve.
      0 KSP Residual norm 1.874956236885e+01 
      1 KSP Residual norm 4.235791024097e+00 
      2 KSP Residual norm 8.279637560943e-01 
      3 KSP Residual norm 2.048302582870e-01 
      4 KSP Residual norm 4.911617860377e-02 
      5 KSP Residual norm 1.069729993117e-02 
      6 KSP Residual norm 2.008021992039e-03 
      7 KSP Residual norm 3.818566879109e-04 
      8 KSP Residual norm 2.948073067494e-05 
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 8
  7 KSP Residual norm 5.262410477332e-09 
      Residual norms for mg_coarse_ solve.
      0 KSP Residual norm 2.803789858673e+01 
      1 KSP Residual norm 3.227222605485e+00 
      2 KSP Residual norm 6.574784892585e-01 
      3 KSP Residual norm 1.333126307485e-01 
      4 KSP Residual norm 1.376960409248e-02 
      5 KSP Residual norm 2.216588274124e-03 
      6 KSP Residual norm 6.406809355570e-04 
      7 KSP Residual norm 1.117560149248e-04 
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 7
  8 KSP Residual norm 6.071313921136e-10 
time taken for solve is64.8633
************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

/home/a/asarkar/sudhipv/software/FreeFem-sources/src/mpi/FreeFem++-mpi on a  named nia0032.scinet.local with 160 processors, by sudhipv Sat Mar  5 06:23:12 2022
Using Petsc Release Version 3.15.0, Mar 30, 2021 

                         Max       Max/Min     Avg       Total
Time (sec):           6.509e+01     1.000   6.509e+01
Objects:              1.450e+02     1.000   1.450e+02
Flop:                 6.812e+12     1.350   6.166e+12  9.866e+14
Flop/sec:             1.047e+11     1.350   9.473e+10  1.516e+13
MPI Messages:         1.638e+03     4.500   9.669e+02  1.547e+05
MPI Message Lengths:  1.390e+07     3.395   1.045e+04  1.616e+09
MPI Reductions:       2.300e+02     1.000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flop
                            and VecAXPY() for complex vectors of length N --> 8N flop

Summary of Stages:   ----- Time ------  ----- Flop ------  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total    Count   %Total     Avg         %Total    Count   %Total
 0:      Main Stage: 6.5088e+01 100.0%  9.8656e+14 100.0%  1.547e+05 100.0%  1.045e+04      100.0%  2.120e+02  92.2%

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flop: Max - maximum over all processors
                  Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   AvgLen: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flop in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flop over all processors)/(max time over all processors)
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flop                              --- Global ---  --- Stage ----  Total
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   AvgLen  Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s
------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

BuildTwoSided          7 1.0 8.6899e-01547.5 0.00e+00 0.0 2.6e+03 4.0e+00 5.0e+00  1  0  2  0  2   1  0  2  0  2     0
BuildTwoSidedF         2 1.0 3.3374e-0319.4 0.00e+00 0.0 0.0e+00 0.0e+00 2.0e+00  0  0  0  0  1   0  0  0  0  1     0
MatMult              115 1.0 3.0524e+00 1.3 1.43e+09 1.0 1.0e+05 6.2e+03 2.0e+00  4  0 65 39  1   4  0 65 39  1 74111
MatMultAdd             8 1.0 1.1129e-01 1.2 6.46e+06 1.0 6.8e+03 1.2e+04 0.0e+00  0  0  4  5  0   0  0  4  5  0  9206
MatMultTranspose       8 1.0 2.9616e-01 3.7 0.00e+00 0.0 6.8e+03 4.1e+04 0.0e+00  0  0  4 17  0   0  0  4 17  0     0
MatSolve              43 1.0 1.2802e+01 1.1 6.81e+12 1.4 0.0e+00 0.0e+00 0.0e+00 19100  0  0  0  19100  0  0  0 77035888
MatLUFactorSym         1 1.0 1.6854e+00 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  3  0  0  0  0   3  0  0  0  0     0
MatLUFactorNum         1 1.0 4.7482e+00 1.2 2.37e+08 1.1 0.0e+00 0.0e+00 0.0e+00  7  0  0  0  0   7  0  0  0  0  7652
MatConvert             1 1.0 5.6930e-02 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 4.0e+00  0  0  0  0  2   0  0  0  0  2     0
MatResidual            8 1.0 4.5088e-01 2.1 1.81e+08 1.0 6.8e+03 8.3e+03 0.0e+00  1  0  4  3  0   1  0  4  3  0 63606
MatAssemblyBegin       6 1.0 3.4933e-03 5.1 0.00e+00 0.0 0.0e+00 0.0e+00 2.0e+00  0  0  0  0  1   0  0  0  0  1     0
MatAssemblyEnd         6 1.0 2.1496e+00 7.3 0.00e+00 0.0 0.0e+00 0.0e+00 1.2e+01  2  0  0  0  5   2  0  0  0  6     0
MatGetRowIJ            3 1.0 2.2513e-02 2.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatCreateSubMats       1 1.0 5.9430e-01 1.4 0.00e+00 0.0 4.2e+03 7.1e+04 1.0e+00  1  0  3 19  0   1  0  3 19  0     0
MatGetOrdering         1 1.0 1.1841e-01 2.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatIncreaseOvrlp       1 1.0 1.0380e-01 1.6 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0
KSPSetUp               3 1.0 1.1481e+01 1.0 1.74e+12 1.4 2.6e+04 1.8e+04 3.8e+01 18 26 16 28 17  18 26 16 28 18 21983050
KSPSolve               1 1.0 2.2340e+01 1.0 5.07e+12 1.4 1.2e+05 8.4e+03 1.4e+02 34 74 81 65 61  34 74 81 65 66 32863638
KSPGMRESOrthog        75 1.0 1.1788e+00 2.4 2.42e+08 1.0 0.0e+00 0.0e+00 7.5e+01  1  0  0  0 33   1  0  0  0 35 32436
PCSetUp                1 1.0 1.2451e+01 1.0 1.74e+12 1.4 2.6e+04 1.8e+04 4.2e+01 19 26 16 28 18  19 26 16 28 20 20270334
PCSetUpOnBlocks       17 1.0 6.5522e+00 1.1 2.37e+08 1.1 0.0e+00 0.0e+00 0.0e+00 10  0  0  0  0  10  0  0  0  0  5545
PCApply                8 1.0 2.1975e+01 1.0 5.07e+12 1.4 1.2e+05 8.5e+03 1.2e+02 33 74 76 62 53  33 74 76 62 58 33408073
PCApplyOnBlocks       43 1.0 1.2834e+01 1.1 6.81e+12 1.4 0.0e+00 0.0e+00 0.0e+00 19100  0  0  0  19100  0  0  0 76843338
VecMDot               75 1.0 1.0403e+00 3.0 1.21e+08 1.0 0.0e+00 0.0e+00 7.5e+01  1  0  0  0 33   1  0  0  0 35 18377
VecNorm               85 1.0 8.4129e-02 2.0 2.94e+07 1.0 0.0e+00 0.0e+00 8.5e+01  0  0  0  0 37   0  0  0  0 40 55194
VecScale             101 1.0 1.7376e-02 1.1 1.47e+07 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0 133616
VecCopy               77 1.0 1.0384e-01 1.4 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecSet               236 1.0 1.4454e-01 1.3 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecAXPY               18 1.0 2.0859e-02 1.5 9.70e+06 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0 73679
VecAYPX               48 1.0 6.7288e-02 1.4 2.58e+07 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0 60903
VecAXPBYCZ            16 1.0 4.1537e-02 2.0 3.23e+07 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0 123326
VecMAXPY              85 1.0 1.7379e-01 1.0 1.47e+08 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0 133774
VecScatterBegin      279 1.0 1.1081e+00 4.1 0.00e+00 0.0 1.3e+05 6.4e+03 3.0e+00  1  0 86 52  1   1  0 86 52  1     0
VecScatterEnd        279 1.0 5.7178e-0115.4 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecNormalize          76 1.0 8.6802e-02 2.0 3.32e+07 1.0 0.0e+00 0.0e+00 7.6e+01  0  0  0  0 33   0  0  0  0 36 60323
SFSetGraph             5 1.0 1.0170e-03 1.6 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
SFSetUp                5 1.0 8.9972e-0127.4 0.00e+00 0.0 5.1e+03 1.7e+03 3.0e+00  1  0  3  1  1   1  0  3  1  1     0
SFReduceBegin         43 1.0 6.8614e-02 1.9 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
SFReduceEnd           43 1.0 1.2302e-04 2.4 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
SFPack               279 1.0 6.4608e-03 3.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
SFUnpack             279 1.0 1.0484e-0312.4 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix    12             12    399512924     0.
       Krylov Solver     5              5        72264     0.
      Preconditioner     5              5         6024     0.
              Viewer     2              1          848     0.
              Vector    85             85    203508088     0.
           Index Set    15             15     10533504     0.
   IS L to G Mapping     1              1      4830536     0.
   Star Forest Graph    11             11        12600     0.
    Distributed Mesh     3              3        15144     0.
     Discrete System     3              3         2712     0.
           Weak Form     3              3         2472     0.
========================================================================================================================
Average time to get PetscTime(): 3.18e-08
Average time for MPI_Barrier(): 6.4768e-05
Average time for zero size MPI_Send(): 1.17337e-06
#PETSc Option Table entries:
-ksp_max_it 200
-ksp_monitor
-ksp_type fgmres
-log_view
-m 2000
-mg_coarse_ksp_converged_reason
-mg_coarse_ksp_monitor
-mg_coarse_ksp_type gmres
-mg_coarse_pc_type hypre
-mg_levels_pc_type asm
-mg_levels_sub_pc_factor_mat_solver_type mumps
-mg_levels_sub_pc_type lu
-n 2000
-nw
-ordo 3
-pc_mg_type multiplicative
-pc_type mg
#End of PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: --prefix=/home/a/asarkar/sudhipv/software/FreeFem-sources/install/ff-petsc/r MAKEFLAGS= --with-debugging=0 COPTFLAGS="-O3 -mtune=native" CXXOPTFLAGS="-O3 -mtune=native" FOPTFLAGS="-O3 -mtune=native" --with-cxx-dialect=C++11 --with-ssl=0 --with-x=0 --with-fortran-bindings=0 --with-cudac=0 --with-cc=/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Compiler/intel2020/openmpi/4.0.3/bin/mpicc --with-cxx=/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Compiler/intel2020/openmpi/4.0.3/bin/mpic++ --with-fc=/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Compiler/intel2020/openmpi/4.0.3/bin/mpif90 --with-scalar-type=real --with-blaslapack-include=/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/imkl/2020.1.217/mkl/include --with-blaslapack-lib="-Wl,-rpath,/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/imkl/2020.1.217/mkl/lib/intel64 -L/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/imkl/2020.1.217/mkl/lib/intel64 -lmkl_rt -lmkl_intel_thread -lmkl_core  -liomp5 -lpthread" --download-metis --download-ptscotch --download-hypre --download-parmetis --download-mmg --download-parmmg --download-superlu --download-suitesparse --download-tetgen --download-slepc --download-hpddm --download-scalapack --download-mumps --download-slepc-configure-arguments=--download-arpack=https://github.com/prj-/arpack-ng/archive/b64dccb.tar.gz PETSC_ARCH=fr
-----------------------------------------
Libraries compiled on 2021-05-02 15:39:49 on nia-login03.scinet.local 
Machine characteristics: Linux-3.10.0-957.27.2.el7.x86_64-x86_64-with-centos-7.6.1810-Core
Using PETSc directory: /home/a/asarkar/sudhipv/software/FreeFem-sources/install/ff-petsc/r
Using PETSc arch: 
-----------------------------------------

Using C compiler: /cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Compiler/intel2020/openmpi/4.0.3/bin/mpicc  -fPIC -wd1572 -O3 -mtune=native   
Using Fortran compiler: /cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Compiler/intel2020/openmpi/4.0.3/bin/mpif90  -fPIC -O3 -mtune=native     
-----------------------------------------

Using include paths: -I/home/a/asarkar/sudhipv/software/FreeFem-sources/install/ff-petsc/r/include -I/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/imkl/2020.1.217/mkl/include
-----------------------------------------

Using C linker: /cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Compiler/intel2020/openmpi/4.0.3/bin/mpicc
Using Fortran linker: /cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Compiler/intel2020/openmpi/4.0.3/bin/mpif90
Using libraries: -Wl,-rpath,/home/a/asarkar/sudhipv/software/FreeFem-sources/install/ff-petsc/r/lib -L/home/a/asarkar/sudhipv/software/FreeFem-sources/install/ff-petsc/r/lib -lpetsc -Wl,-rpath,/home/a/asarkar/sudhipv/software/FreeFem-sources/install/ff-petsc/r/lib -L/home/a/asarkar/sudhipv/software/FreeFem-sources/install/ff-petsc/r/lib -Wl,-rpath,/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/imkl/2020.1.217/mkl/lib/intel64 -L/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/imkl/2020.1.217/mkl/lib/intel64 -Wl,-rpath,/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Compiler/intel2020/openmpi/4.0.3/lib -L/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Compiler/intel2020/openmpi/4.0.3/lib -Wl,-rpath,/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/libfabric/1.10.1/lib -L/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/libfabric/1.10.1/lib -Wl,-rpath,/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/ucx/1.8.0/lib -L/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/ucx/1.8.0/lib -Wl,-rpath,/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/imkl/2020.1.217/lib/intel64 -L/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/imkl/2020.1.217/lib/intel64 -Wl,-rpath,/cvmfs/restricted.computecanada.ca/easybuild/software/2020/Core/intel/2020.1.217/compilers_and_libraries_2020.1.217/linux/tbb/lib/intel64/gcc4.8 -L/cvmfs/restricted.computecanada.ca/easybuild/software/2020/Core/intel/2020.1.217/compilers_and_libraries_2020.1.217/linux/tbb/lib/intel64/gcc4.8 -Wl,-rpath,/cvmfs/restricted.computecanada.ca/easybuild/software/2020/Core/intel/2020.1.217/compilers_and_libraries_2020.1.217/linux/compiler/lib/intel64_lin -L/cvmfs/restricted.computecanada.ca/easybuild/software/2020/Core/intel/2020.1.217/compilers_and_libraries_2020.1.217/linux/compiler/lib/intel64_lin -Wl,-rpath,/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Compiler/intel2020/openmpi/4.0.3/lib64 -L/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Compiler/intel2020/openmpi/4.0.3/lib64 -Wl,-rpath,/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/gcccore/9.3.0/lib/gcc/x86_64-pc-linux-gnu/9.3.0 -L/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/gcccore/9.3.0/lib/gcc/x86_64-pc-linux-gnu/9.3.0 -Wl,-rpath,/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/gcccore/9.3.0/lib64 -L/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/gcccore/9.3.0/lib64 -Wl,-rpath,/cvmfs/soft.computecanada.ca/gentoo/2020/lib64 -L/cvmfs/soft.computecanada.ca/gentoo/2020/lib64 -Wl,-rpath,/cvmfs/soft.computecanada.ca/gentoo/2020/usr/lib64 -L/cvmfs/soft.computecanada.ca/gentoo/2020/usr/lib64 -Wl,-rpath,/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/gcccore/9.3.0/lib -L/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/gcccore/9.3.0/lib -Wl,-rpath,/cvmfs/soft.computecanada.ca/gentoo/2020/lib -L/cvmfs/soft.computecanada.ca/gentoo/2020/lib -Wl,-rpath,/cvmfs/soft.computecanada.ca/gentoo/2020/usr/lib -L/cvmfs/soft.computecanada.ca/gentoo/2020/usr/lib -lHYPRE -lcmumps -ldmumps -lsmumps -lzmumps -lmumps_common -lpord -lscalapack -lumfpack -lklu -lcholmod -lbtf -lccolamd -lcolamd -lcamd -lamd -lsuitesparseconfig -lsuperlu -lmkl_rt -lmkl_intel_thread -lmkl_core -liomp5 -lpthread -lparmmg -lmmg -lmmg3d -lptesmumps -lptscotchparmetis -lptscotch -lptscotcherr -lesmumps -lscotch -lscotcherr -lpthread -lparmetis -lmetis -ltet -lstdc++ -ldl -lmpi_usempif08 -lmpi_usempi_ignore_tkr -lmpi_mpifh -lmpi -lifport -lifcoremt_pic -limf -lsvml -lm -lipgo -lirc -lpthread -lgcc_s -lirc_s -lrt -lquadmath -lstdc++ -ldl
-----------------------------------------

Number of Vertices for coarse grid  4004001
Number of Vertices for fine mesh in  1.6008e+07
2	
	-111111	  1	
order of input  2
order of output  5
Mean of Gaussian  0
Sd of Gaussian  0.3
Size of linear System  9.6048e+07
6	
	1.04602786	0.313808358	0.06656880537	0.01153005531	0.001729508297
	0.0002320378871	
  0 KSP Residual norm 2.499375000000e-04 
      Residual norms for mg_coarse_ solve.
      0 KSP Residual norm 2.293355156601e+05 
      1 KSP Residual norm 2.579384315919e+04 
      2 KSP Residual norm 3.792582497240e+03 
      3 KSP Residual norm 6.431608485478e+02 
      4 KSP Residual norm 9.733271852008e+01 
      5 KSP Residual norm 2.150423783355e+01 
      6 KSP Residual norm 4.726218858643e+00 
      7 KSP Residual norm 9.099505292615e-01 
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 7
  1 KSP Residual norm 2.473231904314e-04 
      Residual norms for mg_coarse_ solve.
      0 KSP Residual norm 5.050034255174e+00 
      1 KSP Residual norm 5.716083578538e-01 
      2 KSP Residual norm 8.800773486689e-02 
      3 KSP Residual norm 1.483076062855e-02 
      4 KSP Residual norm 2.093262045215e-03 
      5 KSP Residual norm 3.815495507259e-04 
      6 KSP Residual norm 1.211418214142e-04 
      7 KSP Residual norm 3.202930553388e-05 
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 7
  2 KSP Residual norm 3.681944659734e-05 
      Residual norms for mg_coarse_ solve.
      0 KSP Residual norm 1.746870894217e+01 
      1 KSP Residual norm 2.063585490500e+00 
      2 KSP Residual norm 3.890314925050e-01 
      3 KSP Residual norm 7.670965760587e-02 
      4 KSP Residual norm 1.857034044846e-02 
      5 KSP Residual norm 5.458218091460e-03 
      6 KSP Residual norm 8.613559864925e-04 
      7 KSP Residual norm 1.478310417093e-04 
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 7
  3 KSP Residual norm 1.902439098996e-06 
      Residual norms for mg_coarse_ solve.
      0 KSP Residual norm 6.459455306298e+01 
      1 KSP Residual norm 8.225577550094e+00 
      2 KSP Residual norm 1.879623876596e+00 
      3 KSP Residual norm 4.203650251387e-01 
      4 KSP Residual norm 1.116576353381e-01 
      5 KSP Residual norm 3.171415754395e-02 
      6 KSP Residual norm 5.466936227894e-03 
      7 KSP Residual norm 8.168079792241e-04 
      8 KSP Residual norm 2.003058248058e-04 
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 8
  4 KSP Residual norm 3.614285761497e-07 
      Residual norms for mg_coarse_ solve.
      0 KSP Residual norm 9.841327651322e+01 
      1 KSP Residual norm 1.159987395946e+01 
      2 KSP Residual norm 2.171163326812e+00 
      3 KSP Residual norm 4.315274772324e-01 
      4 KSP Residual norm 9.876227329786e-02 
      5 KSP Residual norm 2.975285858079e-02 
      6 KSP Residual norm 5.144180363632e-03 
      7 KSP Residual norm 8.609883283484e-04 
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 7
  5 KSP Residual norm 1.098473588229e-07 
      Residual norms for mg_coarse_ solve.
      0 KSP Residual norm 1.910940023405e+01 
      1 KSP Residual norm 2.296911508746e+00 
      2 KSP Residual norm 3.861890421647e-01 
      3 KSP Residual norm 8.237609759451e-02 
      4 KSP Residual norm 2.011974996326e-02 
      5 KSP Residual norm 5.305774230449e-03 
      6 KSP Residual norm 1.244347937953e-03 
      7 KSP Residual norm 2.016292109733e-04 
      8 KSP Residual norm 3.851751743379e-05 
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 8
  6 KSP Residual norm 2.884942892141e-08 
      Residual norms for mg_coarse_ solve.
      0 KSP Residual norm 1.052253834175e+02 
      1 KSP Residual norm 1.251961297029e+01 
      2 KSP Residual norm 2.418341785178e+00 
      3 KSP Residual norm 4.881681379271e-01 
      4 KSP Residual norm 1.188527565521e-01 
      5 KSP Residual norm 3.380296361244e-02 
      6 KSP Residual norm 5.412466344976e-03 
      7 KSP Residual norm 9.211736606120e-04 
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 7
  7 KSP Residual norm 7.806783834431e-09 
      Residual norms for mg_coarse_ solve.
      0 KSP Residual norm 1.377851616021e+02 
      1 KSP Residual norm 1.620340592642e+01 
      2 KSP Residual norm 3.005893905046e+00 
      3 KSP Residual norm 5.963303818234e-01 
      4 KSP Residual norm 1.323287770743e-01 
      5 KSP Residual norm 4.030256587623e-02 
      6 KSP Residual norm 7.068637320339e-03 
      7 KSP Residual norm 1.186212676418e-03 
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 7
  8 KSP Residual norm 1.282977792165e-09 
time taken for solve is83.053
************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

/home/a/asarkar/sudhipv/software/FreeFem-sources/src/mpi/FreeFem++-mpi on a  named nia0032.scinet.local with 240 processors, by sudhipv Sat Mar  5 06:24:40 2022
Using Petsc Release Version 3.15.0, Mar 30, 2021 

                         Max       Max/Min     Avg       Total
Time (sec):           8.314e+01     1.000   8.314e+01
Objects:              1.450e+02     1.000   1.450e+02
Flop:                 1.249e+13     1.265   1.132e+13  2.716e+15
Flop/sec:             1.502e+11     1.265   1.361e+11  3.267e+13
MPI Messages:         1.647e+03     9.000   9.943e+02  2.386e+05
MPI Message Lengths:  1.765e+07     3.360   1.378e+04  3.289e+09
MPI Reductions:       2.320e+02     1.000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flop
                            and VecAXPY() for complex vectors of length N --> 8N flop

Summary of Stages:   ----- Time ------  ----- Flop ------  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total    Count   %Total     Avg         %Total    Count   %Total
 0:      Main Stage: 8.3136e+01 100.0%  2.7162e+15 100.0%  2.386e+05 100.0%  1.378e+04      100.0%  2.140e+02  92.2%

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flop: Max - maximum over all processors
                  Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   AvgLen: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flop in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flop over all processors)/(max time over all processors)
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flop                              --- Global ---  --- Stage ----  Total
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   AvgLen  Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s
------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

BuildTwoSided          7 1.0 9.5805e-01265.5 0.00e+00 0.0 3.9e+03 4.0e+00 5.0e+00  1  0  2  0  2   1  0  2  0  2     0
BuildTwoSidedF         2 1.0 5.0373e-03 3.7 0.00e+00 0.0 0.0e+00 0.0e+00 2.0e+00  0  0  0  0  1   0  0  0  0  1     0
MatMult              116 1.0 4.5925e+00 1.1 2.17e+09 1.0 1.6e+05 7.4e+03 2.0e+00  5  0 66 35  1   5  0 66 35  1 111936
MatMultAdd             8 1.0 1.0591e-01 1.1 6.47e+06 1.0 1.0e+04 1.5e+04 0.0e+00  0  0  4  5  0   0  0  4  5  0 14510
MatMultTranspose       8 1.0 2.8439e-01 4.9 0.00e+00 0.0 1.0e+04 5.0e+04 0.0e+00  0  0  4 16  0   0  0  4 16  0     0
MatSolve              43 1.0 1.6323e+01 1.1 1.25e+13 1.3 0.0e+00 0.0e+00 0.0e+00 19100  0  0  0  19100  0  0  0 166362836
MatLUFactorSym         1 1.0 1.5657e+00 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  2  0  0  0  0   2  0  0  0  0     0
MatLUFactorNum         1 1.0 7.5501e+00 1.1 3.38e+08 1.1 0.0e+00 0.0e+00 0.0e+00  9  0  0  0  0   9  0  0  0  0 10262
MatConvert             1 1.0 8.1786e-02 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 4.0e+00  0  0  0  0  2   0  0  0  0  2     0
MatResidual            8 1.0 6.0895e-01 1.3 2.72e+08 1.0 1.0e+04 1.0e+04 0.0e+00  1  0  4  3  0   1  0  4  3  0 105962
MatAssemblyBegin       6 1.0 5.2221e-03 3.5 0.00e+00 0.0 0.0e+00 0.0e+00 2.0e+00  0  0  0  0  1   0  0  0  0  1     0
MatAssemblyEnd         6 1.0 2.1431e+00 8.0 0.00e+00 0.0 0.0e+00 0.0e+00 1.2e+01  1  0  0  0  5   1  0  0  0  6     0
MatGetRowIJ            3 1.0 3.6834e-02 1.6 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatCreateSubMats       1 1.0 6.6128e-01 1.1 0.00e+00 0.0 6.5e+03 1.3e+05 1.0e+00  1  0  3 26  0   1  0  3 26  0     0
MatGetOrdering         1 1.0 2.1061e-01 1.9 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatIncreaseOvrlp       1 1.0 9.7971e-02 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0
KSPSetUp               3 1.0 1.5357e+01 1.0 3.19e+12 1.3 3.9e+04 2.9e+04 3.8e+01 18 26 16 34 16  18 26 16 34 18 45247028
KSPSolve               1 1.0 3.0053e+01 1.0 9.29e+12 1.3 1.9e+05 1.0e+04 1.4e+02 36 74 81 60 61  36 74 81 60 66 67257820
KSPGMRESOrthog        76 1.0 1.0697e+00 3.5 2.46e+08 1.0 0.0e+00 0.0e+00 7.6e+01  1  0  0  0 33   1  0  0  0 36 54245
PCSetUp                1 1.0 1.6819e+01 1.0 3.19e+12 1.3 3.9e+04 2.9e+04 4.2e+01 20 26 16 34 18  20 26 16 34 20 41313844
PCSetUpOnBlocks       17 1.0 9.2070e+00 1.1 3.38e+08 1.1 0.0e+00 0.0e+00 0.0e+00 11  0  0  0  0  11  0  0  0  0  8415
PCApply                8 1.0 2.9481e+01 1.0 9.29e+12 1.3 1.8e+05 1.0e+04 1.2e+02 35 74 77 57 54  35 74 77 57 58 68559703
PCApplyOnBlocks       43 1.0 1.6359e+01 1.1 1.25e+13 1.3 0.0e+00 0.0e+00 0.0e+00 19100  0  0  0  19100  0  0  0 165993809
VecMDot               76 1.0 9.3136e-01 5.8 1.23e+08 1.0 0.0e+00 0.0e+00 7.6e+01  1  0  0  0 33   1  0  0  0 36 31150
VecNorm               86 1.0 9.8412e-02 3.3 2.97e+07 1.0 0.0e+00 0.0e+00 8.6e+01  0  0  0  0 37   0  0  0  0 40 71263
VecScale             102 1.0 1.7350e-02 1.1 1.48e+07 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0 202112
VecCopy               77 1.0 1.0040e-01 1.5 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecSet               237 1.0 1.3962e-01 1.3 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecAXPY               18 1.0 1.9853e-02 1.2 9.73e+06 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0 116121
VecAYPX               48 1.0 6.3729e-02 1.4 2.59e+07 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0 96457
VecAXPBYCZ            16 1.0 3.8426e-02 1.7 3.24e+07 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0 199967
VecMAXPY              86 1.0 1.7527e-01 1.0 1.49e+08 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0 201153
VecScatterBegin      280 1.0 1.1981e+00 4.5 0.00e+00 0.0 2.0e+05 7.7e+03 3.0e+00  1  0 86 48  1   1  0 86 48  1     0
VecScatterEnd        280 1.0 6.6417e-0163.9 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecNormalize          77 1.0 9.7767e-02 2.8 3.36e+07 1.0 0.0e+00 0.0e+00 7.7e+01  0  0  0  0 33   0  0  0  0 36 81074
SFSetGraph             5 1.0 1.1218e-03 1.7 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
SFSetUp                5 1.0 9.9158e-0127.6 0.00e+00 0.0 7.8e+03 2.1e+03 3.0e+00  1  0  3  0  1   1  0  3  0  1     0
SFReduceBegin         43 1.0 6.2890e-02 1.6 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
SFReduceEnd           43 1.0 1.3051e-04 2.3 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
SFPack               280 1.0 7.0898e-03 3.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
SFUnpack             280 1.0 1.2222e-0315.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix    12             12    549954132     0.
       Krylov Solver     5              5        72264     0.
      Preconditioner     5              5         6024     0.
              Viewer     2              1          848     0.
              Vector    85             85    203207640     0.
           Index Set    15             15     10844056     0.
   IS L to G Mapping     1              1      4840304     0.
   Star Forest Graph    11             11        12600     0.
    Distributed Mesh     3              3        15144     0.
     Discrete System     3              3         2712     0.
           Weak Form     3              3         2472     0.
========================================================================================================================
Average time to get PetscTime(): 2.95e-08
Average time for MPI_Barrier(): 5.67662e-05
Average time for zero size MPI_Send(): 1.33583e-06
#PETSc Option Table entries:
-ksp_max_it 200
-ksp_monitor
-ksp_type fgmres
-log_view
-m 2000
-mg_coarse_ksp_converged_reason
-mg_coarse_ksp_monitor
-mg_coarse_ksp_type gmres
-mg_coarse_pc_type hypre
-mg_levels_pc_type asm
-mg_levels_sub_pc_factor_mat_solver_type mumps
-mg_levels_sub_pc_type lu
-n 2000
-nw
-ordo 5
-pc_mg_type multiplicative
-pc_type mg
#End of PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: --prefix=/home/a/asarkar/sudhipv/software/FreeFem-sources/install/ff-petsc/r MAKEFLAGS= --with-debugging=0 COPTFLAGS="-O3 -mtune=native" CXXOPTFLAGS="-O3 -mtune=native" FOPTFLAGS="-O3 -mtune=native" --with-cxx-dialect=C++11 --with-ssl=0 --with-x=0 --with-fortran-bindings=0 --with-cudac=0 --with-cc=/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Compiler/intel2020/openmpi/4.0.3/bin/mpicc --with-cxx=/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Compiler/intel2020/openmpi/4.0.3/bin/mpic++ --with-fc=/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Compiler/intel2020/openmpi/4.0.3/bin/mpif90 --with-scalar-type=real --with-blaslapack-include=/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/imkl/2020.1.217/mkl/include --with-blaslapack-lib="-Wl,-rpath,/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/imkl/2020.1.217/mkl/lib/intel64 -L/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/imkl/2020.1.217/mkl/lib/intel64 -lmkl_rt -lmkl_intel_thread -lmkl_core  -liomp5 -lpthread" --download-metis --download-ptscotch --download-hypre --download-parmetis --download-mmg --download-parmmg --download-superlu --download-suitesparse --download-tetgen --download-slepc --download-hpddm --download-scalapack --download-mumps --download-slepc-configure-arguments=--download-arpack=https://github.com/prj-/arpack-ng/archive/b64dccb.tar.gz PETSC_ARCH=fr
-----------------------------------------
Libraries compiled on 2021-05-02 15:39:49 on nia-login03.scinet.local 
Machine characteristics: Linux-3.10.0-957.27.2.el7.x86_64-x86_64-with-centos-7.6.1810-Core
Using PETSc directory: /home/a/asarkar/sudhipv/software/FreeFem-sources/install/ff-petsc/r
Using PETSc arch: 
-----------------------------------------

Using C compiler: /cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Compiler/intel2020/openmpi/4.0.3/bin/mpicc  -fPIC -wd1572 -O3 -mtune=native   
Using Fortran compiler: /cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Compiler/intel2020/openmpi/4.0.3/bin/mpif90  -fPIC -O3 -mtune=native     
-----------------------------------------

Using include paths: -I/home/a/asarkar/sudhipv/software/FreeFem-sources/install/ff-petsc/r/include -I/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/imkl/2020.1.217/mkl/include
-----------------------------------------

Using C linker: /cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Compiler/intel2020/openmpi/4.0.3/bin/mpicc
Using Fortran linker: /cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Compiler/intel2020/openmpi/4.0.3/bin/mpif90
Using libraries: -Wl,-rpath,/home/a/asarkar/sudhipv/software/FreeFem-sources/install/ff-petsc/r/lib -L/home/a/asarkar/sudhipv/software/FreeFem-sources/install/ff-petsc/r/lib -lpetsc -Wl,-rpath,/home/a/asarkar/sudhipv/software/FreeFem-sources/install/ff-petsc/r/lib -L/home/a/asarkar/sudhipv/software/FreeFem-sources/install/ff-petsc/r/lib -Wl,-rpath,/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/imkl/2020.1.217/mkl/lib/intel64 -L/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/imkl/2020.1.217/mkl/lib/intel64 -Wl,-rpath,/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Compiler/intel2020/openmpi/4.0.3/lib -L/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Compiler/intel2020/openmpi/4.0.3/lib -Wl,-rpath,/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/libfabric/1.10.1/lib -L/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/libfabric/1.10.1/lib -Wl,-rpath,/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/ucx/1.8.0/lib -L/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/ucx/1.8.0/lib -Wl,-rpath,/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/imkl/2020.1.217/lib/intel64 -L/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/imkl/2020.1.217/lib/intel64 -Wl,-rpath,/cvmfs/restricted.computecanada.ca/easybuild/software/2020/Core/intel/2020.1.217/compilers_and_libraries_2020.1.217/linux/tbb/lib/intel64/gcc4.8 -L/cvmfs/restricted.computecanada.ca/easybuild/software/2020/Core/intel/2020.1.217/compilers_and_libraries_2020.1.217/linux/tbb/lib/intel64/gcc4.8 -Wl,-rpath,/cvmfs/restricted.computecanada.ca/easybuild/software/2020/Core/intel/2020.1.217/compilers_and_libraries_2020.1.217/linux/compiler/lib/intel64_lin -L/cvmfs/restricted.computecanada.ca/easybuild/software/2020/Core/intel/2020.1.217/compilers_and_libraries_2020.1.217/linux/compiler/lib/intel64_lin -Wl,-rpath,/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Compiler/intel2020/openmpi/4.0.3/lib64 -L/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Compiler/intel2020/openmpi/4.0.3/lib64 -Wl,-rpath,/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/gcccore/9.3.0/lib/gcc/x86_64-pc-linux-gnu/9.3.0 -L/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/gcccore/9.3.0/lib/gcc/x86_64-pc-linux-gnu/9.3.0 -Wl,-rpath,/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/gcccore/9.3.0/lib64 -L/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/gcccore/9.3.0/lib64 -Wl,-rpath,/cvmfs/soft.computecanada.ca/gentoo/2020/lib64 -L/cvmfs/soft.computecanada.ca/gentoo/2020/lib64 -Wl,-rpath,/cvmfs/soft.computecanada.ca/gentoo/2020/usr/lib64 -L/cvmfs/soft.computecanada.ca/gentoo/2020/usr/lib64 -Wl,-rpath,/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/gcccore/9.3.0/lib -L/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/gcccore/9.3.0/lib -Wl,-rpath,/cvmfs/soft.computecanada.ca/gentoo/2020/lib -L/cvmfs/soft.computecanada.ca/gentoo/2020/lib -Wl,-rpath,/cvmfs/soft.computecanada.ca/gentoo/2020/usr/lib -L/cvmfs/soft.computecanada.ca/gentoo/2020/usr/lib -lHYPRE -lcmumps -ldmumps -lsmumps -lzmumps -lmumps_common -lpord -lscalapack -lumfpack -lklu -lcholmod -lbtf -lccolamd -lcolamd -lcamd -lamd -lsuitesparseconfig -lsuperlu -lmkl_rt -lmkl_intel_thread -lmkl_core -liomp5 -lpthread -lparmmg -lmmg -lmmg3d -lptesmumps -lptscotchparmetis -lptscotch -lptscotcherr -lesmumps -lscotch -lscotcherr -lpthread -lparmetis -lmetis -ltet -lstdc++ -ldl -lmpi_usempif08 -lmpi_usempi_ignore_tkr -lmpi_mpifh -lmpi -lifport -lifcoremt_pic -limf -lsvml -lm -lipgo -lirc -lpthread -lgcc_s -lirc_s -lrt -lquadmath -lstdc++ -ldl
-----------------------------------------

Number of Vertices for coarse grid  4004001
Number of Vertices for fine mesh in  1.6008e+07
1	
	-111111	
order of input  2
order of output  7
Mean of Gaussian  0
Sd of Gaussian  0.3
Size of linear System  1.28064e+08
6	
	1.04602786	0.313808358	0.06656880537	0.01153005531	0.001729508297
	0.0002320378871	
  0 KSP Residual norm 2.499375000000e-04 
      Residual norms for mg_coarse_ solve.
      0 KSP Residual norm 2.272269566029e+05 
      1 KSP Residual norm 2.462397963343e+04 
      2 KSP Residual norm 3.733509654965e+03 
      3 KSP Residual norm 6.793854088153e+02 
      4 KSP Residual norm 1.523754632529e+02 
      5 KSP Residual norm 3.608183284866e+01 
      6 KSP Residual norm 7.041628921132e+00 
      7 KSP Residual norm 1.236100946893e+00 
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 7
  1 KSP Residual norm 2.469104345692e-04 
      Residual norms for mg_coarse_ solve.
      0 KSP Residual norm 5.569640845124e+00 
      1 KSP Residual norm 6.072701406109e-01 
      2 KSP Residual norm 9.732852299130e-02 
      3 KSP Residual norm 1.862106064825e-02 
      4 KSP Residual norm 4.509630368811e-03 
      5 KSP Residual norm 1.119187066631e-03 
      6 KSP Residual norm 2.539228534044e-04 
      7 KSP Residual norm 6.133179745029e-05 
      8 KSP Residual norm 2.054619309868e-05 
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 8
  2 KSP Residual norm 3.412773814749e-05 
      Residual norms for mg_coarse_ solve.
      0 KSP Residual norm 2.241802443454e+01 
      1 KSP Residual norm 2.625872988674e+00 
      2 KSP Residual norm 5.523193412203e-01 
      3 KSP Residual norm 1.376124507353e-01 
      4 KSP Residual norm 3.797066861959e-02 
      5 KSP Residual norm 8.668916795134e-03 
      6 KSP Residual norm 1.841771222713e-03 
      7 KSP Residual norm 4.772734723920e-04 
      8 KSP Residual norm 1.264190581691e-04 
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 8
  3 KSP Residual norm 2.046816966095e-06 
      Residual norms for mg_coarse_ solve.
      0 KSP Residual norm 8.499819819370e+01 
      1 KSP Residual norm 1.065739367465e+01 
      2 KSP Residual norm 2.587961741882e+00 
      3 KSP Residual norm 6.741540487198e-01 
      4 KSP Residual norm 1.812722061886e-01 
      5 KSP Residual norm 4.454214528192e-02 
      6 KSP Residual norm 1.061482858791e-02 
      7 KSP Residual norm 2.355912511624e-03 
      8 KSP Residual norm 7.239800260595e-04 
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 8
  4 KSP Residual norm 3.855881416133e-07 
      Residual norms for mg_coarse_ solve.
      0 KSP Residual norm 9.241551437830e+01 
      1 KSP Residual norm 1.093410139322e+01 
      2 KSP Residual norm 2.324436941669e+00 
      3 KSP Residual norm 5.839731698843e-01 
      4 KSP Residual norm 1.584797247673e-01 
      5 KSP Residual norm 3.613349814169e-02 
      6 KSP Residual norm 8.218651181870e-03 
      7 KSP Residual norm 2.076233411955e-03 
      8 KSP Residual norm 5.427533805389e-04 
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 8
  5 KSP Residual norm 1.080505920300e-07 
      Residual norms for mg_coarse_ solve.
      0 KSP Residual norm 1.667863985461e+01 
      1 KSP Residual norm 1.977224009108e+00 
      2 KSP Residual norm 3.464651289056e-01 
      3 KSP Residual norm 7.023422093240e-02 
      4 KSP Residual norm 1.653088729027e-02 
      5 KSP Residual norm 4.155316720320e-03 
      6 KSP Residual norm 1.032602785896e-03 
      7 KSP Residual norm 2.012237845930e-04 
      8 KSP Residual norm 3.509926269068e-05 
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 8
  6 KSP Residual norm 3.116157139835e-08 
      Residual norms for mg_coarse_ solve.
      0 KSP Residual norm 7.839506510010e+01 
      1 KSP Residual norm 9.421055692522e+00 
      2 KSP Residual norm 2.080380984870e+00 
      3 KSP Residual norm 5.285784192233e-01 
      4 KSP Residual norm 1.436549236462e-01 
      5 KSP Residual norm 3.234930088692e-02 
      6 KSP Residual norm 7.396153526100e-03 
      7 KSP Residual norm 1.859867699639e-03 
      8 KSP Residual norm 4.846274960357e-04 
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 8
  7 KSP Residual norm 1.007410275933e-08 
      Residual norms for mg_coarse_ solve.
      0 KSP Residual norm 9.465083624163e+01 
      1 KSP Residual norm 1.118944327228e+01 
      2 KSP Residual norm 2.365003668115e+00 
      3 KSP Residual norm 5.938987091525e-01 
      4 KSP Residual norm 1.603333306522e-01 
      5 KSP Residual norm 3.710314758470e-02 
      6 KSP Residual norm 8.730588321053e-03 
      7 KSP Residual norm 2.124300787569e-03 
      8 KSP Residual norm 5.658416808652e-04 
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 8
  8 KSP Residual norm 2.252742314953e-09 
time taken for solve is103.109
************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

/home/a/asarkar/sudhipv/software/FreeFem-sources/src/mpi/FreeFem++-mpi on a  named nia0032.scinet.local with 320 processors, by sudhipv Sat Mar  5 06:26:28 2022
Using Petsc Release Version 3.15.0, Mar 30, 2021 

                         Max       Max/Min     Avg       Total
Time (sec):           1.034e+02     1.000   1.034e+02
Objects:              1.450e+02     1.000   1.450e+02
Flop:                 1.937e+13     1.303   1.730e+13  5.537e+15
Flop/sec:             1.874e+11     1.303   1.674e+11  5.357e+13
MPI Messages:         1.504e+03     4.000   1.035e+03  3.313e+05
MPI Message Lengths:  2.198e+07     2.978   1.678e+04  5.559e+09
MPI Reductions:       2.420e+02     1.000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flop
                            and VecAXPY() for complex vectors of length N --> 8N flop

Summary of Stages:   ----- Time ------  ----- Flop ------  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total    Count   %Total     Avg         %Total    Count   %Total
 0:      Main Stage: 1.0335e+02 100.0%  5.5367e+15 100.0%  3.313e+05 100.0%  1.678e+04      100.0%  2.240e+02  92.6%

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flop: Max - maximum over all processors
                  Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   AvgLen: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flop in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flop over all processors)/(max time over all processors)
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flop                              --- Global ---  --- Stage ----  Total
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   AvgLen  Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s
------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

BuildTwoSided          7 1.0 1.6170e+00244.6 0.00e+00 0.0 5.3e+03 4.0e+00 5.0e+00  1  0  2  0  2   1  0  2  0  2     0
BuildTwoSidedF         2 1.0 5.9868e-03 5.5 0.00e+00 0.0 0.0e+00 0.0e+00 2.0e+00  0  0  0  0  1   0  0  0  0  1     0
MatMult              121 1.0 6.1419e+00 1.1 2.97e+09 1.0 2.2e+05 8.4e+03 2.0e+00  6  0 66 33  1   6  0 66 33  1 152139
MatMultAdd             8 1.0 1.0785e-01 1.1 6.48e+06 1.0 1.4e+04 1.7e+04 0.0e+00  0  0  4  4  0   0  0  4  4  0 19000
MatMultTranspose       8 1.0 3.2390e-01 6.0 0.00e+00 0.0 1.4e+04 5.8e+04 0.0e+00  0  0  4 15  0   0  0  4 15  0     0
MatSolve              43 1.0 1.9634e+01 1.1 1.94e+13 1.3 0.0e+00 0.0e+00 0.0e+00 18100  0  0  0  18100  0  0  0 281942805
MatLUFactorSym         1 1.0 1.6366e+00 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  2  0  0  0  0   2  0  0  0  0     0
MatLUFactorNum         1 1.0 1.0454e+01 1.2 4.34e+08 1.1 0.0e+00 0.0e+00 0.0e+00  9  0  0  0  0   9  0  0  0  0 12638
MatConvert             1 1.0 8.4926e-02 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 4.0e+00  0  0  0  0  2   0  0  0  0  2     0
MatResidual            8 1.0 7.6546e-01 1.3 3.63e+08 1.0 1.4e+04 1.2e+04 0.0e+00  1  0  4  3  0   1  0  4  3  0 149861
MatAssemblyBegin       6 1.0 6.1868e-03 3.6 0.00e+00 0.0 0.0e+00 0.0e+00 2.0e+00  0  0  0  0  1   0  0  0  0  1     0
MatAssemblyEnd         6 1.0 2.5970e+0012.2 0.00e+00 0.0 0.0e+00 0.0e+00 1.2e+01  1  0  0  0  5   1  0  0  0  5     0
MatGetRowIJ            3 1.0 3.8266e-02 1.5 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatCreateSubMats       1 1.0 1.0092e+00 1.2 0.00e+00 0.0 8.8e+03 2.0e+05 1.0e+00  1  0  3 31  0   1  0  3 31  0     0
MatGetOrdering         1 1.0 1.5072e-01 1.7 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatIncreaseOvrlp       1 1.0 1.1825e-01 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0
KSPSetUp               3 1.0 1.9925e+01 1.0 4.95e+12 1.3 5.3e+04 4.1e+04 3.8e+01 19 26 16 39 16  19 26 16 39 17 71085529
KSPSolve               1 1.0 3.8653e+01 1.0 1.44e+13 1.3 2.7e+05 1.1e+04 1.5e+02 37 74 81 56 63  37 74 81 56 68 106598766
KSPGMRESOrthog        81 1.0 1.5480e+00 3.2 2.62e+08 1.0 0.0e+00 0.0e+00 8.1e+01  1  0  0  0 33   1  0  0  0 36 53287
PCSetUp                1 1.0 2.1733e+01 1.0 4.95e+12 1.3 5.3e+04 4.1e+04 4.2e+01 21 26 16 39 17  21 26 16 39 19 65172602
PCSetUpOnBlocks       17 1.0 1.2212e+01 1.1 4.34e+08 1.1 0.0e+00 0.0e+00 0.0e+00 11  0  0  0  0  11  0  0  0  0 10818
PCApply                8 1.0 3.7858e+01 1.0 1.44e+13 1.3 2.6e+05 1.1e+04 1.4e+02 36 74 77 53 56  36 74 77 53 60 108834105
PCApplyOnBlocks       43 1.0 1.9666e+01 1.1 1.94e+13 1.3 0.0e+00 0.0e+00 0.0e+00 18100  0  0  0  18100  0  0  0 281483182
VecMDot               81 1.0 1.4018e+00 4.2 1.31e+08 1.0 0.0e+00 0.0e+00 8.1e+01  1  0  0  0 33   1  0  0  0 36 29424
VecNorm               91 1.0 1.4184e-01 2.2 3.08e+07 1.0 0.0e+00 0.0e+00 9.1e+01  0  0  0  0 38   0  0  0  0 41 68182
VecScale             107 1.0 1.7297e-02 1.1 1.54e+07 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0 279559
VecCopy               77 1.0 1.0490e-01 1.4 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecSet               242 1.0 1.3804e-01 1.4 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecAXPY               18 1.0 1.9876e-02 1.4 9.74e+06 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0 154650
VecAYPX               48 1.0 6.5832e-02 1.5 2.59e+07 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0 124500
VecAXPBYCZ            16 1.0 3.9444e-02 1.6 3.24e+07 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0 259738
VecMAXPY              91 1.0 1.8557e-01 1.1 1.59e+08 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0 268855
VecScatterBegin      285 1.0 1.8631e+00 6.6 0.00e+00 0.0 2.9e+05 8.7e+03 3.0e+00  1  0 86 45  1   1  0 86 45  1     0
VecScatterEnd        285 1.0 5.8714e-0161.7 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecNormalize          82 1.0 1.4077e-01 2.2 3.52e+07 1.0 0.0e+00 0.0e+00 8.2e+01  0  0  0  0 34   0  0  0  0 37 78489
SFSetGraph             5 1.0 1.1959e-03 1.7 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
SFSetUp                5 1.0 1.6478e+0042.3 0.00e+00 0.0 1.1e+04 2.4e+03 3.0e+00  1  0  3  0  1   1  0  3  0  1     0
SFReduceBegin         43 1.0 6.3032e-02 1.5 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
SFReduceEnd           43 1.0 1.2366e-04 2.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
SFPack               285 1.0 8.4311e-03 3.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
SFUnpack             285 1.0 1.4023e-0315.5 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix    12             12    698425244     0.
       Krylov Solver     5              5        72264     0.
      Preconditioner     5              5         6024     0.
              Viewer     2              1          848     0.
              Vector    85             85    202069176     0.
           Index Set    15             15     10632320     0.
   IS L to G Mapping     1              1      4847816     0.
   Star Forest Graph    11             11        12600     0.
    Distributed Mesh     3              3        15144     0.
     Discrete System     3              3         2712     0.
           Weak Form     3              3         2472     0.
========================================================================================================================
Average time to get PetscTime(): 3.02e-08
Average time for MPI_Barrier(): 6.59662e-05
Average time for zero size MPI_Send(): 1.21492e-06
#PETSc Option Table entries:
-ksp_max_it 200
-ksp_monitor
-ksp_type fgmres
-log_view
-m 2000
-mg_coarse_ksp_converged_reason
-mg_coarse_ksp_monitor
-mg_coarse_ksp_type gmres
-mg_coarse_pc_type hypre
-mg_levels_pc_type asm
-mg_levels_sub_pc_factor_mat_solver_type mumps
-mg_levels_sub_pc_type lu
-n 2000
-nw
-ordo 7
-pc_mg_type multiplicative
-pc_type mg
#End of PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: --prefix=/home/a/asarkar/sudhipv/software/FreeFem-sources/install/ff-petsc/r MAKEFLAGS= --with-debugging=0 COPTFLAGS="-O3 -mtune=native" CXXOPTFLAGS="-O3 -mtune=native" FOPTFLAGS="-O3 -mtune=native" --with-cxx-dialect=C++11 --with-ssl=0 --with-x=0 --with-fortran-bindings=0 --with-cudac=0 --with-cc=/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Compiler/intel2020/openmpi/4.0.3/bin/mpicc --with-cxx=/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Compiler/intel2020/openmpi/4.0.3/bin/mpic++ --with-fc=/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Compiler/intel2020/openmpi/4.0.3/bin/mpif90 --with-scalar-type=real --with-blaslapack-include=/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/imkl/2020.1.217/mkl/include --with-blaslapack-lib="-Wl,-rpath,/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/imkl/2020.1.217/mkl/lib/intel64 -L/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/imkl/2020.1.217/mkl/lib/intel64 -lmkl_rt -lmkl_intel_thread -lmkl_core  -liomp5 -lpthread" --download-metis --download-ptscotch --download-hypre --download-parmetis --download-mmg --download-parmmg --download-superlu --download-suitesparse --download-tetgen --download-slepc --download-hpddm --download-scalapack --download-mumps --download-slepc-configure-arguments=--download-arpack=https://github.com/prj-/arpack-ng/archive/b64dccb.tar.gz PETSC_ARCH=fr
-----------------------------------------
Libraries compiled on 2021-05-02 15:39:49 on nia-login03.scinet.local 
Machine characteristics: Linux-3.10.0-957.27.2.el7.x86_64-x86_64-with-centos-7.6.1810-Core
Using PETSc directory: /home/a/asarkar/sudhipv/software/FreeFem-sources/install/ff-petsc/r
Using PETSc arch: 
-----------------------------------------

Using C compiler: /cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Compiler/intel2020/openmpi/4.0.3/bin/mpicc  -fPIC -wd1572 -O3 -mtune=native   
Using Fortran compiler: /cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Compiler/intel2020/openmpi/4.0.3/bin/mpif90  -fPIC -O3 -mtune=native     
-----------------------------------------

Using include paths: -I/home/a/asarkar/sudhipv/software/FreeFem-sources/install/ff-petsc/r/include -I/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/imkl/2020.1.217/mkl/include
-----------------------------------------

Using C linker: /cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Compiler/intel2020/openmpi/4.0.3/bin/mpicc
Using Fortran linker: /cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Compiler/intel2020/openmpi/4.0.3/bin/mpif90
Using libraries: -Wl,-rpath,/home/a/asarkar/sudhipv/software/FreeFem-sources/install/ff-petsc/r/lib -L/home/a/asarkar/sudhipv/software/FreeFem-sources/install/ff-petsc/r/lib -lpetsc -Wl,-rpath,/home/a/asarkar/sudhipv/software/FreeFem-sources/install/ff-petsc/r/lib -L/home/a/asarkar/sudhipv/software/FreeFem-sources/install/ff-petsc/r/lib -Wl,-rpath,/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/imkl/2020.1.217/mkl/lib/intel64 -L/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/imkl/2020.1.217/mkl/lib/intel64 -Wl,-rpath,/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Compiler/intel2020/openmpi/4.0.3/lib -L/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Compiler/intel2020/openmpi/4.0.3/lib -Wl,-rpath,/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/libfabric/1.10.1/lib -L/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/libfabric/1.10.1/lib -Wl,-rpath,/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/ucx/1.8.0/lib -L/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/ucx/1.8.0/lib -Wl,-rpath,/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/imkl/2020.1.217/lib/intel64 -L/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/imkl/2020.1.217/lib/intel64 -Wl,-rpath,/cvmfs/restricted.computecanada.ca/easybuild/software/2020/Core/intel/2020.1.217/compilers_and_libraries_2020.1.217/linux/tbb/lib/intel64/gcc4.8 -L/cvmfs/restricted.computecanada.ca/easybuild/software/2020/Core/intel/2020.1.217/compilers_and_libraries_2020.1.217/linux/tbb/lib/intel64/gcc4.8 -Wl,-rpath,/cvmfs/restricted.computecanada.ca/easybuild/software/2020/Core/intel/2020.1.217/compilers_and_libraries_2020.1.217/linux/compiler/lib/intel64_lin -L/cvmfs/restricted.computecanada.ca/easybuild/software/2020/Core/intel/2020.1.217/compilers_and_libraries_2020.1.217/linux/compiler/lib/intel64_lin -Wl,-rpath,/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Compiler/intel2020/openmpi/4.0.3/lib64 -L/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Compiler/intel2020/openmpi/4.0.3/lib64 -Wl,-rpath,/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/gcccore/9.3.0/lib/gcc/x86_64-pc-linux-gnu/9.3.0 -L/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/gcccore/9.3.0/lib/gcc/x86_64-pc-linux-gnu/9.3.0 -Wl,-rpath,/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/gcccore/9.3.0/lib64 -L/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/gcccore/9.3.0/lib64 -Wl,-rpath,/cvmfs/soft.computecanada.ca/gentoo/2020/lib64 -L/cvmfs/soft.computecanada.ca/gentoo/2020/lib64 -Wl,-rpath,/cvmfs/soft.computecanada.ca/gentoo/2020/usr/lib64 -L/cvmfs/soft.computecanada.ca/gentoo/2020/usr/lib64 -Wl,-rpath,/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/gcccore/9.3.0/lib -L/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/gcccore/9.3.0/lib -Wl,-rpath,/cvmfs/soft.computecanada.ca/gentoo/2020/lib -L/cvmfs/soft.computecanada.ca/gentoo/2020/lib -Wl,-rpath,/cvmfs/soft.computecanada.ca/gentoo/2020/usr/lib -L/cvmfs/soft.computecanada.ca/gentoo/2020/usr/lib -lHYPRE -lcmumps -ldmumps -lsmumps -lzmumps -lmumps_common -lpord -lscalapack -lumfpack -lklu -lcholmod -lbtf -lccolamd -lcolamd -lcamd -lamd -lsuitesparseconfig -lsuperlu -lmkl_rt -lmkl_intel_thread -lmkl_core -liomp5 -lpthread -lparmmg -lmmg -lmmg3d -lptesmumps -lptscotchparmetis -lptscotch -lptscotcherr -lesmumps -lscotch -lscotcherr -lpthread -lparmetis -lmetis -ltet -lstdc++ -ldl -lmpi_usempif08 -lmpi_usempi_ignore_tkr -lmpi_mpifh -lmpi -lifport -lifcoremt_pic -limf -lsvml -lm -lipgo -lirc -lpthread -lgcc_s -lirc_s -lrt -lquadmath -lstdc++ -ldl
-----------------------------------------

Number of Vertices for coarse grid  4004001
Number of Vertices for fine mesh in  1.6008e+07
1	
	-111111	
order of input  2
order of output  9
Mean of Gaussian  0
Sd of Gaussian  0.3
Size of linear System  1.6008e+08
6	
	1.04602786	0.313808358	0.06656880537	0.01153005531	0.001729508297
	0.0002320378871	
  0 KSP Residual norm 2.499375000000e-04 
      Residual norms for mg_coarse_ solve.
      0 KSP Residual norm 2.224792612425e+05 
      1 KSP Residual norm 2.629006192007e+04 
      2 KSP Residual norm 4.003988523120e+03 
      3 KSP Residual norm 7.084272212435e+02 
      4 KSP Residual norm 1.462856648588e+02 
      5 KSP Residual norm 3.479641121510e+01 
      6 KSP Residual norm 8.535194372378e+00 
      7 KSP Residual norm 2.130698424638e+00 
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 7
  1 KSP Residual norm 2.465701838675e-04 
      Residual norms for mg_coarse_ solve.
      0 KSP Residual norm 5.819229551021e+00 
      1 KSP Residual norm 6.967378238095e-01 
      2 KSP Residual norm 1.213092324980e-01 
      3 KSP Residual norm 2.757332821952e-02 
      4 KSP Residual norm 7.641195252592e-03 
      5 KSP Residual norm 2.046181655978e-03 
      6 KSP Residual norm 4.765575633432e-04 
      7 KSP Residual norm 1.224970307927e-04 
      8 KSP Residual norm 3.244366757297e-05 
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 8
  2 KSP Residual norm 3.186286789906e-05 
      Residual norms for mg_coarse_ solve.
      0 KSP Residual norm 2.481884239544e+01 
      1 KSP Residual norm 3.518538485826e+00 
      2 KSP Residual norm 9.418679099480e-01 
      3 KSP Residual norm 2.687299175085e-01 
      4 KSP Residual norm 7.200439626126e-02 
      5 KSP Residual norm 1.648578087874e-02 
      6 KSP Residual norm 3.807458499870e-03 
      7 KSP Residual norm 8.261682915077e-04 
      8 KSP Residual norm 2.649807802237e-04 
      9 KSP Residual norm 6.810965933851e-05 
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 9
  3 KSP Residual norm 2.399751302271e-06 
      Residual norms for mg_coarse_ solve.
      0 KSP Residual norm 7.601805693845e+01 
      1 KSP Residual norm 1.212178145095e+01 
      2 KSP Residual norm 3.621924532051e+00 
      3 KSP Residual norm 9.923249795194e-01 
      4 KSP Residual norm 2.673025902759e-01 
      5 KSP Residual norm 7.020674672074e-02 
      6 KSP Residual norm 1.467302796273e-02 
      7 KSP Residual norm 3.312769295617e-03 
      8 KSP Residual norm 1.189114929793e-03 
      9 KSP Residual norm 2.516360997388e-04 
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 9
  4 KSP Residual norm 4.364747291364e-07 
      Residual norms for mg_coarse_ solve.
      0 KSP Residual norm 6.857952195799e+01 
      1 KSP Residual norm 9.671755777696e+00 
      2 KSP Residual norm 2.543118469583e+00 
      3 KSP Residual norm 7.292628255976e-01 
      4 KSP Residual norm 1.890683753932e-01 
      5 KSP Residual norm 4.273868053033e-02 
      6 KSP Residual norm 1.079048183174e-02 
      7 KSP Residual norm 2.457047455523e-03 
      8 KSP Residual norm 7.498974638531e-04 
      9 KSP Residual norm 1.871027571260e-04 
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 9
  5 KSP Residual norm 1.087227189380e-07 
      Residual norms for mg_coarse_ solve.
      0 KSP Residual norm 1.713615919903e+01 
      1 KSP Residual norm 2.232668265482e+00 
      2 KSP Residual norm 4.206290565715e-01 
      3 KSP Residual norm 9.947341630157e-02 
      4 KSP Residual norm 2.735015207928e-02 
      5 KSP Residual norm 8.397658047735e-03 
      6 KSP Residual norm 2.287967556239e-03 
      7 KSP Residual norm 5.346722106460e-04 
      8 KSP Residual norm 1.310816435146e-04 
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 8
  6 KSP Residual norm 3.268608504522e-08 
      Residual norms for mg_coarse_ solve.
      0 KSP Residual norm 4.414651702034e+01 
      1 KSP Residual norm 6.535952201909e+00 
      2 KSP Residual norm 1.824112504057e+00 
      3 KSP Residual norm 5.137900634101e-01 
      4 KSP Residual norm 1.333091479894e-01 
      5 KSP Residual norm 3.086199583173e-02 
      6 KSP Residual norm 7.621713208598e-03 
      7 KSP Residual norm 1.756361730260e-03 
      8 KSP Residual norm 5.681328186601e-04 
      9 KSP Residual norm 1.323797839027e-04 
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 9
  7 KSP Residual norm 1.160619090167e-08 
      Residual norms for mg_coarse_ solve.
      0 KSP Residual norm 5.375684122553e+01 
      1 KSP Residual norm 7.545503115931e+00 
      2 KSP Residual norm 1.960033549709e+00 
      3 KSP Residual norm 5.635194341294e-01 
      4 KSP Residual norm 1.455532616731e-01 
      5 KSP Residual norm 3.424016705443e-02 
      6 KSP Residual norm 8.945361669135e-03 
      7 KSP Residual norm 1.959225799297e-03 
      8 KSP Residual norm 5.880763463426e-04 
      9 KSP Residual norm 1.458598357484e-04 
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 9
  8 KSP Residual norm 3.012966605740e-09 
      Residual norms for mg_coarse_ solve.
      0 KSP Residual norm 3.683791632532e+01 
      1 KSP Residual norm 4.842103036641e+00 
      2 KSP Residual norm 1.110554456645e+00 
      3 KSP Residual norm 3.210213801353e-01 
      4 KSP Residual norm 8.702054485873e-02 
      5 KSP Residual norm 2.273421597617e-02 
      6 KSP Residual norm 6.114091523565e-03 
      7 KSP Residual norm 1.248732494809e-03 
      8 KSP Residual norm 3.401948486002e-04 
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 8
  9 KSP Residual norm 5.492444878345e-10 
time taken for solve is142.302
************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

/home/a/asarkar/sudhipv/software/FreeFem-sources/src/mpi/FreeFem++-mpi on a  named nia0032.scinet.local with 400 processors, by sudhipv Sat Mar  5 06:28:58 2022
Using Petsc Release Version 3.15.0, Mar 30, 2021 

                         Max       Max/Min     Avg       Total
Time (sec):           1.429e+02     1.000   1.429e+02
Objects:              1.450e+02     1.000   1.450e+02
Flop:                 2.857e+13     1.402   2.629e+13  1.052e+16
Flop/sec:             2.000e+11     1.402   1.840e+11  7.362e+13
MPI Messages:         1.908e+03     4.500   1.181e+03  4.723e+05
MPI Message Lengths:  3.294e+07     3.878   1.904e+04  8.995e+09
MPI Reductions:       2.710e+02     1.000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flop
                            and VecAXPY() for complex vectors of length N --> 8N flop

Summary of Stages:   ----- Time ------  ----- Flop ------  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total    Count   %Total     Avg         %Total    Count   %Total
 0:      Main Stage: 1.4286e+02 100.0%  1.0517e+16 100.0%  4.723e+05 100.0%  1.904e+04      100.0%  2.530e+02  93.4%

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flop: Max - maximum over all processors
                  Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   AvgLen: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flop in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flop over all processors)/(max time over all processors)
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flop                              --- Global ---  --- Stage ----  Total
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   AvgLen  Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s
------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

BuildTwoSided          7 1.0 2.7613e+00774.7 0.00e+00 0.0 6.7e+03 4.0e+00 5.0e+00  1  0  1  0  2   1  0  1  0  2     0
BuildTwoSidedF         2 1.0 6.0788e-03 7.4 0.00e+00 0.0 0.0e+00 0.0e+00 2.0e+00  0  0  0  0  1   0  0  0  0  1     0
MatMult              140 1.0 7.6237e+00 1.2 4.18e+09 1.0 3.2e+05 9.2e+03 2.0e+00  5  0 68 33  1   5  0 68 33  1 215931
MatMultAdd             9 1.0 1.6069e-01 1.5 7.30e+06 1.0 2.0e+04 1.9e+04 0.0e+00  0  0  4  4  0   0  0  4  4  0 17932
MatMultTranspose       9 1.0 6.2643e-01 8.6 0.00e+00 0.0 2.0e+04 6.4e+04 0.0e+00  0  0  4 14  0   0  0  4 14  0     0
MatSolve              47 1.0 2.6016e+01 1.1 2.86e+13 1.4 0.0e+00 0.0e+00 0.0e+00 17100  0  0  0  17100  0  0  0 404176324
MatLUFactorSym         1 1.0 1.7845e+00 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0
MatLUFactorNum         1 1.0 1.3905e+01 1.2 5.26e+08 1.2 0.0e+00 0.0e+00 0.0e+00  9  0  0  0  0   9  0  0  0  0 14387
MatConvert             1 1.0 1.2744e-01 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 4.0e+00  0  0  0  0  1   0  0  0  0  2     0
MatResidual            9 1.0 1.1583e+00 1.8 5.11e+08 1.0 2.0e+04 1.3e+04 0.0e+00  1  0  4  3  0   1  0  4  3  0 174089
MatAssemblyBegin       6 1.0 6.2350e-03 6.5 0.00e+00 0.0 0.0e+00 0.0e+00 2.0e+00  0  0  0  0  1   0  0  0  0  1     0
MatAssemblyEnd         6 1.0 3.7805e+0013.5 0.00e+00 0.0 0.0e+00 0.0e+00 1.2e+01  1  0  0  0  4   1  0  0  0  5     0
MatGetRowIJ            3 1.0 3.3512e-02 1.6 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatCreateSubMats       1 1.0 1.6794e+00 1.7 0.00e+00 0.0 1.1e+04 2.7e+05 1.0e+00  1  0  2 33  0   1  0  2 33  0     0
MatGetOrdering         1 1.0 1.2232e-01 1.8 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatIncreaseOvrlp       1 1.0 1.3554e-01 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0
KSPSetUp               3 1.0 2.5314e+01 1.0 6.69e+12 1.4 6.7e+04 5.4e+04 3.8e+01 18 23 14 40 14  18 23 14 40 15 97235223
KSPSolve               1 1.0 6.5030e+01 1.0 2.19e+13 1.4 3.9e+05 1.3e+04 1.8e+02 46 77 83 55 67  46 77 83 55 72 123874617
KSPGMRESOrthog        95 1.0 7.1871e+00 5.9 3.10e+08 1.0 0.0e+00 0.0e+00 9.5e+01  3  0  0  0 35   3  0  0  0 38 16954
PCSetUp                1 1.0 2.7796e+01 1.0 6.69e+12 1.4 6.7e+04 5.4e+04 4.2e+01 19 23 14 40 15  19 23 14 40 17 88553147
PCSetUpOnBlocks       19 1.0 1.5765e+01 1.2 5.26e+08 1.2 0.0e+00 0.0e+00 0.0e+00 10  0  0  0  0  10  0  0  0  0 12689
PCApply                9 1.0 6.3795e+01 1.0 2.19e+13 1.4 3.7e+05 1.3e+04 1.6e+02 44 77 79 52 60  44 77 79 52 64 126267956
PCApplyOnBlocks       47 1.0 2.6051e+01 1.1 2.86e+13 1.4 0.0e+00 0.0e+00 0.0e+00 17100  0  0  0  17100  0  0  0 403635357
VecMDot               95 1.0 6.9979e+00 7.1 1.55e+08 1.0 0.0e+00 0.0e+00 9.5e+01  3  0  0  0 35   3  0  0  0 38  8706
VecNorm              106 1.0 7.7560e-0110.2 3.45e+07 1.0 0.0e+00 0.0e+00 1.1e+02  0  0  0  0 39   0  0  0  0 42 17445
VecScale             124 1.0 2.0207e-02 1.2 1.72e+07 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0 334789
VecCopy               85 1.0 1.0984e-01 1.4 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecSet               272 1.0 1.6699e-01 1.4 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecAXPY               20 1.0 2.8652e-02 1.8 1.08e+07 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0 148071
VecAYPX               54 1.0 7.8033e-02 1.5 2.92e+07 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0 147703
VecAXPBYCZ            18 1.0 4.5094e-02 1.6 3.65e+07 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0 319493
VecMAXPY             106 1.0 2.8659e-01 1.4 1.86e+08 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0 255049
VecScatterBegin      319 1.0 3.0599e+0010.8 0.00e+00 0.0 4.1e+05 9.5e+03 3.0e+00  1  0 87 43  1   1  0 87 43  1     0
VecScatterEnd        319 1.0 1.5996e+0019.9 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecNormalize          96 1.0 7.5643e-0110.9 3.96e+07 1.0 0.0e+00 0.0e+00 9.6e+01  0  0  0  0 35   0  0  0  0 38 20482
SFSetGraph             5 1.0 1.1790e-03 1.7 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
SFSetUp                5 1.0 2.7982e+0075.7 0.00e+00 0.0 1.3e+04 2.6e+03 3.0e+00  1  0  3  0  1   1  0  3  0  1     0
SFReduceBegin         47 1.0 6.9706e-02 1.4 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
SFReduceEnd           47 1.0 1.2797e-04 2.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
SFPack               319 1.0 1.1086e-02 4.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
SFUnpack             319 1.0 1.7924e-0317.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix    12             12    847529700     0.
       Krylov Solver     5              5        72264     0.
      Preconditioner     5              5         6024     0.
              Viewer     2              1          848     0.
              Vector    85             85    201668952     0.
           Index Set    15             15     10490528     0.
   IS L to G Mapping     1              1      4850240     0.
   Star Forest Graph    11             11        12600     0.
    Distributed Mesh     3              3        15144     0.
     Discrete System     3              3         2712     0.
           Weak Form     3              3         2472     0.
========================================================================================================================
Average time to get PetscTime(): 3.25e-08
Average time for MPI_Barrier(): 0.000821221
Average time for zero size MPI_Send(): 1.34524e-06
#PETSc Option Table entries:
-ksp_max_it 200
-ksp_monitor
-ksp_type fgmres
-log_view
-m 2000
-mg_coarse_ksp_converged_reason
-mg_coarse_ksp_monitor
-mg_coarse_ksp_type gmres
-mg_coarse_pc_type hypre
-mg_levels_pc_type asm
-mg_levels_sub_pc_factor_mat_solver_type mumps
-mg_levels_sub_pc_type lu
-n 2000
-nw
-ordo 9
-pc_mg_type multiplicative
-pc_type mg
#End of PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: --prefix=/home/a/asarkar/sudhipv/software/FreeFem-sources/install/ff-petsc/r MAKEFLAGS= --with-debugging=0 COPTFLAGS="-O3 -mtune=native" CXXOPTFLAGS="-O3 -mtune=native" FOPTFLAGS="-O3 -mtune=native" --with-cxx-dialect=C++11 --with-ssl=0 --with-x=0 --with-fortran-bindings=0 --with-cudac=0 --with-cc=/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Compiler/intel2020/openmpi/4.0.3/bin/mpicc --with-cxx=/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Compiler/intel2020/openmpi/4.0.3/bin/mpic++ --with-fc=/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Compiler/intel2020/openmpi/4.0.3/bin/mpif90 --with-scalar-type=real --with-blaslapack-include=/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/imkl/2020.1.217/mkl/include --with-blaslapack-lib="-Wl,-rpath,/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/imkl/2020.1.217/mkl/lib/intel64 -L/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/imkl/2020.1.217/mkl/lib/intel64 -lmkl_rt -lmkl_intel_thread -lmkl_core  -liomp5 -lpthread" --download-metis --download-ptscotch --download-hypre --download-parmetis --download-mmg --download-parmmg --download-superlu --download-suitesparse --download-tetgen --download-slepc --download-hpddm --download-scalapack --download-mumps --download-slepc-configure-arguments=--download-arpack=https://github.com/prj-/arpack-ng/archive/b64dccb.tar.gz PETSC_ARCH=fr
-----------------------------------------
Libraries compiled on 2021-05-02 15:39:49 on nia-login03.scinet.local 
Machine characteristics: Linux-3.10.0-957.27.2.el7.x86_64-x86_64-with-centos-7.6.1810-Core
Using PETSc directory: /home/a/asarkar/sudhipv/software/FreeFem-sources/install/ff-petsc/r
Using PETSc arch: 
-----------------------------------------

Using C compiler: /cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Compiler/intel2020/openmpi/4.0.3/bin/mpicc  -fPIC -wd1572 -O3 -mtune=native   
Using Fortran compiler: /cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Compiler/intel2020/openmpi/4.0.3/bin/mpif90  -fPIC -O3 -mtune=native     
-----------------------------------------

Using include paths: -I/home/a/asarkar/sudhipv/software/FreeFem-sources/install/ff-petsc/r/include -I/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/imkl/2020.1.217/mkl/include
-----------------------------------------

Using C linker: /cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Compiler/intel2020/openmpi/4.0.3/bin/mpicc
Using Fortran linker: /cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Compiler/intel2020/openmpi/4.0.3/bin/mpif90
Using libraries: -Wl,-rpath,/home/a/asarkar/sudhipv/software/FreeFem-sources/install/ff-petsc/r/lib -L/home/a/asarkar/sudhipv/software/FreeFem-sources/install/ff-petsc/r/lib -lpetsc -Wl,-rpath,/home/a/asarkar/sudhipv/software/FreeFem-sources/install/ff-petsc/r/lib -L/home/a/asarkar/sudhipv/software/FreeFem-sources/install/ff-petsc/r/lib -Wl,-rpath,/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/imkl/2020.1.217/mkl/lib/intel64 -L/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/imkl/2020.1.217/mkl/lib/intel64 -Wl,-rpath,/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Compiler/intel2020/openmpi/4.0.3/lib -L/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Compiler/intel2020/openmpi/4.0.3/lib -Wl,-rpath,/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/libfabric/1.10.1/lib -L/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/libfabric/1.10.1/lib -Wl,-rpath,/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/ucx/1.8.0/lib -L/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/ucx/1.8.0/lib -Wl,-rpath,/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/imkl/2020.1.217/lib/intel64 -L/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/imkl/2020.1.217/lib/intel64 -Wl,-rpath,/cvmfs/restricted.computecanada.ca/easybuild/software/2020/Core/intel/2020.1.217/compilers_and_libraries_2020.1.217/linux/tbb/lib/intel64/gcc4.8 -L/cvmfs/restricted.computecanada.ca/easybuild/software/2020/Core/intel/2020.1.217/compilers_and_libraries_2020.1.217/linux/tbb/lib/intel64/gcc4.8 -Wl,-rpath,/cvmfs/restricted.computecanada.ca/easybuild/software/2020/Core/intel/2020.1.217/compilers_and_libraries_2020.1.217/linux/compiler/lib/intel64_lin -L/cvmfs/restricted.computecanada.ca/easybuild/software/2020/Core/intel/2020.1.217/compilers_and_libraries_2020.1.217/linux/compiler/lib/intel64_lin -Wl,-rpath,/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Compiler/intel2020/openmpi/4.0.3/lib64 -L/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Compiler/intel2020/openmpi/4.0.3/lib64 -Wl,-rpath,/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/gcccore/9.3.0/lib/gcc/x86_64-pc-linux-gnu/9.3.0 -L/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/gcccore/9.3.0/lib/gcc/x86_64-pc-linux-gnu/9.3.0 -Wl,-rpath,/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/gcccore/9.3.0/lib64 -L/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/gcccore/9.3.0/lib64 -Wl,-rpath,/cvmfs/soft.computecanada.ca/gentoo/2020/lib64 -L/cvmfs/soft.computecanada.ca/gentoo/2020/lib64 -Wl,-rpath,/cvmfs/soft.computecanada.ca/gentoo/2020/usr/lib64 -L/cvmfs/soft.computecanada.ca/gentoo/2020/usr/lib64 -Wl,-rpath,/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/gcccore/9.3.0/lib -L/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/gcccore/9.3.0/lib -Wl,-rpath,/cvmfs/soft.computecanada.ca/gentoo/2020/lib -L/cvmfs/soft.computecanada.ca/gentoo/2020/lib -Wl,-rpath,/cvmfs/soft.computecanada.ca/gentoo/2020/usr/lib -L/cvmfs/soft.computecanada.ca/gentoo/2020/usr/lib -lHYPRE -lcmumps -ldmumps -lsmumps -lzmumps -lmumps_common -lpord -lscalapack -lumfpack -lklu -lcholmod -lbtf -lccolamd -lcolamd -lcamd -lamd -lsuitesparseconfig -lsuperlu -lmkl_rt -lmkl_intel_thread -lmkl_core -liomp5 -lpthread -lparmmg -lmmg -lmmg3d -lptesmumps -lptscotchparmetis -lptscotch -lptscotcherr -lesmumps -lscotch -lscotcherr -lpthread -lparmetis -lmetis -ltet -lstdc++ -ldl -lmpi_usempif08 -lmpi_usempi_ignore_tkr -lmpi_mpifh -lmpi -lifport -lifcoremt_pic -limf -lsvml -lm -lipgo -lirc -lpthread -lgcc_s -lirc_s -lrt -lquadmath -lstdc++ -ldl
-----------------------------------------

#############################################################################################################
#############################################################################################################
########     Finsihed Code             #################################################
#############################################################################################################
#############################################################################################################

scontrol show jobid 7014904
JobId=7014904 JobName=poissonss_weak_ord_3-9
   UserId=sudhipv(3061414) GroupId=asarkar(6001189) MCS_label=N/A
   Priority=2100762 Nice=0 Account=def-asarkar QOS=normal
   JobState=COMPLETED Reason=None Dependency=(null)
   Requeue=0 Restarts=0 BatchFlag=1 Reboot=0 ExitCode=0:0
   RunTime=00:07:16 TimeLimit=00:45:00 TimeMin=N/A
   SubmitTime=2022-03-05T06:17:11 EligibleTime=2022-03-05T06:17:11
   AccrueTime=2022-03-05T06:17:11
   StartTime=2022-03-05T06:21:45 EndTime=2022-03-05T06:29:01 Deadline=N/A
   SuspendTime=None SecsPreSuspend=0 LastSchedEval=2022-03-05T06:21:45
   Partition=compute AllocNode:Sid=nia-login02:258025
   ReqNodeList=(null) ExcNodeList=(null)
   NodeList=nia[0032-0033,0175,0275,0385,0465,1037,1088,1107,1120]
   BatchHost=nia0032
   NumNodes=10 NumCPUs=800 NumTasks=400 CPUs/Task=1 ReqB:S:C:T=0:0:*:*
   TRES=cpu=800,mem=1750000M,node=10,billing=400
   Socks/Node=* NtasksPerN:B:S:C=40:0:*:* CoreSpec=*
   MinCPUsNode=40 MinMemoryNode=175000M MinTmpDiskNode=0
   Features=[skylake|cascade] DelayBoot=00:00:00
   OverSubscribe=NO Contiguous=0 Licenses=(null) Network=(null)
   Command=/gpfs/fs0/scratch/a/asarkar/sudhipv/sto_poisson/weak_ss/runff_niagara_ss.sh
   WorkDir=/gpfs/fs0/scratch/a/asarkar/sudhipv/sto_poisson/weak_ss
   StdErr=/gpfs/fs0/scratch/a/asarkar/sudhipv/sto_poisson/weak_ss/poissonss_weak_ord_3-9-7014904.out
   StdIn=/dev/null
   StdOut=/gpfs/fs0/scratch/a/asarkar/sudhipv/sto_poisson/weak_ss/poissonss_weak_ord_3-9-7014904.out
   Power=
   MailUser=sudhipv@cmail.carleton.ca MailType=BEGIN,END,FAIL,REQUEUE,STAGE_OUT

sacct -j 7014904
       JobID    JobName    Account    Elapsed  MaxVMSize     MaxRSS  SystemCPU    UserCPU ExitCode 
------------ ---------- ---------- ---------- ---------- ---------- ---------- ---------- -------- 
7014904      poissonss+ def-asark+   00:07:16                         01:08:15 1-08:28:03      0:0 
7014904.bat+      batch def-asark+   00:07:16 213165800K 168819252K  08:32.775   04:15:28      0:0 
7014904.ext+     extern def-asark+   00:07:16    138360K      1064K  00:00.001  00:00.006      0:0 
7014904.0         orted def-asark+   00:00:07    395280K      3000K  00:01.371  00:01.134      0:0 
7014904.1         orted def-asark+   00:01:08 117858652K  85511716K  04:00.316   02:06:35      0:0 
7014904.2         orted def-asark+   00:00:01    395280K      3000K  00:01.378  00:01.129      0:0 
7014904.3         orted def-asark+   00:01:27 149150292K 111848416K  09:02.890   04:28:52      0:0 
7014904.4         orted def-asark+   00:00:01    395280K      2996K  00:01.352  00:01.147      0:0 
7014904.5         orted def-asark+   00:01:48 186019772K 142689924K  17:16.814   07:46:14      0:0 
7014904.6         orted def-asark+   00:00:02    395280K      3000K  00:01.398  00:01.109      0:0 
7014904.7         orted def-asark+   00:02:29 215358004K 169833388K  29:17.516   13:50:47      0:0 

kernel messages produced during job executions:
[Mar 5 05:48] mlx5_core 0000:06:00.0: mlx5_core_get_rsc:63:(pid 0): Async event for bogus resource 0x910f
[Mar 5 05:49] mlx5_core 0000:06:00.0: mlx5_core_get_rsc:63:(pid 0): Async event for bogus resource 0x9371
[Mar 5 05:50] mlx5_core 0000:06:00.0: mlx5_core_get_rsc:63:(pid 0): Async event for bogus resource 0x95b1
[Mar 5 05:52] mlx5_core 0000:06:00.0: mlx5_core_get_rsc:63:(pid 0): Async event for bogus resource 0x97f5
