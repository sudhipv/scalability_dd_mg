/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Compiler/intel2020/openmpi/4.0.3/bin/mpiexec
/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Compiler/intel2020/openmpi/4.0.3/bin/mpirun
Number of Vertices for coarse mesh in process  0  8003241
0.01 steps with 10 Number of Time steps for total time of 0.1s
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
Linear solve converged due to CONVERGED_RTOL iterations 7
Finished time step 1 Time0.01
Time for 1 timestep 27.6743
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
Linear solve converged due to CONVERGED_RTOL iterations 7
Finished time step 2 Time0.02
Time for 1 timestep 25.063
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
Linear solve converged due to CONVERGED_RTOL iterations 7
Finished time step 3 Time0.03
Time for 1 timestep 24.8988
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
Linear solve converged due to CONVERGED_RTOL iterations 7
Finished time step 4 Time0.04
Time for 1 timestep 25.284
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
Linear solve converged due to CONVERGED_RTOL iterations 7
Finished time step 5 Time0.05
Time for 1 timestep 25.0622
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
Linear solve converged due to CONVERGED_RTOL iterations 7
Finished time step 6 Time0.06
Time for 1 timestep 24.9085
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
Linear solve converged due to CONVERGED_RTOL iterations 7
Finished time step 7 Time0.07
Time for 1 timestep 25.0146
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
Linear solve converged due to CONVERGED_RTOL iterations 7
Finished time step 8 Time0.08
Time for 1 timestep 24.9112
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
Linear solve converged due to CONVERGED_RTOL iterations 7
Finished time step 9 Time0.09
Time for 1 timestep 25.0835
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
Linear solve converged due to CONVERGED_RTOL iterations 7
Finished time step 10 Time0.1
Time for 1 timestep 24.8173
total time taken is288.373
************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

/home/a/asarkar/sudhipv/software/FreeFem-sources/src/mpi/FreeFem++-mpi on a  named nia0309.scinet.local with 80 processors, by sudhipv Thu Oct 21 15:44:28 2021
Using Petsc Release Version 3.15.0, Mar 30, 2021 

                         Max       Max/Min     Avg       Total
Time (sec):           2.890e+02     1.000   2.890e+02
Objects:              2.130e+02     1.000   2.130e+02
Flop:                 1.716e+11     1.322   1.508e+11  1.206e+13
Flop/sec:             5.938e+08     1.322   5.217e+08  4.173e+10
MPI Messages:         9.058e+03     3.500   6.470e+03  5.176e+05
MPI Message Lengths:  4.139e+07     3.093   5.159e+03  2.670e+09
MPI Reductions:       1.146e+03     1.000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flop
                            and VecAXPY() for complex vectors of length N --> 8N flop

Summary of Stages:   ----- Time ------  ----- Flop ------  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total    Count   %Total     Avg         %Total    Count   %Total
 0:      Main Stage: 2.8897e+02 100.0%  1.2060e+13 100.0%  5.176e+05 100.0%  5.159e+03      100.0%  1.128e+03  98.4%

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flop: Max - maximum over all processors
                  Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   AvgLen: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flop in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flop over all processors)/(max time over all processors)
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flop                              --- Global ---  --- Stage ----  Total
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   AvgLen  Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s
------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

BuildTwoSided         25 1.0 1.0305e+01 3.8 0.00e+00 0.0 1.2e+03 4.0e+00 2.3e+01  3  0  0  0  2   3  0  0  0  2     0
BuildTwoSidedF        20 1.0 7.8084e+0015.4 0.00e+00 0.0 0.0e+00 0.0e+00 2.0e+01  2  0  0  0  2   2  0  0  0  2     0
MatMult              800 1.0 7.4633e+00 1.3 2.72e+09 1.0 3.2e+05 3.6e+03 2.0e+00  2  2 62 44  0   2  2 62 44  0 28980
MatMultAdd            70 1.0 1.1502e+00 1.2 5.62e+07 1.0 2.8e+04 6.3e+03 0.0e+00  0  0  5  7  0   0  0  5  7  0  3895
MatMultTranspose      70 1.0 1.4186e+00 1.8 0.00e+00 0.0 2.8e+04 2.1e+04 0.0e+00  0  0  5 22  0   0  0  5 22  0     0
MatSolve             390 1.0 4.2775e+01 1.1 3.22e+10 1.1 0.0e+00 0.0e+00 0.0e+00 14 21  0  0  0  14 21  0  0  0 58292
MatLUFactorSym         1 1.0 8.1706e-01 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatLUFactorNum        10 1.0 1.1969e+02 1.4 1.34e+11 1.4 0.0e+00 0.0e+00 0.0e+00 37 76  0  0  0  37 76  0  0  0 76299
MatConvert            10 1.0 3.1394e-01 1.7 0.00e+00 0.0 0.0e+00 0.0e+00 4.0e+01  0  0  0  0  3   0  0  0  0  4     0
MatResidual           70 1.0 1.1122e+00 1.8 3.94e+08 1.0 2.8e+04 4.3e+03 0.0e+00  0  0  5  5  0   0  0  5  5  0 28192
MatAssemblyBegin      42 1.0 7.8092e+0015.4 0.00e+00 0.0 0.0e+00 0.0e+00 2.0e+01  2  0  0  0  2   2  0  0  0  2     0
MatAssemblyEnd        42 1.0 9.7867e-01 4.6 0.00e+00 0.0 0.0e+00 0.0e+00 4.8e+01  0  0  0  0  4   0  0  0  0  4     0
MatGetRowIJ           21 1.0 2.4020e-02 1.3 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatCreateSubMats      10 1.0 7.1882e-01 1.0 0.00e+00 0.0 5.6e+03 2.3e+04 1.0e+00  0  0  1  5  0   0  0  1  5  0     0
MatGetOrdering         1 1.0 3.1223e-01 1.3 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatIncreaseOvrlp       1 1.0 5.1291e-02 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatZeroEntries        18 1.0 4.4212e-02 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
KSPSetUp              21 1.0 1.3859e+02 1.0 1.45e+11 1.4 9.1e+04 5.4e+03 2.2e+02 48 83 18 18 19  48 83 18 18 20 71906
KSPSolve              10 1.0 6.1991e+01 1.0 2.69e+10 1.1 4.2e+05 4.9e+03 8.2e+02 21 17 81 78 71  21 17 81 78 73 33790
KSPGMRESOrthog       450 1.0 5.0344e+00 2.0 1.62e+09 1.0 0.0e+00 0.0e+00 4.5e+02  1  1  0  0 39   1  1  0  0 40 25555
PCSetUp               19 1.0 1.4180e+02 1.0 1.45e+11 1.4 9.1e+04 5.4e+03 2.7e+02 49 83 18 18 23  49 83 18 18 24 70275
PCSetUpOnBlocks      150 1.0 1.3213e+01 1.4 1.34e+10 1.4 0.0e+00 0.0e+00 0.0e+00  4  8  0  0  0   4  8  0  0  0 69116
PCApply               70 1.0 6.0263e+01 1.0 2.59e+10 1.1 3.9e+05 5.0e+03 6.7e+02 21 17 76 73 58  21 17 76 73 59 33469
PCApplyOnBlocks      390 1.0 1.5035e+02 1.3 1.53e+11 1.3 0.0e+00 0.0e+00 0.0e+00 48 89  0  0  0  48 89  0  0  0 71250
VecMDot              450 1.0 4.0370e+00 2.8 8.08e+08 1.0 0.0e+00 0.0e+00 4.5e+02  1  1  0  0 39   1  1  0  0 40 15934
VecNorm              540 1.0 3.1836e+0122.2 2.23e+08 1.0 0.0e+00 0.0e+00 5.4e+02  4  0  0  0 47   4  0  0  0 48   558
VecScale             680 1.0 1.6297e-01 1.1 1.12e+08 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0 54498
VecCopy              300 1.0 3.5504e-01 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecSet              1846 1.0 1.2339e+00 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecAXPY              160 1.0 2.4885e-01 1.3 8.64e+07 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0 27650
VecAYPX              420 1.0 5.9972e-01 1.4 2.25e+08 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0 29882
VecAXPBYCZ           140 1.0 3.1683e-01 1.3 2.81e+08 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0 70703
VecMAXPY             540 1.0 1.2615e+00 1.0 1.00e+09 1.0 0.0e+00 0.0e+00 0.0e+00  0  1  0  0  0   0  1  0  0  0 63171
VecScatterBegin     2290 1.0 5.6925e+00 2.4 0.00e+00 0.0 4.5e+05 3.7e+03 3.0e+00  1  0 87 62  0   1  0 87 62  0     0
VecScatterEnd       2290 1.0 2.0252e+0022.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecNormalize         460 1.0 3.1842e+0122.1 2.38e+08 1.0 0.0e+00 0.0e+00 4.6e+02  4  0  0  0 40   4  0  0  0 41   596
SFSetGraph             5 1.0 1.2322e-03 1.8 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
SFSetUp                5 1.0 3.6396e+0075.2 0.00e+00 0.0 2.4e+03 8.8e+02 3.0e+00  1  0  0  0  0   1  0  0  0  0     0
SFReduceBegin        390 1.0 5.9138e-01 1.3 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
SFReduceEnd          390 1.0 9.5282e-04 1.7 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
SFPack              2290 1.0 4.1227e-02 3.5 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
SFUnpack            2290 1.0 7.8248e-03 9.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix    21             21    664613404     0.
       Krylov Solver     5              5        72264     0.
      Preconditioner     5              5         6024     0.
              Vector   146            146    340709840     0.
           Index Set    13             13      9658272     0.
   IS L to G Mapping     1              1      4816112     0.
   Star Forest Graph    11             11        12600     0.
    Distributed Mesh     3              3        15144     0.
     Discrete System     3              3         2712     0.
           Weak Form     3              3         2472     0.
              Viewer     2              1          848     0.
========================================================================================================================
Average time to get PetscTime(): 4.54e-08
Average time for MPI_Barrier(): 5.56022e-05
Average time for zero size MPI_Send(): 1.2094e-06
#PETSc Option Table entries:
-ksp_converged_reason
-ksp_pc_side right
-ksp_rtol 1e-8
-ksp_type fgmres
-log_view
-m 2828
-mg_coarse_ksp_converged_reason
-mg_coarse_ksp_type gmres
-mg_coarse_pc_type hypre
-mg_levels_pc_type asm
-mg_levels_sub_pc_type lu
-n 2828
-nw
-pc_mg_type multiplicative
-pc_type mg
#End of PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: --prefix=/home/a/asarkar/sudhipv/software/FreeFem-sources/install/ff-petsc/r MAKEFLAGS= --with-debugging=0 COPTFLAGS="-O3 -mtune=native" CXXOPTFLAGS="-O3 -mtune=native" FOPTFLAGS="-O3 -mtune=native" --with-cxx-dialect=C++11 --with-ssl=0 --with-x=0 --with-fortran-bindings=0 --with-cudac=0 --with-cc=/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Compiler/intel2020/openmpi/4.0.3/bin/mpicc --with-cxx=/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Compiler/intel2020/openmpi/4.0.3/bin/mpic++ --with-fc=/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Compiler/intel2020/openmpi/4.0.3/bin/mpif90 --with-scalar-type=real --with-blaslapack-include=/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/imkl/2020.1.217/mkl/include --with-blaslapack-lib="-Wl,-rpath,/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/imkl/2020.1.217/mkl/lib/intel64 -L/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/imkl/2020.1.217/mkl/lib/intel64 -lmkl_rt -lmkl_intel_thread -lmkl_core  -liomp5 -lpthread" --download-metis --download-ptscotch --download-hypre --download-parmetis --download-mmg --download-parmmg --download-superlu --download-suitesparse --download-tetgen --download-slepc --download-hpddm --download-scalapack --download-mumps --download-slepc-configure-arguments=--download-arpack=https://github.com/prj-/arpack-ng/archive/b64dccb.tar.gz PETSC_ARCH=fr
-----------------------------------------
Libraries compiled on 2021-05-02 15:39:49 on nia-login03.scinet.local 
Machine characteristics: Linux-3.10.0-957.27.2.el7.x86_64-x86_64-with-centos-7.6.1810-Core
Using PETSc directory: /home/a/asarkar/sudhipv/software/FreeFem-sources/install/ff-petsc/r
Using PETSc arch: 
-----------------------------------------

Using C compiler: /cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Compiler/intel2020/openmpi/4.0.3/bin/mpicc  -fPIC -wd1572 -O3 -mtune=native   
Using Fortran compiler: /cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Compiler/intel2020/openmpi/4.0.3/bin/mpif90  -fPIC -O3 -mtune=native     
-----------------------------------------

Using include paths: -I/home/a/asarkar/sudhipv/software/FreeFem-sources/install/ff-petsc/r/include -I/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/imkl/2020.1.217/mkl/include
-----------------------------------------

Using C linker: /cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Compiler/intel2020/openmpi/4.0.3/bin/mpicc
Using Fortran linker: /cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Compiler/intel2020/openmpi/4.0.3/bin/mpif90
Using libraries: -Wl,-rpath,/home/a/asarkar/sudhipv/software/FreeFem-sources/install/ff-petsc/r/lib -L/home/a/asarkar/sudhipv/software/FreeFem-sources/install/ff-petsc/r/lib -lpetsc -Wl,-rpath,/home/a/asarkar/sudhipv/software/FreeFem-sources/install/ff-petsc/r/lib -L/home/a/asarkar/sudhipv/software/FreeFem-sources/install/ff-petsc/r/lib -Wl,-rpath,/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/imkl/2020.1.217/mkl/lib/intel64 -L/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/imkl/2020.1.217/mkl/lib/intel64 -Wl,-rpath,/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Compiler/intel2020/openmpi/4.0.3/lib -L/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Compiler/intel2020/openmpi/4.0.3/lib -Wl,-rpath,/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/libfabric/1.10.1/lib -L/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/libfabric/1.10.1/lib -Wl,-rpath,/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/ucx/1.8.0/lib -L/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/ucx/1.8.0/lib -Wl,-rpath,/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/imkl/2020.1.217/lib/intel64 -L/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/imkl/2020.1.217/lib/intel64 -Wl,-rpath,/cvmfs/restricted.computecanada.ca/easybuild/software/2020/Core/intel/2020.1.217/compilers_and_libraries_2020.1.217/linux/tbb/lib/intel64/gcc4.8 -L/cvmfs/restricted.computecanada.ca/easybuild/software/2020/Core/intel/2020.1.217/compilers_and_libraries_2020.1.217/linux/tbb/lib/intel64/gcc4.8 -Wl,-rpath,/cvmfs/restricted.computecanada.ca/easybuild/software/2020/Core/intel/2020.1.217/compilers_and_libraries_2020.1.217/linux/compiler/lib/intel64_lin -L/cvmfs/restricted.computecanada.ca/easybuild/software/2020/Core/intel/2020.1.217/compilers_and_libraries_2020.1.217/linux/compiler/lib/intel64_lin -Wl,-rpath,/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Compiler/intel2020/openmpi/4.0.3/lib64 -L/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Compiler/intel2020/openmpi/4.0.3/lib64 -Wl,-rpath,/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/gcccore/9.3.0/lib/gcc/x86_64-pc-linux-gnu/9.3.0 -L/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/gcccore/9.3.0/lib/gcc/x86_64-pc-linux-gnu/9.3.0 -Wl,-rpath,/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/gcccore/9.3.0/lib64 -L/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/gcccore/9.3.0/lib64 -Wl,-rpath,/cvmfs/soft.computecanada.ca/gentoo/2020/lib64 -L/cvmfs/soft.computecanada.ca/gentoo/2020/lib64 -Wl,-rpath,/cvmfs/soft.computecanada.ca/gentoo/2020/usr/lib64 -L/cvmfs/soft.computecanada.ca/gentoo/2020/usr/lib64 -Wl,-rpath,/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/gcccore/9.3.0/lib -L/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/gcccore/9.3.0/lib -Wl,-rpath,/cvmfs/soft.computecanada.ca/gentoo/2020/lib -L/cvmfs/soft.computecanada.ca/gentoo/2020/lib -Wl,-rpath,/cvmfs/soft.computecanada.ca/gentoo/2020/usr/lib -L/cvmfs/soft.computecanada.ca/gentoo/2020/usr/lib -lHYPRE -lcmumps -ldmumps -lsmumps -lzmumps -lmumps_common -lpord -lscalapack -lumfpack -lklu -lcholmod -lbtf -lccolamd -lcolamd -lcamd -lamd -lsuitesparseconfig -lsuperlu -lmkl_rt -lmkl_intel_thread -lmkl_core -liomp5 -lpthread -lparmmg -lmmg -lmmg3d -lptesmumps -lptscotchparmetis -lptscotch -lptscotcherr -lesmumps -lscotch -lscotcherr -lpthread -lparmetis -lmetis -ltet -lstdc++ -ldl -lmpi_usempif08 -lmpi_usempi_ignore_tkr -lmpi_mpifh -lmpi -lifport -lifcoremt_pic -limf -lsvml -lm -lipgo -lirc -lpthread -lgcc_s -lirc_s -lrt -lquadmath -lstdc++ -ldl
-----------------------------------------

#############################################################################################################
#############################################################################################################
########     Finsihed Code with NL 80 Processors             #################################################
#############################################################################################################
#############################################################################################################
Number of Vertices for coarse mesh in process  0  8003241
0.01 steps with 10 Number of Time steps for total time of 0.1s
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
Linear solve converged due to CONVERGED_RTOL iterations 7
Finished time step 1 Time0.01
Time for 1 timestep 11.6555
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
Linear solve converged due to CONVERGED_RTOL iterations 7
Finished time step 2 Time0.02
Time for 1 timestep 10.3979
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
Linear solve converged due to CONVERGED_RTOL iterations 7
Finished time step 3 Time0.03
Time for 1 timestep 10.0996
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
Linear solve converged due to CONVERGED_RTOL iterations 7
Finished time step 4 Time0.04
Time for 1 timestep 10.0714
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
Linear solve converged due to CONVERGED_RTOL iterations 7
Finished time step 5 Time0.05
Time for 1 timestep 10.1066
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
Linear solve converged due to CONVERGED_RTOL iterations 7
Finished time step 6 Time0.06
Time for 1 timestep 10.0719
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
Linear solve converged due to CONVERGED_RTOL iterations 7
Finished time step 7 Time0.07
Time for 1 timestep 10.0886
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
Linear solve converged due to CONVERGED_RTOL iterations 7
Finished time step 8 Time0.08
Time for 1 timestep 10.0637
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
Linear solve converged due to CONVERGED_RTOL iterations 7
Finished time step 9 Time0.09
Time for 1 timestep 10.0489
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
Linear solve converged due to CONVERGED_RTOL iterations 7
Finished time step 10 Time0.1
Time for 1 timestep 10.1259
total time taken is132.971
************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

/home/a/asarkar/sudhipv/software/FreeFem-sources/src/mpi/FreeFem++-mpi on a  named nia0309.scinet.local with 160 processors, by sudhipv Thu Oct 21 15:46:49 2021
Using Petsc Release Version 3.15.0, Mar 30, 2021 

                         Max       Max/Min     Avg       Total
Time (sec):           1.330e+02     1.000   1.330e+02
Objects:              2.130e+02     1.000   2.130e+02
Flop:                 6.619e+10     1.309   5.633e+10  9.013e+12
Flop/sec:             4.976e+08     1.309   4.234e+08  6.775e+10
MPI Messages:         1.035e+04     8.000   6.858e+03  1.097e+06
MPI Message Lengths:  2.997e+07     3.205   3.529e+03  3.872e+09
MPI Reductions:       1.146e+03     1.000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flop
                            and VecAXPY() for complex vectors of length N --> 8N flop

Summary of Stages:   ----- Time ------  ----- Flop ------  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total    Count   %Total     Avg         %Total    Count   %Total
 0:      Main Stage: 1.3303e+02 100.0%  9.0129e+12 100.0%  1.097e+06 100.0%  3.529e+03      100.0%  1.128e+03  98.4%

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flop: Max - maximum over all processors
                  Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   AvgLen: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flop in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flop over all processors)/(max time over all processors)
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flop                              --- Global ---  --- Stage ----  Total
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   AvgLen  Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s
------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

BuildTwoSided         25 1.0 4.6935e+00 5.3 0.00e+00 0.0 2.5e+03 4.0e+00 2.3e+01  3  0  0  0  2   3  0  0  0  2     0
BuildTwoSidedF        20 1.0 3.9571e+00 9.8 0.00e+00 0.0 0.0e+00 0.0e+00 2.0e+01  3  0  0  0  2   3  0  0  0  2     0
MatMult              800 1.0 3.4848e+00 1.3 1.36e+09 1.0 6.8e+05 2.5e+03 2.0e+00  2  2 62 44  0   2  2 62 44  0 62066
MatMultAdd            70 1.0 5.9347e-01 1.4 2.82e+07 1.0 5.9e+04 4.3e+03 0.0e+00  0  0  5  7  0   0  0  5  7  0  7549
MatMultTranspose      70 1.0 6.8544e-01 2.2 0.00e+00 0.0 5.9e+04 1.5e+04 0.0e+00  0  0  5 22  0   0  0  5 22  0     0
MatSolve             390 1.0 2.0712e+01 1.2 1.51e+10 1.1 0.0e+00 0.0e+00 0.0e+00 14 25  0  0  0  14 25  0  0  0 110797
MatLUFactorSym         1 1.0 3.6580e-01 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatLUFactorNum        10 1.0 3.9385e+01 1.4 4.84e+10 1.4 0.0e+00 0.0e+00 0.0e+00 26 70  0  0  0  26 70  0  0  0 159549
MatConvert            10 1.0 1.4995e-01 1.5 0.00e+00 0.0 0.0e+00 0.0e+00 4.0e+01  0  0  0  0  3   0  0  0  0  4     0
MatResidual           70 1.0 5.2653e-01 2.3 1.97e+08 1.0 5.9e+04 2.9e+03 0.0e+00  0  0  5  5  0   0  0  5  5  0 59550
MatAssemblyBegin      42 1.0 3.9576e+00 9.8 0.00e+00 0.0 0.0e+00 0.0e+00 2.0e+01  3  0  0  0  2   3  0  0  0  2     0
MatAssemblyEnd        42 1.0 5.3474e-01 4.1 0.00e+00 0.0 0.0e+00 0.0e+00 4.8e+01  0  0  0  0  4   0  0  0  0  4     0
MatGetRowIJ           21 1.0 1.0920e-02 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatCreateSubMats      10 1.0 4.0564e-01 1.1 0.00e+00 0.0 1.2e+04 1.6e+04 1.0e+00  0  0  1  5  0   0  0  1  5  0     0
MatGetOrdering         1 1.0 1.3613e-01 1.3 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatIncreaseOvrlp       1 1.0 3.7071e-02 1.6 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatZeroEntries        18 1.0 2.3215e-02 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
KSPSetUp              21 1.0 4.8256e+01 1.0 5.34e+10 1.4 1.9e+05 3.7e+03 2.2e+02 36 78 18 18 19  36 78 18 18 20 146318
KSPSolve              10 1.0 2.8866e+01 1.0 1.28e+10 1.1 8.9e+05 3.4e+03 8.2e+02 22 22 81 78 71  22 22 81 78 73 67626
KSPGMRESOrthog       450 1.0 2.3324e+00 2.5 8.10e+08 1.0 0.0e+00 0.0e+00 4.5e+02  1  1  0  0 39   1  1  0  0 40 55159
PCSetUp               19 1.0 4.9848e+01 1.0 5.34e+10 1.4 1.9e+05 3.7e+03 2.7e+02 37 78 18 18 23  37 78 18 18 24 141644
PCSetUpOnBlocks      150 1.0 4.4009e+00 1.3 4.84e+09 1.4 0.0e+00 0.0e+00 0.0e+00  3  7  0  0  0   3  7  0  0  0 142786
PCApply               70 1.0 2.8282e+01 1.0 1.23e+10 1.1 8.3e+05 3.4e+03 6.7e+02 21 21 76 73 58  21 21 76 73 59 66275
PCApplyOnBlocks      390 1.0 5.6431e+01 1.3 5.86e+10 1.3 0.0e+00 0.0e+00 0.0e+00 37 88  0  0  0  37 88  0  0  0 140885
VecMDot              450 1.0 1.8966e+00 3.8 4.05e+08 1.0 0.0e+00 0.0e+00 4.5e+02  1  1  0  0 39   1  1  0  0 40 33917
VecNorm              540 1.0 9.9315e+0022.7 1.12e+08 1.0 0.0e+00 0.0e+00 5.4e+02  4  0  0  0 47   4  0  0  0 48  1789
VecScale             680 1.0 4.1624e-02 1.1 5.60e+07 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0 213372
VecCopy              300 1.0 1.6203e-01 1.5 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecSet              1846 1.0 5.9920e-01 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecAXPY              160 1.0 9.0727e-02 2.0 4.33e+07 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0 75840
VecAYPX              420 1.0 2.8390e-01 1.3 1.13e+08 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0 63124
VecAXPBYCZ           140 1.0 1.5586e-01 1.9 1.41e+08 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0 143728
VecMAXPY             540 1.0 5.5780e-01 1.0 5.02e+08 1.0 0.0e+00 0.0e+00 0.0e+00  0  1  0  0  0   0  1  0  0  0 142863
VecScatterBegin     2290 1.0 2.2018e+00 2.0 0.00e+00 0.0 9.5e+05 2.5e+03 3.0e+00  1  0 87 62  0   1  0 87 62  0     0
VecScatterEnd       2290 1.0 9.9653e-0142.5 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecNormalize         460 1.0 9.9084e+0024.1 1.20e+08 1.0 0.0e+00 0.0e+00 4.6e+02  4  0  0  0 40   4  0  0  0 41  1914
SFSetGraph             5 1.0 5.8707e-04 1.8 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
SFSetUp                5 1.0 1.0839e+0040.4 0.00e+00 0.0 5.1e+03 6.0e+02 3.0e+00  0  0  0  0  0   0  0  0  0  0     0
SFReduceBegin        390 1.0 2.8322e-01 1.6 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
SFReduceEnd          390 1.0 7.6157e-04 1.9 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
SFPack              2290 1.0 2.3131e-02 3.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
SFUnpack            2290 1.0 5.7906e-03 7.7 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix    21             21    308546476     0.
       Krylov Solver     5              5        72264     0.
      Preconditioner     5              5         6024     0.
              Vector   146            146    170097808     0.
           Index Set    13             13      4844644     0.
   IS L to G Mapping     1              1      2411180     0.
   Star Forest Graph    11             11        12600     0.
    Distributed Mesh     3              3        15144     0.
     Discrete System     3              3         2712     0.
           Weak Form     3              3         2472     0.
              Viewer     2              1          848     0.
========================================================================================================================
Average time to get PetscTime(): 3.34e-08
Average time for MPI_Barrier(): 6.24304e-05
Average time for zero size MPI_Send(): 1.2811e-06
#PETSc Option Table entries:
-ksp_converged_reason
-ksp_pc_side right
-ksp_rtol 1e-8
-ksp_type fgmres
-log_view
-m 2828
-mg_coarse_ksp_converged_reason
-mg_coarse_ksp_type gmres
-mg_coarse_pc_type hypre
-mg_levels_pc_type asm
-mg_levels_sub_pc_type lu
-n 2828
-nw
-pc_mg_type multiplicative
-pc_type mg
#End of PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: --prefix=/home/a/asarkar/sudhipv/software/FreeFem-sources/install/ff-petsc/r MAKEFLAGS= --with-debugging=0 COPTFLAGS="-O3 -mtune=native" CXXOPTFLAGS="-O3 -mtune=native" FOPTFLAGS="-O3 -mtune=native" --with-cxx-dialect=C++11 --with-ssl=0 --with-x=0 --with-fortran-bindings=0 --with-cudac=0 --with-cc=/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Compiler/intel2020/openmpi/4.0.3/bin/mpicc --with-cxx=/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Compiler/intel2020/openmpi/4.0.3/bin/mpic++ --with-fc=/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Compiler/intel2020/openmpi/4.0.3/bin/mpif90 --with-scalar-type=real --with-blaslapack-include=/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/imkl/2020.1.217/mkl/include --with-blaslapack-lib="-Wl,-rpath,/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/imkl/2020.1.217/mkl/lib/intel64 -L/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/imkl/2020.1.217/mkl/lib/intel64 -lmkl_rt -lmkl_intel_thread -lmkl_core  -liomp5 -lpthread" --download-metis --download-ptscotch --download-hypre --download-parmetis --download-mmg --download-parmmg --download-superlu --download-suitesparse --download-tetgen --download-slepc --download-hpddm --download-scalapack --download-mumps --download-slepc-configure-arguments=--download-arpack=https://github.com/prj-/arpack-ng/archive/b64dccb.tar.gz PETSC_ARCH=fr
-----------------------------------------
Libraries compiled on 2021-05-02 15:39:49 on nia-login03.scinet.local 
Machine characteristics: Linux-3.10.0-957.27.2.el7.x86_64-x86_64-with-centos-7.6.1810-Core
Using PETSc directory: /home/a/asarkar/sudhipv/software/FreeFem-sources/install/ff-petsc/r
Using PETSc arch: 
-----------------------------------------

Using C compiler: /cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Compiler/intel2020/openmpi/4.0.3/bin/mpicc  -fPIC -wd1572 -O3 -mtune=native   
Using Fortran compiler: /cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Compiler/intel2020/openmpi/4.0.3/bin/mpif90  -fPIC -O3 -mtune=native     
-----------------------------------------

Using include paths: -I/home/a/asarkar/sudhipv/software/FreeFem-sources/install/ff-petsc/r/include -I/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/imkl/2020.1.217/mkl/include
-----------------------------------------

Using C linker: /cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Compiler/intel2020/openmpi/4.0.3/bin/mpicc
Using Fortran linker: /cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Compiler/intel2020/openmpi/4.0.3/bin/mpif90
Using libraries: -Wl,-rpath,/home/a/asarkar/sudhipv/software/FreeFem-sources/install/ff-petsc/r/lib -L/home/a/asarkar/sudhipv/software/FreeFem-sources/install/ff-petsc/r/lib -lpetsc -Wl,-rpath,/home/a/asarkar/sudhipv/software/FreeFem-sources/install/ff-petsc/r/lib -L/home/a/asarkar/sudhipv/software/FreeFem-sources/install/ff-petsc/r/lib -Wl,-rpath,/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/imkl/2020.1.217/mkl/lib/intel64 -L/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/imkl/2020.1.217/mkl/lib/intel64 -Wl,-rpath,/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Compiler/intel2020/openmpi/4.0.3/lib -L/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Compiler/intel2020/openmpi/4.0.3/lib -Wl,-rpath,/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/libfabric/1.10.1/lib -L/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/libfabric/1.10.1/lib -Wl,-rpath,/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/ucx/1.8.0/lib -L/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/ucx/1.8.0/lib -Wl,-rpath,/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/imkl/2020.1.217/lib/intel64 -L/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/imkl/2020.1.217/lib/intel64 -Wl,-rpath,/cvmfs/restricted.computecanada.ca/easybuild/software/2020/Core/intel/2020.1.217/compilers_and_libraries_2020.1.217/linux/tbb/lib/intel64/gcc4.8 -L/cvmfs/restricted.computecanada.ca/easybuild/software/2020/Core/intel/2020.1.217/compilers_and_libraries_2020.1.217/linux/tbb/lib/intel64/gcc4.8 -Wl,-rpath,/cvmfs/restricted.computecanada.ca/easybuild/software/2020/Core/intel/2020.1.217/compilers_and_libraries_2020.1.217/linux/compiler/lib/intel64_lin -L/cvmfs/restricted.computecanada.ca/easybuild/software/2020/Core/intel/2020.1.217/compilers_and_libraries_2020.1.217/linux/compiler/lib/intel64_lin -Wl,-rpath,/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Compiler/intel2020/openmpi/4.0.3/lib64 -L/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Compiler/intel2020/openmpi/4.0.3/lib64 -Wl,-rpath,/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/gcccore/9.3.0/lib/gcc/x86_64-pc-linux-gnu/9.3.0 -L/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/gcccore/9.3.0/lib/gcc/x86_64-pc-linux-gnu/9.3.0 -Wl,-rpath,/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/gcccore/9.3.0/lib64 -L/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/gcccore/9.3.0/lib64 -Wl,-rpath,/cvmfs/soft.computecanada.ca/gentoo/2020/lib64 -L/cvmfs/soft.computecanada.ca/gentoo/2020/lib64 -Wl,-rpath,/cvmfs/soft.computecanada.ca/gentoo/2020/usr/lib64 -L/cvmfs/soft.computecanada.ca/gentoo/2020/usr/lib64 -Wl,-rpath,/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/gcccore/9.3.0/lib -L/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/gcccore/9.3.0/lib -Wl,-rpath,/cvmfs/soft.computecanada.ca/gentoo/2020/lib -L/cvmfs/soft.computecanada.ca/gentoo/2020/lib -Wl,-rpath,/cvmfs/soft.computecanada.ca/gentoo/2020/usr/lib -L/cvmfs/soft.computecanada.ca/gentoo/2020/usr/lib -lHYPRE -lcmumps -ldmumps -lsmumps -lzmumps -lmumps_common -lpord -lscalapack -lumfpack -lklu -lcholmod -lbtf -lccolamd -lcolamd -lcamd -lamd -lsuitesparseconfig -lsuperlu -lmkl_rt -lmkl_intel_thread -lmkl_core -liomp5 -lpthread -lparmmg -lmmg -lmmg3d -lptesmumps -lptscotchparmetis -lptscotch -lptscotcherr -lesmumps -lscotch -lscotcherr -lpthread -lparmetis -lmetis -ltet -lstdc++ -ldl -lmpi_usempif08 -lmpi_usempi_ignore_tkr -lmpi_mpifh -lmpi -lifport -lifcoremt_pic -limf -lsvml -lm -lipgo -lirc -lpthread -lgcc_s -lirc_s -lrt -lquadmath -lstdc++ -ldl
-----------------------------------------

#############################################################################################################
#############################################################################################################
########     Finsihed Code with NL 160 Processors             #################################################
#############################################################################################################
#############################################################################################################
Number of Vertices for coarse mesh in process  0  8003241
0.01 steps with 10 Number of Time steps for total time of 0.1s
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 5
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
Linear solve converged due to CONVERGED_RTOL iterations 7
Finished time step 1 Time0.01
Time for 1 timestep 6.87003
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 5
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
Linear solve converged due to CONVERGED_RTOL iterations 7
Finished time step 2 Time0.02
Time for 1 timestep 6.11478
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 5
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
Linear solve converged due to CONVERGED_RTOL iterations 7
Finished time step 3 Time0.03
Time for 1 timestep 6.12939
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 5
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
Linear solve converged due to CONVERGED_RTOL iterations 7
Finished time step 4 Time0.04
Time for 1 timestep 6.07793
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 5
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
Linear solve converged due to CONVERGED_RTOL iterations 7
Finished time step 5 Time0.05
Time for 1 timestep 6.0543
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 5
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
Linear solve converged due to CONVERGED_RTOL iterations 7
Finished time step 6 Time0.06
Time for 1 timestep 6.14421
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 5
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
Linear solve converged due to CONVERGED_RTOL iterations 7
Finished time step 7 Time0.07
Time for 1 timestep 6.06556
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 5
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
Linear solve converged due to CONVERGED_RTOL iterations 7
Finished time step 8 Time0.08
Time for 1 timestep 6.03899
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
Linear solve converged due to CONVERGED_RTOL iterations 7
Finished time step 9 Time0.09
Time for 1 timestep 6.22429
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
Linear solve converged due to CONVERGED_RTOL iterations 7
Finished time step 10 Time0.1
Time for 1 timestep 6.08167
total time taken is91.7146
************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

/home/a/asarkar/sudhipv/software/FreeFem-sources/src/mpi/FreeFem++-mpi on a  named nia0309.scinet.local with 240 processors, by sudhipv Thu Oct 21 15:48:29 2021
Using Petsc Release Version 3.15.0, Mar 30, 2021 

                         Max       Max/Min     Avg       Total
Time (sec):           9.180e+01     1.000   9.180e+01
Objects:              2.130e+02     1.000   2.130e+02
Flop:                 3.830e+10     1.368   3.242e+10  7.782e+12
Flop/sec:             4.173e+08     1.368   3.532e+08  8.477e+10
MPI Messages:         1.042e+04     4.000   7.085e+03  1.700e+06
MPI Message Lengths:  2.564e+07     3.085   2.815e+03  4.786e+09
MPI Reductions:       1.162e+03     1.000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flop
                            and VecAXPY() for complex vectors of length N --> 8N flop

Summary of Stages:   ----- Time ------  ----- Flop ------  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total    Count   %Total     Avg         %Total    Count   %Total
 0:      Main Stage: 9.1798e+01 100.0%  7.7817e+12 100.0%  1.700e+06 100.0%  2.815e+03      100.0%  1.144e+03  98.5%

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flop: Max - maximum over all processors
                  Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   AvgLen: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flop in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flop over all processors)/(max time over all processors)
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flop                              --- Global ---  --- Stage ----  Total
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   AvgLen  Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s
------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

BuildTwoSided         25 1.0 2.8065e+00 5.1 0.00e+00 0.0 3.9e+03 4.0e+00 2.3e+01  2  0  0  0  2   2  0  0  0  2     0
BuildTwoSidedF        20 1.0 2.5046e+00 9.4 0.00e+00 0.0 0.0e+00 0.0e+00 2.0e+01  2  0  0  0  2   2  0  0  0  2     0
MatMult              808 1.0 2.1805e+00 1.3 9.12e+08 1.0 1.1e+06 2.0e+03 2.0e+00  2  3 62 44  0   2  3 62 44  0 99574
MatMultAdd            70 1.0 3.9337e-01 1.4 1.88e+07 1.0 9.1e+04 3.5e+03 0.0e+00  0  0  5  7  0   0  0  5  7  0 11389
MatMultTranspose      70 1.0 4.0347e-01 1.9 0.00e+00 0.0 9.1e+04 1.2e+04 0.0e+00  0  0  5 22  0   0  0  5 22  0     0
MatSolve             390 1.0 1.2953e+01 1.1 9.63e+09 1.1 0.0e+00 0.0e+00 0.0e+00 13 28  0  0  0  13 28  0  0  0 169064
MatLUFactorSym         1 1.0 2.5078e-01 1.3 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatLUFactorNum        10 1.0 2.0926e+01 1.6 2.69e+10 1.5 0.0e+00 0.0e+00 0.0e+00 19 66  0  0  0  19 66  0  0  0 246349
MatConvert            10 1.0 1.1924e-01 1.3 0.00e+00 0.0 0.0e+00 0.0e+00 4.0e+01  0  0  0  0  3   0  0  0  0  3     0
MatResidual           70 1.0 3.3803e-01 2.1 1.32e+08 1.0 9.1e+04 2.4e+03 0.0e+00  0  0  5  5  0   0  0  5  5  0 92759
MatAssemblyBegin      42 1.0 2.5052e+00 9.3 0.00e+00 0.0 0.0e+00 0.0e+00 2.0e+01  2  0  0  0  2   2  0  0  0  2     0
MatAssemblyEnd        42 1.0 3.1839e-01 5.0 0.00e+00 0.0 0.0e+00 0.0e+00 4.8e+01  0  0  0  0  4   0  0  0  0  4     0
MatGetRowIJ           21 1.0 7.4997e-03 1.3 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatCreateSubMats      10 1.0 2.8152e-01 1.2 0.00e+00 0.0 1.8e+04 1.3e+04 1.0e+00  0  0  1  5  0   0  0  1  5  0     0
MatGetOrdering         1 1.0 8.9136e-02 1.3 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatIncreaseOvrlp       1 1.0 2.0792e-02 1.3 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatZeroEntries        18 1.0 2.8431e-02 2.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
KSPSetUp              21 1.0 2.6266e+01 1.0 3.01e+10 1.5 3.0e+05 3.0e+03 2.2e+02 29 76 18 18 19  29 76 18 18 19 224717
KSPSolve              10 1.0 1.9127e+01 1.0 8.19e+09 1.1 1.4e+06 2.7e+03 8.4e+02 21 24 81 78 72  21 24 81 78 73 98252
KSPGMRESOrthog       458 1.0 1.6264e+00 1.8 5.46e+08 1.0 0.0e+00 0.0e+00 4.6e+02  1  2  0  0 39   1  2  0  0 40 79893
PCSetUp               19 1.0 2.7448e+01 1.0 3.01e+10 1.5 3.0e+05 3.0e+03 2.7e+02 30 76 18 18 23  30 76 18 18 23 215039
PCSetUpOnBlocks      150 1.0 2.3881e+00 1.5 2.69e+09 1.5 0.0e+00 0.0e+00 0.0e+00  2  7  0  0  0   2  7  0  0  0 215874
PCApply               70 1.0 1.8486e+01 1.0 7.86e+09 1.1 1.3e+06 2.7e+03 6.8e+02 20 23 76 73 59  20 23 76 73 60 97450
PCApplyOnBlocks      390 1.0 3.1435e+01 1.3 3.38e+10 1.4 0.0e+00 0.0e+00 0.0e+00 31 88  0  0  0  31 88  0  0  0 217264
VecMDot              458 1.0 1.3685e+00 2.1 2.73e+08 1.0 0.0e+00 0.0e+00 4.6e+02  1  1  0  0 39   1  1  0  0 40 47475
VecNorm              548 1.0 7.0636e+0031.2 7.53e+07 1.0 0.0e+00 0.0e+00 5.5e+02  3  0  0  0 47   3  0  0  0 48  2533
VecScale             688 1.0 1.7915e-02 1.3 3.76e+07 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0 499328
VecCopy              300 1.0 9.0249e-02 1.3 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecSet              1854 1.0 3.9667e-01 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecAXPY              160 1.0 4.5401e-02 1.8 2.89e+07 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0 151555
VecAYPX              420 1.0 1.7352e-01 1.4 7.52e+07 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0 103280
VecAXPBYCZ           140 1.0 9.5651e-02 1.4 9.40e+07 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0 234197
VecMAXPY             548 1.0 3.3379e-01 1.1 3.38e+08 1.0 0.0e+00 0.0e+00 0.0e+00  0  1  0  0  0   0  1  0  0  0 241041
VecScatterBegin     2298 1.0 1.4714e+00 2.2 0.00e+00 0.0 1.5e+06 2.0e+03 3.0e+00  1  0 87 62  0   1  0 87 62  0     0
VecScatterEnd       2298 1.0 5.8019e-0110.6 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecNormalize         468 1.0 7.0203e+0038.8 8.07e+07 1.0 0.0e+00 0.0e+00 4.7e+02  3  0  0  0 40   3  0  0  0 41  2729
SFSetGraph             5 1.0 3.7899e-04 1.7 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
SFSetUp                5 1.0 7.8735e-0162.8 0.00e+00 0.0 7.8e+03 4.8e+02 3.0e+00  0  0  0  0  0   0  0  0  0  0     0
SFReduceBegin        390 1.0 1.6802e-01 1.3 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
SFReduceEnd          390 1.0 5.8992e-04 1.7 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
SFPack              2298 1.0 1.6231e-02 2.4 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
SFUnpack            2298 1.0 4.6198e-03 7.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix    21             21    193976860     0.
       Krylov Solver     5              5        72264     0.
      Preconditioner     5              5         6024     0.
              Vector   146            146    113314224     0.
           Index Set    13             13      3238812     0.
   IS L to G Mapping     1              1      1609268     0.
   Star Forest Graph    11             11        12600     0.
    Distributed Mesh     3              3        15144     0.
     Discrete System     3              3         2712     0.
           Weak Form     3              3         2472     0.
              Viewer     2              1          848     0.
========================================================================================================================
Average time to get PetscTime(): 3.13e-08
Average time for MPI_Barrier(): 7.09356e-05
Average time for zero size MPI_Send(): 1.40726e-06
#PETSc Option Table entries:
-ksp_converged_reason
-ksp_pc_side right
-ksp_rtol 1e-8
-ksp_type fgmres
-log_view
-m 2828
-mg_coarse_ksp_converged_reason
-mg_coarse_ksp_type gmres
-mg_coarse_pc_type hypre
-mg_levels_pc_type asm
-mg_levels_sub_pc_type lu
-n 2828
-nw
-pc_mg_type multiplicative
-pc_type mg
#End of PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: --prefix=/home/a/asarkar/sudhipv/software/FreeFem-sources/install/ff-petsc/r MAKEFLAGS= --with-debugging=0 COPTFLAGS="-O3 -mtune=native" CXXOPTFLAGS="-O3 -mtune=native" FOPTFLAGS="-O3 -mtune=native" --with-cxx-dialect=C++11 --with-ssl=0 --with-x=0 --with-fortran-bindings=0 --with-cudac=0 --with-cc=/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Compiler/intel2020/openmpi/4.0.3/bin/mpicc --with-cxx=/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Compiler/intel2020/openmpi/4.0.3/bin/mpic++ --with-fc=/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Compiler/intel2020/openmpi/4.0.3/bin/mpif90 --with-scalar-type=real --with-blaslapack-include=/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/imkl/2020.1.217/mkl/include --with-blaslapack-lib="-Wl,-rpath,/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/imkl/2020.1.217/mkl/lib/intel64 -L/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/imkl/2020.1.217/mkl/lib/intel64 -lmkl_rt -lmkl_intel_thread -lmkl_core  -liomp5 -lpthread" --download-metis --download-ptscotch --download-hypre --download-parmetis --download-mmg --download-parmmg --download-superlu --download-suitesparse --download-tetgen --download-slepc --download-hpddm --download-scalapack --download-mumps --download-slepc-configure-arguments=--download-arpack=https://github.com/prj-/arpack-ng/archive/b64dccb.tar.gz PETSC_ARCH=fr
-----------------------------------------
Libraries compiled on 2021-05-02 15:39:49 on nia-login03.scinet.local 
Machine characteristics: Linux-3.10.0-957.27.2.el7.x86_64-x86_64-with-centos-7.6.1810-Core
Using PETSc directory: /home/a/asarkar/sudhipv/software/FreeFem-sources/install/ff-petsc/r
Using PETSc arch: 
-----------------------------------------

Using C compiler: /cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Compiler/intel2020/openmpi/4.0.3/bin/mpicc  -fPIC -wd1572 -O3 -mtune=native   
Using Fortran compiler: /cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Compiler/intel2020/openmpi/4.0.3/bin/mpif90  -fPIC -O3 -mtune=native     
-----------------------------------------

Using include paths: -I/home/a/asarkar/sudhipv/software/FreeFem-sources/install/ff-petsc/r/include -I/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/imkl/2020.1.217/mkl/include
-----------------------------------------

Using C linker: /cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Compiler/intel2020/openmpi/4.0.3/bin/mpicc
Using Fortran linker: /cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Compiler/intel2020/openmpi/4.0.3/bin/mpif90
Using libraries: -Wl,-rpath,/home/a/asarkar/sudhipv/software/FreeFem-sources/install/ff-petsc/r/lib -L/home/a/asarkar/sudhipv/software/FreeFem-sources/install/ff-petsc/r/lib -lpetsc -Wl,-rpath,/home/a/asarkar/sudhipv/software/FreeFem-sources/install/ff-petsc/r/lib -L/home/a/asarkar/sudhipv/software/FreeFem-sources/install/ff-petsc/r/lib -Wl,-rpath,/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/imkl/2020.1.217/mkl/lib/intel64 -L/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/imkl/2020.1.217/mkl/lib/intel64 -Wl,-rpath,/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Compiler/intel2020/openmpi/4.0.3/lib -L/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Compiler/intel2020/openmpi/4.0.3/lib -Wl,-rpath,/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/libfabric/1.10.1/lib -L/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/libfabric/1.10.1/lib -Wl,-rpath,/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/ucx/1.8.0/lib -L/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/ucx/1.8.0/lib -Wl,-rpath,/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/imkl/2020.1.217/lib/intel64 -L/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/imkl/2020.1.217/lib/intel64 -Wl,-rpath,/cvmfs/restricted.computecanada.ca/easybuild/software/2020/Core/intel/2020.1.217/compilers_and_libraries_2020.1.217/linux/tbb/lib/intel64/gcc4.8 -L/cvmfs/restricted.computecanada.ca/easybuild/software/2020/Core/intel/2020.1.217/compilers_and_libraries_2020.1.217/linux/tbb/lib/intel64/gcc4.8 -Wl,-rpath,/cvmfs/restricted.computecanada.ca/easybuild/software/2020/Core/intel/2020.1.217/compilers_and_libraries_2020.1.217/linux/compiler/lib/intel64_lin -L/cvmfs/restricted.computecanada.ca/easybuild/software/2020/Core/intel/2020.1.217/compilers_and_libraries_2020.1.217/linux/compiler/lib/intel64_lin -Wl,-rpath,/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Compiler/intel2020/openmpi/4.0.3/lib64 -L/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Compiler/intel2020/openmpi/4.0.3/lib64 -Wl,-rpath,/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/gcccore/9.3.0/lib/gcc/x86_64-pc-linux-gnu/9.3.0 -L/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/gcccore/9.3.0/lib/gcc/x86_64-pc-linux-gnu/9.3.0 -Wl,-rpath,/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/gcccore/9.3.0/lib64 -L/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/gcccore/9.3.0/lib64 -Wl,-rpath,/cvmfs/soft.computecanada.ca/gentoo/2020/lib64 -L/cvmfs/soft.computecanada.ca/gentoo/2020/lib64 -Wl,-rpath,/cvmfs/soft.computecanada.ca/gentoo/2020/usr/lib64 -L/cvmfs/soft.computecanada.ca/gentoo/2020/usr/lib64 -Wl,-rpath,/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/gcccore/9.3.0/lib -L/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/gcccore/9.3.0/lib -Wl,-rpath,/cvmfs/soft.computecanada.ca/gentoo/2020/lib -L/cvmfs/soft.computecanada.ca/gentoo/2020/lib -Wl,-rpath,/cvmfs/soft.computecanada.ca/gentoo/2020/usr/lib -L/cvmfs/soft.computecanada.ca/gentoo/2020/usr/lib -lHYPRE -lcmumps -ldmumps -lsmumps -lzmumps -lmumps_common -lpord -lscalapack -lumfpack -lklu -lcholmod -lbtf -lccolamd -lcolamd -lcamd -lamd -lsuitesparseconfig -lsuperlu -lmkl_rt -lmkl_intel_thread -lmkl_core -liomp5 -lpthread -lparmmg -lmmg -lmmg3d -lptesmumps -lptscotchparmetis -lptscotch -lptscotcherr -lesmumps -lscotch -lscotcherr -lpthread -lparmetis -lmetis -ltet -lstdc++ -ldl -lmpi_usempif08 -lmpi_usempi_ignore_tkr -lmpi_mpifh -lmpi -lifport -lifcoremt_pic -limf -lsvml -lm -lipgo -lirc -lpthread -lgcc_s -lirc_s -lrt -lquadmath -lstdc++ -ldl
-----------------------------------------

#############################################################################################################
#############################################################################################################
########     Finsihed Code with NL 240 Processors             #################################################
#############################################################################################################
#############################################################################################################
Number of Vertices for coarse mesh in process  0  8003241
0.01 steps with 10 Number of Time steps for total time of 0.1s
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 5
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
Linear solve converged due to CONVERGED_RTOL iterations 7
Finished time step 1 Time0.01
Time for 1 timestep 4.86277
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 5
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
Linear solve converged due to CONVERGED_RTOL iterations 7
Finished time step 2 Time0.02
Time for 1 timestep 4.1121
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 5
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
Linear solve converged due to CONVERGED_RTOL iterations 7
Finished time step 3 Time0.03
Time for 1 timestep 4.13666
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 5
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
Linear solve converged due to CONVERGED_RTOL iterations 7
Finished time step 4 Time0.04
Time for 1 timestep 4.11798
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 5
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
Linear solve converged due to CONVERGED_RTOL iterations 7
Finished time step 5 Time0.05
Time for 1 timestep 4.13936
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 5
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
Linear solve converged due to CONVERGED_RTOL iterations 7
Finished time step 6 Time0.06
Time for 1 timestep 4.19087
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 5
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
Linear solve converged due to CONVERGED_RTOL iterations 7
Finished time step 7 Time0.07
Time for 1 timestep 4.10228
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 5
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
Linear solve converged due to CONVERGED_RTOL iterations 7
Finished time step 8 Time0.08
Time for 1 timestep 4.06558
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 5
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
Linear solve converged due to CONVERGED_RTOL iterations 7
Finished time step 9 Time0.09
Time for 1 timestep 4.10047
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 5
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
Linear solve converged due to CONVERGED_RTOL iterations 7
Finished time step 10 Time0.1
Time for 1 timestep 4.15182
total time taken is71.3481
************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

/home/a/asarkar/sudhipv/software/FreeFem-sources/src/mpi/FreeFem++-mpi on a  named nia0309.scinet.local with 320 processors, by sudhipv Thu Oct 21 15:49:44 2021
Using Petsc Release Version 3.15.0, Mar 30, 2021 

                         Max       Max/Min     Avg       Total
Time (sec):           7.144e+01     1.000   7.144e+01
Objects:              2.130e+02     1.000   2.130e+02
Flop:                 2.433e+10     1.288   2.159e+10  6.909e+12
Flop/sec:             3.405e+08     1.288   3.022e+08  9.670e+10
MPI Messages:         1.043e+04     4.000   7.172e+03  2.295e+06
MPI Message Lengths:  2.173e+07     2.672   2.465e+03  5.656e+09
MPI Reductions:       1.166e+03     1.000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flop
                            and VecAXPY() for complex vectors of length N --> 8N flop

Summary of Stages:   ----- Time ------  ----- Flop ------  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total    Count   %Total     Avg         %Total    Count   %Total
 0:      Main Stage: 7.1442e+01 100.0%  6.9087e+12 100.0%  2.295e+06 100.0%  2.465e+03      100.0%  1.148e+03  98.5%

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flop: Max - maximum over all processors
                  Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   AvgLen: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flop in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flop over all processors)/(max time over all processors)
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flop                              --- Global ---  --- Stage ----  Total
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   AvgLen  Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s
------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

BuildTwoSided         25 1.0 1.5631e+00 4.2 0.00e+00 0.0 5.3e+03 4.0e+00 2.3e+01  1  0  0  0  2   1  0  0  0  2     0
BuildTwoSidedF        20 1.0 1.3031e+00 3.7 0.00e+00 0.0 0.0e+00 0.0e+00 2.0e+01  1  0  0  0  2   1  0  0  0  2     0
MatMult              810 1.0 1.5385e+00 1.2 6.86e+08 1.0 1.4e+06 1.7e+03 2.0e+00  2  3 62 44  0   2  3 62 44  0 141264
MatMultAdd            70 1.0 2.8240e-01 1.4 1.41e+07 1.0 1.2e+05 3.0e+03 0.0e+00  0  0  5  7  0   0  0  5  7  0 15865
MatMultTranspose      70 1.0 2.8629e-01 2.0 0.00e+00 0.0 1.2e+05 1.0e+04 0.0e+00  0  0  5 22  0   0  0  5 22  0     0
MatSolve             390 1.0 9.2500e+00 1.1 6.89e+09 1.1 0.0e+00 0.0e+00 0.0e+00 12 30  0  0  0  12 30  0  0  0 227402
MatLUFactorSym         1 1.0 1.9848e-01 1.4 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatLUFactorNum        10 1.0 1.2422e+01 1.5 1.62e+10 1.4 0.0e+00 0.0e+00 0.0e+00 15 63  0  0  0  15 63  0  0  0 351631
MatConvert            10 1.0 1.1186e-01 1.4 0.00e+00 0.0 0.0e+00 0.0e+00 4.0e+01  0  0  0  0  3   0  0  0  0  3     0
MatResidual           70 1.0 2.3532e-01 1.8 9.89e+07 1.0 1.2e+05 2.1e+03 0.0e+00  0  0  5  5  0   0  0  5  5  0 133246
MatAssemblyBegin      42 1.0 1.3037e+00 3.7 0.00e+00 0.0 0.0e+00 0.0e+00 2.0e+01  1  0  0  0  2   1  0  0  0  2     0
MatAssemblyEnd        42 1.0 2.0328e-01 2.4 0.00e+00 0.0 0.0e+00 0.0e+00 4.8e+01  0  0  0  0  4   0  0  0  0  4     0
MatGetRowIJ           21 1.0 8.1639e-03 2.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatCreateSubMats      10 1.0 2.1147e-01 1.2 0.00e+00 0.0 2.5e+04 1.1e+04 1.0e+00  0  0  1  5  0   0  0  1  5  0     0
MatGetOrdering         1 1.0 9.2707e-02 1.9 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatIncreaseOvrlp       1 1.0 2.1897e-02 1.6 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatZeroEntries        18 1.0 1.2243e-02 1.3 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
KSPSetUp              21 1.0 1.6465e+01 1.0 1.85e+10 1.4 4.0e+05 2.6e+03 2.2e+02 23 74 17 18 19  23 74 17 18 19 309194
KSPSolve              10 1.0 1.3929e+01 1.0 5.91e+09 1.1 1.9e+06 2.4e+03 8.4e+02 19 26 81 78 72  19 26 81 78 73 130506
KSPGMRESOrthog       460 1.0 1.3071e+00 1.8 4.11e+08 1.0 0.0e+00 0.0e+00 4.6e+02  1  2  0  0 39   1  2  0  0 40 99652
PCSetUp               19 1.0 1.7469e+01 1.0 1.85e+10 1.4 4.0e+05 2.6e+03 2.7e+02 24 74 17 18 23  24 74 17 18 23 291423
PCSetUpOnBlocks      150 1.0 1.4753e+00 1.4 1.62e+09 1.4 0.0e+00 0.0e+00 0.0e+00  2  6  0  0  0   2  6  0  0  0 296080
PCApply               70 1.0 1.3483e+01 1.0 5.67e+09 1.1 1.7e+06 2.4e+03 6.9e+02 19 25 76 73 59  19 25 76 73 60 129048
PCApplyOnBlocks      390 1.0 2.0166e+01 1.3 2.13e+10 1.3 0.0e+00 0.0e+00 0.0e+00 26 87  0  0  0  26 87  0  0  0 299245
VecMDot              460 1.0 1.1280e+00 2.0 2.06e+08 1.0 0.0e+00 0.0e+00 4.6e+02  1  1  0  0 39   1  1  0  0 40 57737
VecNorm              550 1.0 4.0849e+0019.8 5.67e+07 1.0 0.0e+00 0.0e+00 5.5e+02  2  0  0  0 47   2  0  0  0 48  4388
VecScale             690 1.0 1.0300e-02 1.5 2.83e+07 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0 870033
VecCopy              300 1.0 6.2930e-02 1.4 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecSet              1856 1.0 2.9756e-01 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecAXPY              160 1.0 3.1742e-02 1.7 2.17e+07 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0 216769
VecAYPX              420 1.0 1.2643e-01 1.3 5.65e+07 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0 141750
VecAXPBYCZ           140 1.0 6.8093e-02 1.3 7.06e+07 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0 328979
VecMAXPY             550 1.0 2.2516e-01 1.1 2.55e+08 1.0 0.0e+00 0.0e+00 0.0e+00  0  1  0  0  0   0  1  0  0  0 358189
VecScatterBegin     2300 1.0 9.5190e-01 2.0 0.00e+00 0.0 2.0e+06 1.8e+03 3.0e+00  1  0 87 62  0   1  0 87 62  0     0
VecScatterEnd       2300 1.0 3.8340e-0111.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecNormalize         470 1.0 4.0519e+0023.5 6.08e+07 1.0 0.0e+00 0.0e+00 4.7e+02  2  0  0  0 40   2  0  0  0 41  4739
SFSetGraph             5 1.0 3.5245e-04 2.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
SFSetUp                5 1.0 4.7514e-0123.9 0.00e+00 0.0 1.1e+04 4.2e+02 3.0e+00  0  0  0  0  0   0  0  0  0  0     0
SFReduceBegin        390 1.0 1.0657e-01 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
SFReduceEnd          390 1.0 4.9970e-04 1.8 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
SFPack              2300 1.0 1.2712e-02 2.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
SFUnpack            2300 1.0 4.0804e-03 7.3 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix    21             21    142424464     0.
       Krylov Solver     5              5        72264     0.
      Preconditioner     5              5         6024     0.
              Vector   146            146     84936016     0.
           Index Set    13             13      2437308     0.
   IS L to G Mapping     1              1      1208984     0.
   Star Forest Graph    11             11        12600     0.
    Distributed Mesh     3              3        15144     0.
     Discrete System     3              3         2712     0.
           Weak Form     3              3         2472     0.
              Viewer     2              1          848     0.
========================================================================================================================
Average time to get PetscTime(): 3.13e-08
Average time for MPI_Barrier(): 7.74174e-05
Average time for zero size MPI_Send(): 1.32653e-06
#PETSc Option Table entries:
-ksp_converged_reason
-ksp_pc_side right
-ksp_rtol 1e-8
-ksp_type fgmres
-log_view
-m 2828
-mg_coarse_ksp_converged_reason
-mg_coarse_ksp_type gmres
-mg_coarse_pc_type hypre
-mg_levels_pc_type asm
-mg_levels_sub_pc_type lu
-n 2828
-nw
-pc_mg_type multiplicative
-pc_type mg
#End of PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: --prefix=/home/a/asarkar/sudhipv/software/FreeFem-sources/install/ff-petsc/r MAKEFLAGS= --with-debugging=0 COPTFLAGS="-O3 -mtune=native" CXXOPTFLAGS="-O3 -mtune=native" FOPTFLAGS="-O3 -mtune=native" --with-cxx-dialect=C++11 --with-ssl=0 --with-x=0 --with-fortran-bindings=0 --with-cudac=0 --with-cc=/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Compiler/intel2020/openmpi/4.0.3/bin/mpicc --with-cxx=/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Compiler/intel2020/openmpi/4.0.3/bin/mpic++ --with-fc=/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Compiler/intel2020/openmpi/4.0.3/bin/mpif90 --with-scalar-type=real --with-blaslapack-include=/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/imkl/2020.1.217/mkl/include --with-blaslapack-lib="-Wl,-rpath,/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/imkl/2020.1.217/mkl/lib/intel64 -L/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/imkl/2020.1.217/mkl/lib/intel64 -lmkl_rt -lmkl_intel_thread -lmkl_core  -liomp5 -lpthread" --download-metis --download-ptscotch --download-hypre --download-parmetis --download-mmg --download-parmmg --download-superlu --download-suitesparse --download-tetgen --download-slepc --download-hpddm --download-scalapack --download-mumps --download-slepc-configure-arguments=--download-arpack=https://github.com/prj-/arpack-ng/archive/b64dccb.tar.gz PETSC_ARCH=fr
-----------------------------------------
Libraries compiled on 2021-05-02 15:39:49 on nia-login03.scinet.local 
Machine characteristics: Linux-3.10.0-957.27.2.el7.x86_64-x86_64-with-centos-7.6.1810-Core
Using PETSc directory: /home/a/asarkar/sudhipv/software/FreeFem-sources/install/ff-petsc/r
Using PETSc arch: 
-----------------------------------------

Using C compiler: /cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Compiler/intel2020/openmpi/4.0.3/bin/mpicc  -fPIC -wd1572 -O3 -mtune=native   
Using Fortran compiler: /cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Compiler/intel2020/openmpi/4.0.3/bin/mpif90  -fPIC -O3 -mtune=native     
-----------------------------------------

Using include paths: -I/home/a/asarkar/sudhipv/software/FreeFem-sources/install/ff-petsc/r/include -I/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/imkl/2020.1.217/mkl/include
-----------------------------------------

Using C linker: /cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Compiler/intel2020/openmpi/4.0.3/bin/mpicc
Using Fortran linker: /cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Compiler/intel2020/openmpi/4.0.3/bin/mpif90
Using libraries: -Wl,-rpath,/home/a/asarkar/sudhipv/software/FreeFem-sources/install/ff-petsc/r/lib -L/home/a/asarkar/sudhipv/software/FreeFem-sources/install/ff-petsc/r/lib -lpetsc -Wl,-rpath,/home/a/asarkar/sudhipv/software/FreeFem-sources/install/ff-petsc/r/lib -L/home/a/asarkar/sudhipv/software/FreeFem-sources/install/ff-petsc/r/lib -Wl,-rpath,/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/imkl/2020.1.217/mkl/lib/intel64 -L/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/imkl/2020.1.217/mkl/lib/intel64 -Wl,-rpath,/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Compiler/intel2020/openmpi/4.0.3/lib -L/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Compiler/intel2020/openmpi/4.0.3/lib -Wl,-rpath,/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/libfabric/1.10.1/lib -L/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/libfabric/1.10.1/lib -Wl,-rpath,/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/ucx/1.8.0/lib -L/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/ucx/1.8.0/lib -Wl,-rpath,/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/imkl/2020.1.217/lib/intel64 -L/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/imkl/2020.1.217/lib/intel64 -Wl,-rpath,/cvmfs/restricted.computecanada.ca/easybuild/software/2020/Core/intel/2020.1.217/compilers_and_libraries_2020.1.217/linux/tbb/lib/intel64/gcc4.8 -L/cvmfs/restricted.computecanada.ca/easybuild/software/2020/Core/intel/2020.1.217/compilers_and_libraries_2020.1.217/linux/tbb/lib/intel64/gcc4.8 -Wl,-rpath,/cvmfs/restricted.computecanada.ca/easybuild/software/2020/Core/intel/2020.1.217/compilers_and_libraries_2020.1.217/linux/compiler/lib/intel64_lin -L/cvmfs/restricted.computecanada.ca/easybuild/software/2020/Core/intel/2020.1.217/compilers_and_libraries_2020.1.217/linux/compiler/lib/intel64_lin -Wl,-rpath,/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Compiler/intel2020/openmpi/4.0.3/lib64 -L/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Compiler/intel2020/openmpi/4.0.3/lib64 -Wl,-rpath,/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/gcccore/9.3.0/lib/gcc/x86_64-pc-linux-gnu/9.3.0 -L/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/gcccore/9.3.0/lib/gcc/x86_64-pc-linux-gnu/9.3.0 -Wl,-rpath,/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/gcccore/9.3.0/lib64 -L/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/gcccore/9.3.0/lib64 -Wl,-rpath,/cvmfs/soft.computecanada.ca/gentoo/2020/lib64 -L/cvmfs/soft.computecanada.ca/gentoo/2020/lib64 -Wl,-rpath,/cvmfs/soft.computecanada.ca/gentoo/2020/usr/lib64 -L/cvmfs/soft.computecanada.ca/gentoo/2020/usr/lib64 -Wl,-rpath,/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/gcccore/9.3.0/lib -L/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/gcccore/9.3.0/lib -Wl,-rpath,/cvmfs/soft.computecanada.ca/gentoo/2020/lib -L/cvmfs/soft.computecanada.ca/gentoo/2020/lib -Wl,-rpath,/cvmfs/soft.computecanada.ca/gentoo/2020/usr/lib -L/cvmfs/soft.computecanada.ca/gentoo/2020/usr/lib -lHYPRE -lcmumps -ldmumps -lsmumps -lzmumps -lmumps_common -lpord -lscalapack -lumfpack -lklu -lcholmod -lbtf -lccolamd -lcolamd -lcamd -lamd -lsuitesparseconfig -lsuperlu -lmkl_rt -lmkl_intel_thread -lmkl_core -liomp5 -lpthread -lparmmg -lmmg -lmmg3d -lptesmumps -lptscotchparmetis -lptscotch -lptscotcherr -lesmumps -lscotch -lscotcherr -lpthread -lparmetis -lmetis -ltet -lstdc++ -ldl -lmpi_usempif08 -lmpi_usempi_ignore_tkr -lmpi_mpifh -lmpi -lifport -lifcoremt_pic -limf -lsvml -lm -lipgo -lirc -lpthread -lgcc_s -lirc_s -lrt -lquadmath -lstdc++ -ldl
-----------------------------------------

#############################################################################################################
#############################################################################################################
########     Finsihed Code with 40 Processors             #################################################
#############################################################################################################
#############################################################################################################
Number of Vertices for coarse mesh in process  0  8003241
0.01 steps with 10 Number of Time steps for total time of 0.1s
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 5
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
Linear solve converged due to CONVERGED_RTOL iterations 7
Finished time step 1 Time0.01
Time for 1 timestep 4.11299
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 5
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
Linear solve converged due to CONVERGED_RTOL iterations 7
Finished time step 2 Time0.02
Time for 1 timestep 3.31226
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 5
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
Linear solve converged due to CONVERGED_RTOL iterations 7
Finished time step 3 Time0.03
Time for 1 timestep 3.29573
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 5
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
Linear solve converged due to CONVERGED_RTOL iterations 7
Finished time step 4 Time0.04
Time for 1 timestep 3.29052
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 5
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
Linear solve converged due to CONVERGED_RTOL iterations 7
Finished time step 5 Time0.05
Time for 1 timestep 3.229
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 5
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
Linear solve converged due to CONVERGED_RTOL iterations 7
Finished time step 6 Time0.06
Time for 1 timestep 3.22768
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 5
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
Linear solve converged due to CONVERGED_RTOL iterations 7
Finished time step 7 Time0.07
Time for 1 timestep 3.18507
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 5
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
Linear solve converged due to CONVERGED_RTOL iterations 7
Finished time step 8 Time0.08
Time for 1 timestep 3.21773
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 5
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
Linear solve converged due to CONVERGED_RTOL iterations 7
Finished time step 9 Time0.09
Time for 1 timestep 3.20207
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 5
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
Linear solve converged due to CONVERGED_RTOL iterations 7
Finished time step 10 Time0.1
Time for 1 timestep 3.30797
total time taken is62.3145
************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

/home/a/asarkar/sudhipv/software/FreeFem-sources/src/mpi/FreeFem++-mpi on a  named nia0309.scinet.local with 400 processors, by sudhipv Thu Oct 21 15:50:50 2021
Using Petsc Release Version 3.15.0, Mar 30, 2021 

                         Max       Max/Min     Avg       Total
Time (sec):           6.241e+01     1.000   6.241e+01
Objects:              2.130e+02     1.000   2.130e+02
Flop:                 1.939e+10     1.376   1.599e+10  6.396e+12
Flop/sec:             3.106e+08     1.376   2.562e+08  1.025e+11
MPI Messages:         1.043e+04     4.000   7.250e+03  2.900e+06
MPI Message Lengths:  1.972e+07     3.010   2.170e+03  6.292e+09
MPI Reductions:       1.166e+03     1.000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flop
                            and VecAXPY() for complex vectors of length N --> 8N flop

Summary of Stages:   ----- Time ------  ----- Flop ------  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total    Count   %Total     Avg         %Total    Count   %Total
 0:      Main Stage: 6.2409e+01 100.0%  6.3958e+12 100.0%  2.900e+06 100.0%  2.170e+03      100.0%  1.148e+03  98.5%

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flop: Max - maximum over all processors
                  Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   AvgLen: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flop in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flop over all processors)/(max time over all processors)
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flop                              --- Global ---  --- Stage ----  Total
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   AvgLen  Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s
------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

BuildTwoSided         25 1.0 2.1656e+00 4.0 0.00e+00 0.0 6.7e+03 4.0e+00 2.3e+01  3  0  0  0  2   3  0  0  0  2     0
BuildTwoSidedF        20 1.0 1.7799e+00 5.6 0.00e+00 0.0 0.0e+00 0.0e+00 2.0e+01  2  0  0  0  2   2  0  0  0  2     0
MatMult              810 1.0 1.2630e+00 1.2 5.50e+08 1.0 1.8e+06 1.5e+03 2.0e+00  2  3 62 44  0   2  3 62 44  0 172066
MatMultAdd            70 1.0 1.9969e-01 1.4 1.13e+07 1.0 1.6e+05 2.7e+03 0.0e+00  0  0  5  7  0   0  0  5  7  0 22436
MatMultTranspose      70 1.0 2.2302e-01 1.9 0.00e+00 0.0 1.6e+05 9.0e+03 0.0e+00  0  0  5 22  0   0  0  5 22  0     0
MatSolve             390 1.0 7.3431e+00 1.2 5.46e+09 1.1 0.0e+00 0.0e+00 0.0e+00 11 32  0  0  0  11 32  0  0  0 278765
MatLUFactorSym         1 1.0 1.8397e-01 1.7 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatLUFactorNum        10 1.0 8.8976e+00 1.5 1.28e+10 1.6 0.0e+00 0.0e+00 0.0e+00 12 61  0  0  0  12 61  0  0  0 439625
MatConvert            10 1.0 8.7150e-02 1.3 0.00e+00 0.0 0.0e+00 0.0e+00 4.0e+01  0  0  0  0  3   0  0  0  0  3     0
MatResidual           70 1.0 2.1389e-01 2.3 7.92e+07 1.0 1.6e+05 1.8e+03 0.0e+00  0  0  5  4  0   0  0  5  4  0 146594
MatAssemblyBegin      42 1.0 1.7806e+00 5.6 0.00e+00 0.0 0.0e+00 0.0e+00 2.0e+01  2  0  0  0  2   2  0  0  0  2     0
MatAssemblyEnd        42 1.0 2.7312e-01 2.8 0.00e+00 0.0 0.0e+00 0.0e+00 4.8e+01  0  0  0  0  4   0  0  0  0  4     0
MatGetRowIJ           21 1.0 7.2198e-03 2.3 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatCreateSubMats      10 1.0 1.8269e-01 1.3 0.00e+00 0.0 3.1e+04 9.7e+03 1.0e+00  0  0  1  5  0   0  0  1  5  0     0
MatGetOrdering         1 1.0 7.1296e-02 2.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatIncreaseOvrlp       1 1.0 2.1406e-02 1.7 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatZeroEntries        18 1.0 1.0346e-02 1.4 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
KSPSetUp              21 1.0 1.2270e+01 1.0 1.47e+10 1.5 5.1e+05 2.3e+03 2.2e+02 20 72 17 18 19  20 72 17 18 19 376402
KSPSolve              10 1.0 1.1195e+01 1.0 4.69e+09 1.1 2.4e+06 2.1e+03 8.4e+02 18 28 81 78 72  18 28 81 78 73 158746
KSPGMRESOrthog       460 1.0 1.1658e+00 1.7 3.30e+08 1.0 0.0e+00 0.0e+00 4.6e+02  1  2  0  0 39   1  2  0  0 40 111732
PCSetUp               19 1.0 1.3133e+01 1.0 1.47e+10 1.5 5.1e+05 2.3e+03 2.7e+02 21 72 17 18 23  21 72 17 18 23 351664
PCSetUpOnBlocks      150 1.0 1.2236e+00 1.7 1.28e+09 1.6 0.0e+00 0.0e+00 0.0e+00  1  6  0  0  0   1  6  0  0  0 319680
PCApply               70 1.0 1.0812e+01 1.0 4.50e+09 1.1 2.2e+06 2.1e+03 6.9e+02 17 27 76 73 59  17 27 76 73 60 157191
PCApplyOnBlocks      390 1.0 1.5068e+01 1.3 1.70e+10 1.4 0.0e+00 0.0e+00 0.0e+00 22 87  0  0  0  22 87  0  0  0 369486
VecMDot              460 1.0 1.0336e+00 1.9 1.65e+08 1.0 0.0e+00 0.0e+00 4.6e+02  1  1  0  0 39   1  1  0  0 40 63007
VecNorm              550 1.0 3.0243e+0015.4 4.54e+07 1.0 0.0e+00 0.0e+00 5.5e+02  3  0  0  0 47   3  0  0  0 48  5926
VecScale             690 1.0 7.1640e-03 1.4 2.27e+07 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0 1250898
VecCopy              300 1.0 4.4646e-02 1.4 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecSet              1856 1.0 2.3704e-01 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecAXPY              160 1.0 2.5394e-02 1.8 1.74e+07 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0 270958
VecAYPX              420 1.0 9.6113e-02 1.3 4.53e+07 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0 186456
VecAXPBYCZ           140 1.0 5.1372e-02 1.5 5.66e+07 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0 436060
VecMAXPY             550 1.0 1.6820e-01 1.1 2.04e+08 1.0 0.0e+00 0.0e+00 0.0e+00  0  1  0  0  0   0  1  0  0  0 479491
VecScatterBegin     2300 1.0 8.7997e-01 2.2 0.00e+00 0.0 2.5e+06 1.6e+03 3.0e+00  1  0 87 62  0   1  0 87 62  0     0
VecScatterEnd       2300 1.0 4.1347e-01 9.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecNormalize         470 1.0 2.9892e+0018.4 4.88e+07 1.0 0.0e+00 0.0e+00 4.7e+02  2  0  0  0 40   2  0  0  0 41  6424
SFSetGraph             5 1.0 3.0495e-04 2.4 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
SFSetUp                5 1.0 5.0635e-0127.7 0.00e+00 0.0 1.3e+04 3.7e+02 3.0e+00  1  0  0  0  0   1  0  0  0  0     0
SFReduceBegin        390 1.0 7.8170e-02 1.3 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
SFReduceEnd          390 1.0 4.1324e-04 1.7 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
SFPack              2300 1.0 1.0282e-02 2.6 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
SFUnpack            2300 1.0 3.5249e-03 7.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix    21             21    113045808     0.
       Krylov Solver     5              5        72264     0.
      Preconditioner     5              5         6024     0.
              Vector   146            146     67995080     0.
           Index Set    13             13      1952336     0.
   IS L to G Mapping     1              1       967316     0.
   Star Forest Graph    11             11        12600     0.
    Distributed Mesh     3              3        15144     0.
     Discrete System     3              3         2712     0.
           Weak Form     3              3         2472     0.
              Viewer     2              1          848     0.
========================================================================================================================
Average time to get PetscTime(): 2.99e-08
Average time for MPI_Barrier(): 9.85556e-05
Average time for zero size MPI_Send(): 1.32451e-06
#PETSc Option Table entries:
-ksp_converged_reason
-ksp_pc_side right
-ksp_rtol 1e-8
-ksp_type fgmres
-log_view
-m 2828
-mg_coarse_ksp_converged_reason
-mg_coarse_ksp_type gmres
-mg_coarse_pc_type hypre
-mg_levels_pc_type asm
-mg_levels_sub_pc_type lu
-n 2828
-nw
-pc_mg_type multiplicative
-pc_type mg
#End of PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: --prefix=/home/a/asarkar/sudhipv/software/FreeFem-sources/install/ff-petsc/r MAKEFLAGS= --with-debugging=0 COPTFLAGS="-O3 -mtune=native" CXXOPTFLAGS="-O3 -mtune=native" FOPTFLAGS="-O3 -mtune=native" --with-cxx-dialect=C++11 --with-ssl=0 --with-x=0 --with-fortran-bindings=0 --with-cudac=0 --with-cc=/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Compiler/intel2020/openmpi/4.0.3/bin/mpicc --with-cxx=/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Compiler/intel2020/openmpi/4.0.3/bin/mpic++ --with-fc=/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Compiler/intel2020/openmpi/4.0.3/bin/mpif90 --with-scalar-type=real --with-blaslapack-include=/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/imkl/2020.1.217/mkl/include --with-blaslapack-lib="-Wl,-rpath,/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/imkl/2020.1.217/mkl/lib/intel64 -L/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/imkl/2020.1.217/mkl/lib/intel64 -lmkl_rt -lmkl_intel_thread -lmkl_core  -liomp5 -lpthread" --download-metis --download-ptscotch --download-hypre --download-parmetis --download-mmg --download-parmmg --download-superlu --download-suitesparse --download-tetgen --download-slepc --download-hpddm --download-scalapack --download-mumps --download-slepc-configure-arguments=--download-arpack=https://github.com/prj-/arpack-ng/archive/b64dccb.tar.gz PETSC_ARCH=fr
-----------------------------------------
Libraries compiled on 2021-05-02 15:39:49 on nia-login03.scinet.local 
Machine characteristics: Linux-3.10.0-957.27.2.el7.x86_64-x86_64-with-centos-7.6.1810-Core
Using PETSc directory: /home/a/asarkar/sudhipv/software/FreeFem-sources/install/ff-petsc/r
Using PETSc arch: 
-----------------------------------------

Using C compiler: /cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Compiler/intel2020/openmpi/4.0.3/bin/mpicc  -fPIC -wd1572 -O3 -mtune=native   
Using Fortran compiler: /cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Compiler/intel2020/openmpi/4.0.3/bin/mpif90  -fPIC -O3 -mtune=native     
-----------------------------------------

Using include paths: -I/home/a/asarkar/sudhipv/software/FreeFem-sources/install/ff-petsc/r/include -I/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/imkl/2020.1.217/mkl/include
-----------------------------------------

Using C linker: /cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Compiler/intel2020/openmpi/4.0.3/bin/mpicc
Using Fortran linker: /cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Compiler/intel2020/openmpi/4.0.3/bin/mpif90
Using libraries: -Wl,-rpath,/home/a/asarkar/sudhipv/software/FreeFem-sources/install/ff-petsc/r/lib -L/home/a/asarkar/sudhipv/software/FreeFem-sources/install/ff-petsc/r/lib -lpetsc -Wl,-rpath,/home/a/asarkar/sudhipv/software/FreeFem-sources/install/ff-petsc/r/lib -L/home/a/asarkar/sudhipv/software/FreeFem-sources/install/ff-petsc/r/lib -Wl,-rpath,/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/imkl/2020.1.217/mkl/lib/intel64 -L/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/imkl/2020.1.217/mkl/lib/intel64 -Wl,-rpath,/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Compiler/intel2020/openmpi/4.0.3/lib -L/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Compiler/intel2020/openmpi/4.0.3/lib -Wl,-rpath,/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/libfabric/1.10.1/lib -L/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/libfabric/1.10.1/lib -Wl,-rpath,/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/ucx/1.8.0/lib -L/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/ucx/1.8.0/lib -Wl,-rpath,/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/imkl/2020.1.217/lib/intel64 -L/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/imkl/2020.1.217/lib/intel64 -Wl,-rpath,/cvmfs/restricted.computecanada.ca/easybuild/software/2020/Core/intel/2020.1.217/compilers_and_libraries_2020.1.217/linux/tbb/lib/intel64/gcc4.8 -L/cvmfs/restricted.computecanada.ca/easybuild/software/2020/Core/intel/2020.1.217/compilers_and_libraries_2020.1.217/linux/tbb/lib/intel64/gcc4.8 -Wl,-rpath,/cvmfs/restricted.computecanada.ca/easybuild/software/2020/Core/intel/2020.1.217/compilers_and_libraries_2020.1.217/linux/compiler/lib/intel64_lin -L/cvmfs/restricted.computecanada.ca/easybuild/software/2020/Core/intel/2020.1.217/compilers_and_libraries_2020.1.217/linux/compiler/lib/intel64_lin -Wl,-rpath,/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Compiler/intel2020/openmpi/4.0.3/lib64 -L/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Compiler/intel2020/openmpi/4.0.3/lib64 -Wl,-rpath,/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/gcccore/9.3.0/lib/gcc/x86_64-pc-linux-gnu/9.3.0 -L/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/gcccore/9.3.0/lib/gcc/x86_64-pc-linux-gnu/9.3.0 -Wl,-rpath,/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/gcccore/9.3.0/lib64 -L/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/gcccore/9.3.0/lib64 -Wl,-rpath,/cvmfs/soft.computecanada.ca/gentoo/2020/lib64 -L/cvmfs/soft.computecanada.ca/gentoo/2020/lib64 -Wl,-rpath,/cvmfs/soft.computecanada.ca/gentoo/2020/usr/lib64 -L/cvmfs/soft.computecanada.ca/gentoo/2020/usr/lib64 -Wl,-rpath,/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/gcccore/9.3.0/lib -L/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/gcccore/9.3.0/lib -Wl,-rpath,/cvmfs/soft.computecanada.ca/gentoo/2020/lib -L/cvmfs/soft.computecanada.ca/gentoo/2020/lib -Wl,-rpath,/cvmfs/soft.computecanada.ca/gentoo/2020/usr/lib -L/cvmfs/soft.computecanada.ca/gentoo/2020/usr/lib -lHYPRE -lcmumps -ldmumps -lsmumps -lzmumps -lmumps_common -lpord -lscalapack -lumfpack -lklu -lcholmod -lbtf -lccolamd -lcolamd -lcamd -lamd -lsuitesparseconfig -lsuperlu -lmkl_rt -lmkl_intel_thread -lmkl_core -liomp5 -lpthread -lparmmg -lmmg -lmmg3d -lptesmumps -lptscotchparmetis -lptscotch -lptscotcherr -lesmumps -lscotch -lscotcherr -lpthread -lparmetis -lmetis -ltet -lstdc++ -ldl -lmpi_usempif08 -lmpi_usempi_ignore_tkr -lmpi_mpifh -lmpi -lifport -lifcoremt_pic -limf -lsvml -lm -lipgo -lirc -lpthread -lgcc_s -lirc_s -lrt -lquadmath -lstdc++ -ldl
-----------------------------------------

#############################################################################################################
#############################################################################################################
########     Finsihed Code with NL 400 Processors             #################################################
#############################################################################################################
#############################################################################################################
Number of Vertices for coarse mesh in process  0  8003241
0.01 steps with 10 Number of Time steps for total time of 0.1s
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 5
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 5
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 5
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 5
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 5
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 5
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 5
Linear solve converged due to CONVERGED_RTOL iterations 7
Finished time step 1 Time0.01
Time for 1 timestep 2.5019
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 5
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 5
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 5
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 5
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 5
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 5
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 5
Linear solve converged due to CONVERGED_RTOL iterations 7
Finished time step 2 Time0.02
Time for 1 timestep 2.05377
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 5
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 5
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 5
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 5
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 5
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 5
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 5
Linear solve converged due to CONVERGED_RTOL iterations 7
Finished time step 3 Time0.03
Time for 1 timestep 2.0188
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 5
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
Linear solve converged due to CONVERGED_RTOL iterations 7
Finished time step 4 Time0.04
Time for 1 timestep 2.01938
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 5
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
Linear solve converged due to CONVERGED_RTOL iterations 7
Finished time step 5 Time0.05
Time for 1 timestep 2.08722
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 5
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
Linear solve converged due to CONVERGED_RTOL iterations 7
Finished time step 6 Time0.06
Time for 1 timestep 2.04248
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 5
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
Linear solve converged due to CONVERGED_RTOL iterations 7
Finished time step 7 Time0.07
Time for 1 timestep 1.97389
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 5
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
Linear solve converged due to CONVERGED_RTOL iterations 7
Finished time step 8 Time0.08
Time for 1 timestep 2.02348
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 5
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
Linear solve converged due to CONVERGED_RTOL iterations 7
Finished time step 9 Time0.09
Time for 1 timestep 2.01033
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 5
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
Linear solve converged due to CONVERGED_RTOL iterations 7
Finished time step 10 Time0.1
Time for 1 timestep 2.15479
total time taken is48.88
************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

/home/a/asarkar/sudhipv/software/FreeFem-sources/src/mpi/FreeFem++-mpi on a  named nia0309.scinet.local with 600 processors, by sudhipv Thu Oct 21 15:51:48 2021
Using Petsc Release Version 3.15.0, Mar 30, 2021 

                         Max       Max/Min     Avg       Total
Time (sec):           4.898e+01     1.000   4.898e+01
Objects:              2.130e+02     1.000   2.130e+02
Flop:                 1.118e+10     1.376   9.227e+09  5.536e+12
Flop/sec:             2.282e+08     1.376   1.884e+08  1.130e+11
MPI Messages:         1.190e+04     9.000   7.465e+03  4.479e+06
MPI Message Lengths:  1.633e+07     3.340   1.751e+03  7.842e+09
MPI Reductions:       1.202e+03     1.000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flop
                            and VecAXPY() for complex vectors of length N --> 8N flop

Summary of Stages:   ----- Time ------  ----- Flop ------  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total    Count   %Total     Avg         %Total    Count   %Total
 0:      Main Stage: 4.8979e+01 100.0%  5.5364e+12 100.0%  4.479e+06 100.0%  1.751e+03      100.0%  1.184e+03  98.5%

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flop: Max - maximum over all processors
                  Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   AvgLen: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flop in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flop over all processors)/(max time over all processors)
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flop                              --- Global ---  --- Stage ----  Total
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   AvgLen  Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s
------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

BuildTwoSided         25 1.0 1.3834e+00 3.5 0.00e+00 0.0 1.0e+04 4.0e+00 2.3e+01  2  0  0  0  2   2  0  0  0  2     0
BuildTwoSidedF        20 1.0 1.2279e+00 3.8 0.00e+00 0.0 0.0e+00 0.0e+00 2.0e+01  2  0  0  0  2   2  0  0  0  2     0
MatMult              828 1.0 8.9664e-01 1.4 3.71e+08 1.0 2.8e+06 1.2e+03 2.0e+00  2  4 63 44  0   2  4 63 44  0 244469
MatMultAdd            70 1.0 1.3392e-01 1.4 7.56e+06 1.0 2.4e+05 2.2e+03 0.0e+00  0  0  5  7  0   0  0  5  7  0 33455
MatMultTranspose      70 1.0 1.5560e-01 2.0 0.00e+00 0.0 2.4e+05 7.4e+03 0.0e+00  0  0  5 22  0   0  0  5 22  0     0
MatSolve             390 1.0 4.7461e+00 1.2 3.48e+09 1.1 0.0e+00 0.0e+00 0.0e+00  9 35  0  0  0   9 35  0  0  0 408317
MatLUFactorSym         1 1.0 1.1552e-01 1.6 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatLUFactorNum        10 1.0 4.4933e+00 1.7 6.95e+09 1.6 0.0e+00 0.0e+00 0.0e+00  7 57  0  0  0   7 57  0  0  0 702350
MatConvert            10 1.0 7.1753e-02 1.3 0.00e+00 0.0 0.0e+00 0.0e+00 4.0e+01  0  0  0  0  3   0  0  0  0  3     0
MatResidual           70 1.0 1.2724e-01 2.3 5.29e+07 1.0 2.4e+05 1.5e+03 0.0e+00  0  1  5  4  0   0  1  5  4  0 246434
MatAssemblyBegin      42 1.0 1.2286e+00 3.8 0.00e+00 0.0 0.0e+00 0.0e+00 2.0e+01  2  0  0  0  2   2  0  0  0  2     0
MatAssemblyEnd        42 1.0 2.1743e-01 3.1 0.00e+00 0.0 0.0e+00 0.0e+00 4.8e+01  0  0  0  0  4   0  0  0  0  4     0
MatGetRowIJ           21 1.0 4.7550e-03 2.4 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatCreateSubMats      10 1.0 1.1787e-01 1.2 0.00e+00 0.0 4.7e+04 7.9e+03 1.0e+00  0  0  1  5  0   0  0  1  5  0     0
MatGetOrdering         1 1.0 4.4532e-02 2.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatIncreaseOvrlp       1 1.0 1.6664e-02 1.9 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatZeroEntries        18 1.0 1.0003e-02 1.9 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
KSPSetUp              21 1.0 6.5993e+00 1.0 8.15e+09 1.5 7.7e+05 1.8e+03 2.2e+02 13 69 17 18 19  13 69 17 18 19 580672
KSPSolve              10 1.0 7.7343e+00 1.0 3.03e+09 1.1 3.7e+06 1.7e+03 8.8e+02 16 31 82 78 73  16 31 82 78 74 220368
KSPGMRESOrthog       478 1.0 8.6131e-01 1.9 2.25e+08 1.0 0.0e+00 0.0e+00 4.8e+02  1  2  0  0 40   1  2  0  0 40 154573
PCSetUp               19 1.0 7.3638e+00 1.0 8.15e+09 1.5 7.7e+05 1.8e+03 2.7e+02 15 69 17 18 22  15 69 17 18 23 520391
PCSetUpOnBlocks      150 1.0 5.5834e-01 1.5 6.95e+08 1.6 0.0e+00 0.0e+00 0.0e+00  1  6  0  0  0   1  6  0  0  0 565222
PCApply               70 1.0 7.4850e+00 1.0 2.89e+09 1.1 3.4e+06 1.7e+03 7.2e+02 15 29 76 74 60  15 29 76 74 61 217318
PCApplyOnBlocks      390 1.0 8.6692e+00 1.4 9.74e+09 1.4 0.0e+00 0.0e+00 0.0e+00 15 86  0  0  0  15 86  0  0  0 551164
VecMDot              478 1.0 7.8425e-01 2.1 1.13e+08 1.0 0.0e+00 0.0e+00 4.8e+02  1  1  0  0 40   1  1  0  0 40 84880
VecNorm              568 1.0 1.8761e+0011.1 3.09e+07 1.0 0.0e+00 0.0e+00 5.7e+02  2  0  0  0 47   2  0  0  0 48  9707
VecScale             708 1.0 5.5657e-03 1.6 1.54e+07 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0 1636005
VecCopy              300 1.0 2.0915e-02 1.3 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecSet              1874 1.0 1.5352e-01 1.3 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecAXPY              160 1.0 1.5475e-02 1.7 1.16e+07 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0 444627
VecAYPX              420 1.0 6.0897e-02 1.5 3.03e+07 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0 294283
VecAXPBYCZ           140 1.0 3.2654e-02 1.8 3.78e+07 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0 686018
VecMAXPY             568 1.0 1.0346e-01 1.1 1.39e+08 1.0 0.0e+00 0.0e+00 0.0e+00  0  1  0  0  0   0  1  0  0  0 796228
VecScatterBegin     2318 1.0 4.4013e-01 1.8 0.00e+00 0.0 3.9e+06 1.3e+03 3.0e+00  1  0 87 62  0   1  0 87 62  0     0
VecScatterEnd       2318 1.0 3.5308e-0113.6 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecNormalize         488 1.0 1.8521e+0012.8 3.33e+07 1.0 0.0e+00 0.0e+00 4.9e+02  2  0  0  0 41   2  0  0  0 41 10602
SFSetGraph             5 1.0 1.3790e-04 1.8 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
SFSetUp                5 1.0 2.1112e-0112.0 0.00e+00 0.0 2.0e+04 3.0e+02 3.0e+00  0  0  0  0  0   0  0  0  0  0     0
SFReduceBegin        390 1.0 4.5063e-02 1.4 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
SFReduceEnd          390 1.0 3.8344e-04 1.9 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
SFPack              2318 1.0 8.1979e-03 2.4 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
SFUnpack            2318 1.0 2.9172e-03 6.5 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix    21             21     72006936     0.
       Krylov Solver     5              5        72264     0.
      Preconditioner     5              5         6024     0.
              Vector   146            146     45446608     0.
           Index Set    13             13      1307900     0.
   IS L to G Mapping     1              1       646160     0.
   Star Forest Graph    11             11        12600     0.
    Distributed Mesh     3              3        15144     0.
     Discrete System     3              3         2712     0.
           Weak Form     3              3         2472     0.
              Viewer     2              1          848     0.
========================================================================================================================
Average time to get PetscTime(): 3.03e-08
Average time for MPI_Barrier(): 0.000158153
Average time for zero size MPI_Send(): 1.36595e-06
#PETSc Option Table entries:
-ksp_converged_reason
-ksp_pc_side right
-ksp_rtol 1e-8
-ksp_type fgmres
-log_view
-m 2828
-mg_coarse_ksp_converged_reason
-mg_coarse_ksp_type gmres
-mg_coarse_pc_type hypre
-mg_levels_pc_type asm
-mg_levels_sub_pc_type lu
-n 2828
-nw
-pc_mg_type multiplicative
-pc_type mg
#End of PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: --prefix=/home/a/asarkar/sudhipv/software/FreeFem-sources/install/ff-petsc/r MAKEFLAGS= --with-debugging=0 COPTFLAGS="-O3 -mtune=native" CXXOPTFLAGS="-O3 -mtune=native" FOPTFLAGS="-O3 -mtune=native" --with-cxx-dialect=C++11 --with-ssl=0 --with-x=0 --with-fortran-bindings=0 --with-cudac=0 --with-cc=/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Compiler/intel2020/openmpi/4.0.3/bin/mpicc --with-cxx=/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Compiler/intel2020/openmpi/4.0.3/bin/mpic++ --with-fc=/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Compiler/intel2020/openmpi/4.0.3/bin/mpif90 --with-scalar-type=real --with-blaslapack-include=/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/imkl/2020.1.217/mkl/include --with-blaslapack-lib="-Wl,-rpath,/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/imkl/2020.1.217/mkl/lib/intel64 -L/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/imkl/2020.1.217/mkl/lib/intel64 -lmkl_rt -lmkl_intel_thread -lmkl_core  -liomp5 -lpthread" --download-metis --download-ptscotch --download-hypre --download-parmetis --download-mmg --download-parmmg --download-superlu --download-suitesparse --download-tetgen --download-slepc --download-hpddm --download-scalapack --download-mumps --download-slepc-configure-arguments=--download-arpack=https://github.com/prj-/arpack-ng/archive/b64dccb.tar.gz PETSC_ARCH=fr
-----------------------------------------
Libraries compiled on 2021-05-02 15:39:49 on nia-login03.scinet.local 
Machine characteristics: Linux-3.10.0-957.27.2.el7.x86_64-x86_64-with-centos-7.6.1810-Core
Using PETSc directory: /home/a/asarkar/sudhipv/software/FreeFem-sources/install/ff-petsc/r
Using PETSc arch: 
-----------------------------------------

Using C compiler: /cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Compiler/intel2020/openmpi/4.0.3/bin/mpicc  -fPIC -wd1572 -O3 -mtune=native   
Using Fortran compiler: /cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Compiler/intel2020/openmpi/4.0.3/bin/mpif90  -fPIC -O3 -mtune=native     
-----------------------------------------

Using include paths: -I/home/a/asarkar/sudhipv/software/FreeFem-sources/install/ff-petsc/r/include -I/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/imkl/2020.1.217/mkl/include
-----------------------------------------

Using C linker: /cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Compiler/intel2020/openmpi/4.0.3/bin/mpicc
Using Fortran linker: /cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Compiler/intel2020/openmpi/4.0.3/bin/mpif90
Using libraries: -Wl,-rpath,/home/a/asarkar/sudhipv/software/FreeFem-sources/install/ff-petsc/r/lib -L/home/a/asarkar/sudhipv/software/FreeFem-sources/install/ff-petsc/r/lib -lpetsc -Wl,-rpath,/home/a/asarkar/sudhipv/software/FreeFem-sources/install/ff-petsc/r/lib -L/home/a/asarkar/sudhipv/software/FreeFem-sources/install/ff-petsc/r/lib -Wl,-rpath,/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/imkl/2020.1.217/mkl/lib/intel64 -L/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/imkl/2020.1.217/mkl/lib/intel64 -Wl,-rpath,/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Compiler/intel2020/openmpi/4.0.3/lib -L/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Compiler/intel2020/openmpi/4.0.3/lib -Wl,-rpath,/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/libfabric/1.10.1/lib -L/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/libfabric/1.10.1/lib -Wl,-rpath,/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/ucx/1.8.0/lib -L/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/ucx/1.8.0/lib -Wl,-rpath,/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/imkl/2020.1.217/lib/intel64 -L/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/imkl/2020.1.217/lib/intel64 -Wl,-rpath,/cvmfs/restricted.computecanada.ca/easybuild/software/2020/Core/intel/2020.1.217/compilers_and_libraries_2020.1.217/linux/tbb/lib/intel64/gcc4.8 -L/cvmfs/restricted.computecanada.ca/easybuild/software/2020/Core/intel/2020.1.217/compilers_and_libraries_2020.1.217/linux/tbb/lib/intel64/gcc4.8 -Wl,-rpath,/cvmfs/restricted.computecanada.ca/easybuild/software/2020/Core/intel/2020.1.217/compilers_and_libraries_2020.1.217/linux/compiler/lib/intel64_lin -L/cvmfs/restricted.computecanada.ca/easybuild/software/2020/Core/intel/2020.1.217/compilers_and_libraries_2020.1.217/linux/compiler/lib/intel64_lin -Wl,-rpath,/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Compiler/intel2020/openmpi/4.0.3/lib64 -L/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Compiler/intel2020/openmpi/4.0.3/lib64 -Wl,-rpath,/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/gcccore/9.3.0/lib/gcc/x86_64-pc-linux-gnu/9.3.0 -L/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/gcccore/9.3.0/lib/gcc/x86_64-pc-linux-gnu/9.3.0 -Wl,-rpath,/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/gcccore/9.3.0/lib64 -L/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/gcccore/9.3.0/lib64 -Wl,-rpath,/cvmfs/soft.computecanada.ca/gentoo/2020/lib64 -L/cvmfs/soft.computecanada.ca/gentoo/2020/lib64 -Wl,-rpath,/cvmfs/soft.computecanada.ca/gentoo/2020/usr/lib64 -L/cvmfs/soft.computecanada.ca/gentoo/2020/usr/lib64 -Wl,-rpath,/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/gcccore/9.3.0/lib -L/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/gcccore/9.3.0/lib -Wl,-rpath,/cvmfs/soft.computecanada.ca/gentoo/2020/lib -L/cvmfs/soft.computecanada.ca/gentoo/2020/lib -Wl,-rpath,/cvmfs/soft.computecanada.ca/gentoo/2020/usr/lib -L/cvmfs/soft.computecanada.ca/gentoo/2020/usr/lib -lHYPRE -lcmumps -ldmumps -lsmumps -lzmumps -lmumps_common -lpord -lscalapack -lumfpack -lklu -lcholmod -lbtf -lccolamd -lcolamd -lcamd -lamd -lsuitesparseconfig -lsuperlu -lmkl_rt -lmkl_intel_thread -lmkl_core -liomp5 -lpthread -lparmmg -lmmg -lmmg3d -lptesmumps -lptscotchparmetis -lptscotch -lptscotcherr -lesmumps -lscotch -lscotcherr -lpthread -lparmetis -lmetis -ltet -lstdc++ -ldl -lmpi_usempif08 -lmpi_usempi_ignore_tkr -lmpi_mpifh -lmpi -lifport -lifcoremt_pic -limf -lsvml -lm -lipgo -lirc -lpthread -lgcc_s -lirc_s -lrt -lquadmath -lstdc++ -ldl
-----------------------------------------

#############################################################################################################
#############################################################################################################
########     Finsihed Code with NL 400 Processors             #################################################
#############################################################################################################
#############################################################################################################

scontrol show jobid 6265893
JobId=6265893 JobName=heat_strong_HYPRE
   UserId=sudhipv(3061414) GroupId=asarkar(6001189) MCS_label=N/A
   Priority=2049407 Nice=0 Account=def-asarkar QOS=normal
   JobState=COMPLETED Reason=None Dependency=(null)
   Requeue=0 Restarts=0 BatchFlag=1 Reboot=0 ExitCode=0:0
   RunTime=00:12:42 TimeLimit=00:30:00 TimeMin=N/A
   SubmitTime=2021-10-21T15:39:07 EligibleTime=2021-10-21T15:39:07
   AccrueTime=2021-10-21T15:39:07
   StartTime=2021-10-21T15:39:07 EndTime=2021-10-21T15:51:49 Deadline=N/A
   SuspendTime=None SecsPreSuspend=0 LastSchedEval=2021-10-21T15:39:07
   Partition=compute AllocNode:Sid=nia-login05:197179
   ReqNodeList=(null) ExcNodeList=(null)
   NodeList=nia[0309-0311,0383,1496-1506]
   BatchHost=nia0309
   NumNodes=15 NumCPUs=1200 NumTasks=600 CPUs/Task=1 ReqB:S:C:T=0:0:*:*
   TRES=cpu=1200,mem=2625000M,node=15,billing=600
   Socks/Node=* NtasksPerN:B:S:C=40:0:*:* CoreSpec=*
   MinCPUsNode=40 MinMemoryNode=175000M MinTmpDiskNode=0
   Features=[skylake|cascade] DelayBoot=00:00:00
   OverSubscribe=NO Contiguous=0 Licenses=(null) Network=(null)
   Command=/gpfs/fs0/scratch/a/asarkar/sudhipv/poisson_hpc/strong_heat/runff_niagara_heat.sh
   WorkDir=/gpfs/fs0/scratch/a/asarkar/sudhipv/poisson_hpc/strong_heat
   StdErr=/gpfs/fs0/scratch/a/asarkar/sudhipv/poisson_hpc/strong_heat/heat_strong_HYPRE-6265893.out
   StdIn=/dev/null
   StdOut=/gpfs/fs0/scratch/a/asarkar/sudhipv/poisson_hpc/strong_heat/heat_strong_HYPRE-6265893.out
   Power=
   MailUser=sudhipv@cmail.carleton.ca MailType=BEGIN,END,FAIL,REQUEUE,STAGE_OUT

sacct -j 6265893
       JobID    JobName    Account    Elapsed  MaxVMSize     MaxRSS  SystemCPU    UserCPU ExitCode 
------------ ---------- ---------- ---------- ---------- ---------- ---------- ---------- -------- 
6265893      heat_stro+ def-asark+   00:12:42                        51:25.173 1-15:30:10      0:0 
6265893.bat+      batch def-asark+   00:12:42  96492644K  76582280K  07:07.232   07:41:00      0:0 
6265893.ext+     extern def-asark+   00:12:42    138360K      1072K  00:00.003  00:00.009      0:0 
6265893.0         orted def-asark+   00:05:11  84461380K  65948980K  01:44.174   03:09:55      0:0 
6265893.1         orted def-asark+   00:02:19  93851992K  75239468K  03:45.247   04:22:29      0:0 
6265893.2         orted def-asark+   00:01:41  94925828K  76413452K  06:52.416   05:02:01      0:0 
6265893.3         orted def-asark+   00:01:15  88037796K  69212792K  08:12.656   05:29:33      0:0 
6265893.4         orted def-asark+   00:01:06  90172732K  71521128K  09:53.035   06:10:59      0:0 
6265893.5         orted def-asark+   00:01:00  95748460K  77021632K  13:50.407   07:34:11      0:0 
