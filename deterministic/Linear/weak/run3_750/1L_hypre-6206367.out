/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Compiler/intel2020/openmpi/4.0.3/bin/mpiexec
/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Compiler/intel2020/openmpi/4.0.3/bin/mpirun
Number of Vertices for fine mesh564001
Linear solve converged due to CONVERGED_RTOL iterations 6
KSP final norm of residual 2.41742e-07
time taken for solve is3.25297
L2 Norm of uerr is7.5842e-07
************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

/home/a/asarkar/sudhipv/software/FreeFem-sources/src/mpi/FreeFem++-mpi on a  named nia0525.scinet.local with 40 processors, by sudhipv Thu Oct  7 06:39:00 2021
Using Petsc Release Version 3.15.0, Mar 30, 2021 

                         Max       Max/Min     Avg       Total
Time (sec):           3.899e+00     1.000   3.899e+00
Objects:              4.000e+01     1.000   4.000e+01
Flop:                 3.056e+06     1.045   3.001e+06  1.200e+08
Flop/sec:             7.840e+05     1.045   7.698e+05  3.079e+07
MPI Messages:         9.600e+01     4.000   5.580e+01  2.232e+03
MPI Message Lengths:  6.598e+04     2.441   9.459e+02  2.111e+06
MPI Reductions:       5.500e+01     1.000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flop
                            and VecAXPY() for complex vectors of length N --> 8N flop

Summary of Stages:   ----- Time ------  ----- Flop ------  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total    Count   %Total     Avg         %Total    Count   %Total
 0:      Main Stage: 3.8986e+00 100.0%  1.2005e+08 100.0%  2.232e+03 100.0%  9.459e+02      100.0%  3.700e+01  67.3%

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flop: Max - maximum over all processors
                  Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   AvgLen: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flop in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flop over all processors)/(max time over all processors)
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flop                              --- Global ---  --- Stage ----  Total
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   AvgLen  Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s
------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

BuildTwoSided          2 1.0 1.0699e-03 2.2 0.00e+00 0.0 1.9e+02 4.0e+00 2.0e+00  0  0  8  0  4   0  0  8  0  5     0
BuildTwoSidedF         1 1.0 1.3032e-03 1.5 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  2   0  0  0  0  3     0
MatMult                7 1.0 8.6869e-03 1.2 1.30e+06 1.0 1.7e+03 6.0e+02 1.0e+00  0 43 75 48  2   0 43 75 48  3  5899
MatConvert             1 1.0 4.0422e-02 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 4.0e+00  1  0  0  0  7   1  0  0  0 11     0
MatAssemblyBegin       3 1.0 2.9165e-03 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  2   0  0  0  0  3     0
MatAssemblyEnd         3 1.0 5.7041e-02 5.6 0.00e+00 0.0 0.0e+00 0.0e+00 8.0e+00  1  0  0  0 15   1  0  0  0 22     0
MatGetRowIJ            2 1.0 2.9246e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecMDot                6 1.0 5.2411e-03 4.3 6.03e+05 1.0 0.0e+00 0.0e+00 6.0e+00  0 20  0  0 11   0 20  0  0 16  4520
VecNorm                8 1.0 3.6173e-01 1.0 2.30e+05 1.0 0.0e+00 0.0e+00 8.0e+00  9  8  0  0 15   9  8  0  0 22    25
VecScale               7 1.0 3.0826e-04 2.6 1.01e+05 1.0 0.0e+00 0.0e+00 0.0e+00  0  3  0  0  0   0  3  0  0  0 12807
VecCopy                3 1.0 3.9539e-04 7.7 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecSet                10 1.0 1.6787e-04 1.8 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecAXPY                1 1.0 3.2131e-02 1.0 2.87e+04 1.0 0.0e+00 0.0e+00 0.0e+00  1  1  0  0  0   1  1  0  0  0    35
VecAYPX                1 1.0 2.8495e-0418.2 1.44e+04 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  1979
VecMAXPY               7 1.0 6.0087e-04 3.3 7.76e+05 1.0 0.0e+00 0.0e+00 0.0e+00  0 25  0  0  0   0 25  0  0  0 50687
VecScatterBegin        7 1.0 5.7438e-03 1.1 0.00e+00 0.0 1.7e+03 6.0e+02 1.0e+00  0  0 75 48  2   0  0 75 48  3     0
VecScatterEnd          7 1.0 1.3964e-0319.4 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecNormalize           7 1.0 3.6183e-01 1.0 3.02e+05 1.0 0.0e+00 0.0e+00 7.0e+00  9 10  0  0 13   9 10  0  0 19    33
SFSetGraph             1 1.0 6.5700e-07 1.5 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
SFSetUp                1 1.0 1.9661e-03 1.3 0.00e+00 0.0 3.7e+02 1.8e+02 1.0e+00  0  0 17  3  2   0  0 17  3  3     0
SFPack                 7 1.0 3.2171e-04 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
SFUnpack               7 1.0 3.8330e-06 2.9 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
KSPSetUp               1 1.0 1.5365e-03 1.3 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
KSPSolve               1 1.0 6.0272e-01 1.0 2.83e+06 1.0 1.5e+03 5.9e+02 1.4e+01 15 92 67 42 25  15 92 67 42 38   184
KSPGMRESOrthog         6 1.0 6.5338e-03 2.6 1.21e+06 1.0 0.0e+00 0.0e+00 6.0e+00  0 39  0  0 11   0 39  0  0 16  7251
PCSetUp                1 1.0 1.0270e-01 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 4.0e+00  3  0  0  0  7   3  0  0  0 11     0
PCApply                7 1.0 4.4883e-02 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0
------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     5              5      3643892     0.
              Vector    22             22      2020320     0.
           Index Set     3              3        64168     0.
   Star Forest Graph     3              3         3384     0.
       Krylov Solver     1              1        18856     0.
      Preconditioner     1              1         1496     0.
              Viewer     2              1          848     0.
    Distributed Mesh     1              1         5048     0.
     Discrete System     1              1          904     0.
           Weak Form     1              1          824     0.
========================================================================================================================
Average time to get PetscTime(): 2.89e-08
Average time for MPI_Barrier(): 9.9113e-05
Average time for zero size MPI_Send(): 1.10775e-06
#PETSc Option Table entries:
-ksp_converged_reason
-ksp_pc_side right
-ksp_rtol 1e-8
-ksp_view_final_residual
-log_view
-m 750
-n 750
-nw
-pc_type hypre
#End of PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: --prefix=/home/a/asarkar/sudhipv/software/FreeFem-sources/install/ff-petsc/r MAKEFLAGS= --with-debugging=0 COPTFLAGS="-O3 -mtune=native" CXXOPTFLAGS="-O3 -mtune=native" FOPTFLAGS="-O3 -mtune=native" --with-cxx-dialect=C++11 --with-ssl=0 --with-x=0 --with-fortran-bindings=0 --with-cudac=0 --with-cc=/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Compiler/intel2020/openmpi/4.0.3/bin/mpicc --with-cxx=/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Compiler/intel2020/openmpi/4.0.3/bin/mpic++ --with-fc=/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Compiler/intel2020/openmpi/4.0.3/bin/mpif90 --with-scalar-type=real --with-blaslapack-include=/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/imkl/2020.1.217/mkl/include --with-blaslapack-lib="-Wl,-rpath,/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/imkl/2020.1.217/mkl/lib/intel64 -L/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/imkl/2020.1.217/mkl/lib/intel64 -lmkl_rt -lmkl_intel_thread -lmkl_core  -liomp5 -lpthread" --download-metis --download-ptscotch --download-hypre --download-parmetis --download-mmg --download-parmmg --download-superlu --download-suitesparse --download-tetgen --download-slepc --download-hpddm --download-scalapack --download-mumps --download-slepc-configure-arguments=--download-arpack=https://github.com/prj-/arpack-ng/archive/b64dccb.tar.gz PETSC_ARCH=fr
-----------------------------------------
Libraries compiled on 2021-05-02 15:39:49 on nia-login03.scinet.local 
Machine characteristics: Linux-3.10.0-957.27.2.el7.x86_64-x86_64-with-centos-7.6.1810-Core
Using PETSc directory: /home/a/asarkar/sudhipv/software/FreeFem-sources/install/ff-petsc/r
Using PETSc arch: 
-----------------------------------------

Using C compiler: /cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Compiler/intel2020/openmpi/4.0.3/bin/mpicc  -fPIC -wd1572 -O3 -mtune=native   
Using Fortran compiler: /cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Compiler/intel2020/openmpi/4.0.3/bin/mpif90  -fPIC -O3 -mtune=native     
-----------------------------------------

Using include paths: -I/home/a/asarkar/sudhipv/software/FreeFem-sources/install/ff-petsc/r/include -I/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/imkl/2020.1.217/mkl/include
-----------------------------------------

Using C linker: /cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Compiler/intel2020/openmpi/4.0.3/bin/mpicc
Using Fortran linker: /cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Compiler/intel2020/openmpi/4.0.3/bin/mpif90
Using libraries: -Wl,-rpath,/home/a/asarkar/sudhipv/software/FreeFem-sources/install/ff-petsc/r/lib -L/home/a/asarkar/sudhipv/software/FreeFem-sources/install/ff-petsc/r/lib -lpetsc -Wl,-rpath,/home/a/asarkar/sudhipv/software/FreeFem-sources/install/ff-petsc/r/lib -L/home/a/asarkar/sudhipv/software/FreeFem-sources/install/ff-petsc/r/lib -Wl,-rpath,/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/imkl/2020.1.217/mkl/lib/intel64 -L/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/imkl/2020.1.217/mkl/lib/intel64 -Wl,-rpath,/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Compiler/intel2020/openmpi/4.0.3/lib -L/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Compiler/intel2020/openmpi/4.0.3/lib -Wl,-rpath,/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/libfabric/1.10.1/lib -L/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/libfabric/1.10.1/lib -Wl,-rpath,/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/ucx/1.8.0/lib -L/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/ucx/1.8.0/lib -Wl,-rpath,/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/imkl/2020.1.217/lib/intel64 -L/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/imkl/2020.1.217/lib/intel64 -Wl,-rpath,/cvmfs/restricted.computecanada.ca/easybuild/software/2020/Core/intel/2020.1.217/compilers_and_libraries_2020.1.217/linux/tbb/lib/intel64/gcc4.8 -L/cvmfs/restricted.computecanada.ca/easybuild/software/2020/Core/intel/2020.1.217/compilers_and_libraries_2020.1.217/linux/tbb/lib/intel64/gcc4.8 -Wl,-rpath,/cvmfs/restricted.computecanada.ca/easybuild/software/2020/Core/intel/2020.1.217/compilers_and_libraries_2020.1.217/linux/compiler/lib/intel64_lin -L/cvmfs/restricted.computecanada.ca/easybuild/software/2020/Core/intel/2020.1.217/compilers_and_libraries_2020.1.217/linux/compiler/lib/intel64_lin -Wl,-rpath,/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Compiler/intel2020/openmpi/4.0.3/lib64 -L/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Compiler/intel2020/openmpi/4.0.3/lib64 -Wl,-rpath,/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/gcccore/9.3.0/lib/gcc/x86_64-pc-linux-gnu/9.3.0 -L/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/gcccore/9.3.0/lib/gcc/x86_64-pc-linux-gnu/9.3.0 -Wl,-rpath,/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/gcccore/9.3.0/lib64 -L/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/gcccore/9.3.0/lib64 -Wl,-rpath,/cvmfs/soft.computecanada.ca/gentoo/2020/lib64 -L/cvmfs/soft.computecanada.ca/gentoo/2020/lib64 -Wl,-rpath,/cvmfs/soft.computecanada.ca/gentoo/2020/usr/lib64 -L/cvmfs/soft.computecanada.ca/gentoo/2020/usr/lib64 -Wl,-rpath,/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/gcccore/9.3.0/lib -L/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/gcccore/9.3.0/lib -Wl,-rpath,/cvmfs/soft.computecanada.ca/gentoo/2020/lib -L/cvmfs/soft.computecanada.ca/gentoo/2020/lib -Wl,-rpath,/cvmfs/soft.computecanada.ca/gentoo/2020/usr/lib -L/cvmfs/soft.computecanada.ca/gentoo/2020/usr/lib -lHYPRE -lcmumps -ldmumps -lsmumps -lzmumps -lmumps_common -lpord -lscalapack -lumfpack -lklu -lcholmod -lbtf -lccolamd -lcolamd -lcamd -lamd -lsuitesparseconfig -lsuperlu -lmkl_rt -lmkl_intel_thread -lmkl_core -liomp5 -lpthread -lparmmg -lmmg -lmmg3d -lptesmumps -lptscotchparmetis -lptscotch -lptscotcherr -lesmumps -lscotch -lscotcherr -lpthread -lparmetis -lmetis -ltet -lstdc++ -ldl -lmpi_usempif08 -lmpi_usempi_ignore_tkr -lmpi_mpifh -lmpi -lifport -lifcoremt_pic -limf -lsvml -lm -lipgo -lirc -lpthread -lgcc_s -lirc_s -lrt -lquadmath -lstdc++ -ldl
-----------------------------------------

#############################################################################################################
#############################################################################################################
########     Finsihed Code with NL 10 Processors             #################################################
#############################################################################################################
#############################################################################################################
Number of Vertices for fine mesh1127844
Linear solve converged due to CONVERGED_RTOL iterations 7
KSP final norm of residual 5.84131e-08
time taken for solve is3.91918
L2 Norm of uerr is1.98964e-07
************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

/home/a/asarkar/sudhipv/software/FreeFem-sources/src/mpi/FreeFem++-mpi on a  named nia0525.scinet.local with 80 processors, by sudhipv Thu Oct  7 06:39:08 2021
Using Petsc Release Version 3.15.0, Mar 30, 2021 

                         Max       Max/Min     Avg       Total
Time (sec):           4.074e+00     1.001   4.073e+00
Objects:              4.000e+01     1.000   4.000e+01
Flop:                 3.730e+06     1.048   3.650e+06  2.920e+08
Flop/sec:             9.160e+05     1.048   8.960e+05  7.168e+07
MPI Messages:         1.040e+02     4.000   6.630e+01  5.304e+03
MPI Message Lengths:  7.433e+04     3.055   8.993e+02  4.770e+06
MPI Reductions:       5.700e+01     1.000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flop
                            and VecAXPY() for complex vectors of length N --> 8N flop

Summary of Stages:   ----- Time ------  ----- Flop ------  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total    Count   %Total     Avg         %Total    Count   %Total
 0:      Main Stage: 4.0733e+00 100.0%  2.9197e+08 100.0%  5.304e+03 100.0%  8.993e+02      100.0%  3.900e+01  68.4%

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flop: Max - maximum over all processors
                  Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   AvgLen: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flop in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flop over all processors)/(max time over all processors)
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flop                              --- Global ---  --- Stage ----  Total
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   AvgLen  Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s
------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

BuildTwoSided          2 1.0 5.4610e-0317.7 0.00e+00 0.0 4.1e+02 4.0e+00 2.0e+00  0  0  8  0  4   0  0  8  0  5     0
BuildTwoSidedF         1 1.0 4.3688e-0320.1 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  2   0  0  0  0  3     0
MatMult                8 1.0 8.6789e-03 2.9 1.50e+06 1.0 4.1e+03 6.0e+02 1.0e+00  0 40 77 51  2   0 40 77 51  3 13499
MatConvert             1 1.0 1.1626e-02 1.4 0.00e+00 0.0 0.0e+00 0.0e+00 4.0e+00  0  0  0  0  7   0  0  0  0 10     0
MatAssemblyBegin       3 1.0 4.4231e-03 2.0 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  2   0  0  0  0  3     0
MatAssemblyEnd         3 1.0 6.6646e-02 5.9 0.00e+00 0.0 0.0e+00 0.0e+00 8.0e+00  1  0  0  0 14   1  0  0  0 21     0
MatGetRowIJ            2 1.0 3.0094e-04160.8 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecMDot                7 1.0 1.2345e-02 7.2 8.07e+05 1.0 0.0e+00 0.0e+00 7.0e+00  0 22  0  0 12   0 22  0  0 18  5116
VecNorm                9 1.0 7.8446e-02 1.0 2.59e+05 1.0 0.0e+00 0.0e+00 9.0e+00  2  7  0  0 16   2  7  0  0 23   259
VecScale               8 1.0 6.4439e-04 9.3 1.15e+05 1.0 0.0e+00 0.0e+00 0.0e+00  0  3  0  0  0   0  3  0  0  0 14002
VecCopy                3 1.0 3.2394e-04 6.4 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecSet                11 1.0 1.9168e-04 1.8 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecAXPY                1 1.0 1.2731e-0325.6 2.88e+04 1.0 0.0e+00 0.0e+00 0.0e+00  0  1  0  0  0   0  1  0  0  0  1772
VecAYPX                1 1.0 3.4188e-0422.2 1.44e+04 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  3299
VecMAXPY               8 1.0 5.6347e-04 2.8 1.01e+06 1.0 0.0e+00 0.0e+00 0.0e+00  0 27  0  0  0   0 27  0  0  0 140112
VecScatterBegin        8 1.0 6.1536e-03 4.7 0.00e+00 0.0 4.1e+03 6.0e+02 1.0e+00  0  0 77 51  2   0  0 77 51  3     0
VecScatterEnd          8 1.0 4.9780e-0385.6 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecNormalize           8 1.0 7.7143e-02 1.0 3.46e+05 1.0 0.0e+00 0.0e+00 8.0e+00  2  9  0  0 14   2  9  0  0 21   351
SFSetGraph             1 1.0 3.2110e-06 8.7 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
SFSetUp                1 1.0 2.0781e-03 1.9 0.00e+00 0.0 8.2e+02 1.8e+02 1.0e+00  0  0 15  3  2   0  0 15  3  3     0
SFPack                 8 1.0 3.3958e-0423.3 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
SFUnpack               8 1.0 3.7690e-06 2.5 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
KSPSetUp               1 1.0 1.4172e-0313.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
KSPSolve               1 1.0 1.5739e-01 1.0 3.50e+06 1.0 3.7e+03 5.8e+02 1.6e+01  4 94 69 45 28   4 94 69 45 41  1740
KSPGMRESOrthog         7 1.0 1.2788e-02 4.1 1.61e+06 1.0 0.0e+00 0.0e+00 7.0e+00  0 43  0  0 12   0 43  0  0 18  9878
PCSetUp                1 1.0 8.6844e-02 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 4.0e+00  2  0  0  0  7   2  0  0  0 10     0
PCApply                8 1.0 6.5961e-02 1.3 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0
------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     5              5      3669364     0.
              Vector    22             22      2034832     0.
           Index Set     3              3        63992     0.
   Star Forest Graph     3              3         3384     0.
       Krylov Solver     1              1        18856     0.
      Preconditioner     1              1         1496     0.
              Viewer     2              1          848     0.
    Distributed Mesh     1              1         5048     0.
     Discrete System     1              1          904     0.
           Weak Form     1              1          824     0.
========================================================================================================================
Average time to get PetscTime(): 3.18e-08
Average time for MPI_Barrier(): 0.000118247
Average time for zero size MPI_Send(): 1.15613e-06
#PETSc Option Table entries:
-ksp_converged_reason
-ksp_pc_side right
-ksp_rtol 1e-8
-ksp_view_final_residual
-log_view
-m 1061
-n 1061
-nw
-pc_type hypre
#End of PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: --prefix=/home/a/asarkar/sudhipv/software/FreeFem-sources/install/ff-petsc/r MAKEFLAGS= --with-debugging=0 COPTFLAGS="-O3 -mtune=native" CXXOPTFLAGS="-O3 -mtune=native" FOPTFLAGS="-O3 -mtune=native" --with-cxx-dialect=C++11 --with-ssl=0 --with-x=0 --with-fortran-bindings=0 --with-cudac=0 --with-cc=/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Compiler/intel2020/openmpi/4.0.3/bin/mpicc --with-cxx=/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Compiler/intel2020/openmpi/4.0.3/bin/mpic++ --with-fc=/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Compiler/intel2020/openmpi/4.0.3/bin/mpif90 --with-scalar-type=real --with-blaslapack-include=/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/imkl/2020.1.217/mkl/include --with-blaslapack-lib="-Wl,-rpath,/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/imkl/2020.1.217/mkl/lib/intel64 -L/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/imkl/2020.1.217/mkl/lib/intel64 -lmkl_rt -lmkl_intel_thread -lmkl_core  -liomp5 -lpthread" --download-metis --download-ptscotch --download-hypre --download-parmetis --download-mmg --download-parmmg --download-superlu --download-suitesparse --download-tetgen --download-slepc --download-hpddm --download-scalapack --download-mumps --download-slepc-configure-arguments=--download-arpack=https://github.com/prj-/arpack-ng/archive/b64dccb.tar.gz PETSC_ARCH=fr
-----------------------------------------
Libraries compiled on 2021-05-02 15:39:49 on nia-login03.scinet.local 
Machine characteristics: Linux-3.10.0-957.27.2.el7.x86_64-x86_64-with-centos-7.6.1810-Core
Using PETSc directory: /home/a/asarkar/sudhipv/software/FreeFem-sources/install/ff-petsc/r
Using PETSc arch: 
-----------------------------------------

Using C compiler: /cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Compiler/intel2020/openmpi/4.0.3/bin/mpicc  -fPIC -wd1572 -O3 -mtune=native   
Using Fortran compiler: /cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Compiler/intel2020/openmpi/4.0.3/bin/mpif90  -fPIC -O3 -mtune=native     
-----------------------------------------

Using include paths: -I/home/a/asarkar/sudhipv/software/FreeFem-sources/install/ff-petsc/r/include -I/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/imkl/2020.1.217/mkl/include
-----------------------------------------

Using C linker: /cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Compiler/intel2020/openmpi/4.0.3/bin/mpicc
Using Fortran linker: /cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Compiler/intel2020/openmpi/4.0.3/bin/mpif90
Using libraries: -Wl,-rpath,/home/a/asarkar/sudhipv/software/FreeFem-sources/install/ff-petsc/r/lib -L/home/a/asarkar/sudhipv/software/FreeFem-sources/install/ff-petsc/r/lib -lpetsc -Wl,-rpath,/home/a/asarkar/sudhipv/software/FreeFem-sources/install/ff-petsc/r/lib -L/home/a/asarkar/sudhipv/software/FreeFem-sources/install/ff-petsc/r/lib -Wl,-rpath,/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/imkl/2020.1.217/mkl/lib/intel64 -L/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/imkl/2020.1.217/mkl/lib/intel64 -Wl,-rpath,/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Compiler/intel2020/openmpi/4.0.3/lib -L/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Compiler/intel2020/openmpi/4.0.3/lib -Wl,-rpath,/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/libfabric/1.10.1/lib -L/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/libfabric/1.10.1/lib -Wl,-rpath,/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/ucx/1.8.0/lib -L/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/ucx/1.8.0/lib -Wl,-rpath,/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/imkl/2020.1.217/lib/intel64 -L/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/imkl/2020.1.217/lib/intel64 -Wl,-rpath,/cvmfs/restricted.computecanada.ca/easybuild/software/2020/Core/intel/2020.1.217/compilers_and_libraries_2020.1.217/linux/tbb/lib/intel64/gcc4.8 -L/cvmfs/restricted.computecanada.ca/easybuild/software/2020/Core/intel/2020.1.217/compilers_and_libraries_2020.1.217/linux/tbb/lib/intel64/gcc4.8 -Wl,-rpath,/cvmfs/restricted.computecanada.ca/easybuild/software/2020/Core/intel/2020.1.217/compilers_and_libraries_2020.1.217/linux/compiler/lib/intel64_lin -L/cvmfs/restricted.computecanada.ca/easybuild/software/2020/Core/intel/2020.1.217/compilers_and_libraries_2020.1.217/linux/compiler/lib/intel64_lin -Wl,-rpath,/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Compiler/intel2020/openmpi/4.0.3/lib64 -L/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Compiler/intel2020/openmpi/4.0.3/lib64 -Wl,-rpath,/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/gcccore/9.3.0/lib/gcc/x86_64-pc-linux-gnu/9.3.0 -L/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/gcccore/9.3.0/lib/gcc/x86_64-pc-linux-gnu/9.3.0 -Wl,-rpath,/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/gcccore/9.3.0/lib64 -L/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/gcccore/9.3.0/lib64 -Wl,-rpath,/cvmfs/soft.computecanada.ca/gentoo/2020/lib64 -L/cvmfs/soft.computecanada.ca/gentoo/2020/lib64 -Wl,-rpath,/cvmfs/soft.computecanada.ca/gentoo/2020/usr/lib64 -L/cvmfs/soft.computecanada.ca/gentoo/2020/usr/lib64 -Wl,-rpath,/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/gcccore/9.3.0/lib -L/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/gcccore/9.3.0/lib -Wl,-rpath,/cvmfs/soft.computecanada.ca/gentoo/2020/lib -L/cvmfs/soft.computecanada.ca/gentoo/2020/lib -Wl,-rpath,/cvmfs/soft.computecanada.ca/gentoo/2020/usr/lib -L/cvmfs/soft.computecanada.ca/gentoo/2020/usr/lib -lHYPRE -lcmumps -ldmumps -lsmumps -lzmumps -lmumps_common -lpord -lscalapack -lumfpack -lklu -lcholmod -lbtf -lccolamd -lcolamd -lcamd -lamd -lsuitesparseconfig -lsuperlu -lmkl_rt -lmkl_intel_thread -lmkl_core -liomp5 -lpthread -lparmmg -lmmg -lmmg3d -lptesmumps -lptscotchparmetis -lptscotch -lptscotcherr -lesmumps -lscotch -lscotcherr -lpthread -lparmetis -lmetis -ltet -lstdc++ -ldl -lmpi_usempif08 -lmpi_usempi_ignore_tkr -lmpi_mpifh -lmpi -lifport -lifcoremt_pic -limf -lsvml -lm -lipgo -lirc -lpthread -lgcc_s -lirc_s -lrt -lquadmath -lstdc++ -ldl
-----------------------------------------

#############################################################################################################
#############################################################################################################
########     Finsihed Code with NL 10 Processors             #################################################
#############################################################################################################
#############################################################################################################
Number of Vertices for fine mesh2253001
Linear solve converged due to CONVERGED_RTOL iterations 7
KSP final norm of residual 9.95121e-08
time taken for solve is7.33458
L2 Norm of uerr is8.2736e-07
************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

/home/a/asarkar/sudhipv/software/FreeFem-sources/src/mpi/FreeFem++-mpi on a  named nia0525.scinet.local with 160 processors, by sudhipv Thu Oct  7 06:39:19 2021
Using Petsc Release Version 3.15.0, Mar 30, 2021 

                         Max       Max/Min     Avg       Total
Time (sec):           7.413e+00     1.000   7.412e+00
Objects:              4.000e+01     1.000   4.000e+01
Flop:                 3.725e+06     1.047   3.646e+06  5.833e+08
Flop/sec:             5.025e+05     1.047   4.919e+05  7.870e+07
MPI Messages:         1.040e+02     4.000   6.906e+01  1.105e+04
MPI Message Lengths:  8.378e+04     2.973   8.978e+02  9.921e+06
MPI Reductions:       5.700e+01     1.000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flop
                            and VecAXPY() for complex vectors of length N --> 8N flop

Summary of Stages:   ----- Time ------  ----- Flop ------  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total    Count   %Total     Avg         %Total    Count   %Total
 0:      Main Stage: 7.4122e+00 100.0%  5.8333e+08 100.0%  1.105e+04 100.0%  8.978e+02      100.0%  3.900e+01  68.4%

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flop: Max - maximum over all processors
                  Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   AvgLen: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flop in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flop over all processors)/(max time over all processors)
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flop                              --- Global ---  --- Stage ----  Total
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   AvgLen  Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s
------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

BuildTwoSided          2 1.0 5.7675e-0310.3 0.00e+00 0.0 8.5e+02 4.0e+00 2.0e+00  0  0  8  0  4   0  0  8  0  5     0
BuildTwoSidedF         1 1.0 3.8172e-03 5.7 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  2   0  0  0  0  3     0
MatMult                8 1.0 8.8020e-03 2.6 1.50e+06 1.0 8.5e+03 5.9e+02 1.0e+00  0 40 77 51  2   0 40 77 51  3 26599
MatConvert             1 1.0 1.2826e-02 1.4 0.00e+00 0.0 0.0e+00 0.0e+00 4.0e+00  0  0  0  0  7   0  0  0  0 10     0
MatAssemblyBegin       3 1.0 3.8737e-03 1.6 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  2   0  0  0  0  3     0
MatAssemblyEnd         3 1.0 6.6844e-02 6.8 0.00e+00 0.0 0.0e+00 0.0e+00 8.0e+00  1  0  0  0 14   1  0  0  0 21     0
MatGetRowIJ            2 1.0 2.7800e-04150.4 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecMDot                7 1.0 1.6115e-02 3.1 8.05e+05 1.0 0.0e+00 0.0e+00 7.0e+00  0 22  0  0 12   0 22  0  0 18  7829
VecNorm                9 1.0 8.8302e-02 1.0 2.59e+05 1.0 0.0e+00 0.0e+00 9.0e+00  1  7  0  0 16   1  7  0  0 23   459
VecScale               8 1.0 4.1903e-04 6.3 1.15e+05 1.0 0.0e+00 0.0e+00 0.0e+00  0  3  0  0  0   0  3  0  0  0 43014
VecCopy                3 1.0 4.0850e-04 7.9 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecSet                11 1.0 2.2229e-04 2.3 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecAXPY                1 1.0 1.2452e-0320.7 2.88e+04 1.0 0.0e+00 0.0e+00 0.0e+00  0  1  0  0  0   0  1  0  0  0  3619
VecAYPX                1 1.0 2.6331e-0418.0 1.44e+04 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  8557
VecMAXPY               8 1.0 8.5930e-04 3.9 1.01e+06 1.0 0.0e+00 0.0e+00 0.0e+00  0 27  0  0  0   0 27  0  0  0 183533
VecScatterBegin        8 1.0 6.0755e-03 3.8 0.00e+00 0.0 8.5e+03 5.9e+02 1.0e+00  0  0 77 51  2   0  0 77 51  3     0
VecScatterEnd          8 1.0 4.6176e-0370.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecNormalize           8 1.0 8.6895e-02 1.0 3.45e+05 1.0 0.0e+00 0.0e+00 8.0e+00  1  9  0  0 14   1  9  0  0 21   622
SFSetGraph             1 1.0 2.1460e-06 5.4 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
SFSetUp                1 1.0 2.6667e-03 1.9 0.00e+00 0.0 1.7e+03 1.8e+02 1.0e+00  0  0 15  3  2   0  0 15  3  3     0
SFPack                 8 1.0 4.5458e-0434.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
SFUnpack               8 1.0 7.0240e-06 4.9 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
KSPSetUp               1 1.0 1.3673e-0319.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
KSPSolve               1 1.0 1.6945e-01 1.0 3.49e+06 1.0 7.6e+03 5.8e+02 1.6e+01  2 94 69 45 28   2 94 69 45 41  3230
KSPGMRESOrthog         7 1.0 1.6373e-02 2.5 1.61e+06 1.0 0.0e+00 0.0e+00 7.0e+00  0 43  0  0 12   0 43  0  0 18 15411
PCSetUp                1 1.0 8.5860e-02 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 4.0e+00  1  0  0  0  7   1  0  0  0 10     0
PCApply                8 1.0 6.4728e-02 1.3 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0
------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     5              5      3651272     0.
              Vector    22             22      2024328     0.
           Index Set     3              3        64108     0.
   Star Forest Graph     3              3         3384     0.
       Krylov Solver     1              1        18856     0.
      Preconditioner     1              1         1496     0.
              Viewer     2              1          848     0.
    Distributed Mesh     1              1         5048     0.
     Discrete System     1              1          904     0.
           Weak Form     1              1          824     0.
========================================================================================================================
Average time to get PetscTime(): 3.02e-08
Average time for MPI_Barrier(): 0.000130385
Average time for zero size MPI_Send(): 1.17845e-06
#PETSc Option Table entries:
-ksp_converged_reason
-ksp_pc_side right
-ksp_rtol 1e-8
-ksp_view_final_residual
-log_view
-m 1500
-n 1500
-nw
-pc_type hypre
#End of PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: --prefix=/home/a/asarkar/sudhipv/software/FreeFem-sources/install/ff-petsc/r MAKEFLAGS= --with-debugging=0 COPTFLAGS="-O3 -mtune=native" CXXOPTFLAGS="-O3 -mtune=native" FOPTFLAGS="-O3 -mtune=native" --with-cxx-dialect=C++11 --with-ssl=0 --with-x=0 --with-fortran-bindings=0 --with-cudac=0 --with-cc=/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Compiler/intel2020/openmpi/4.0.3/bin/mpicc --with-cxx=/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Compiler/intel2020/openmpi/4.0.3/bin/mpic++ --with-fc=/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Compiler/intel2020/openmpi/4.0.3/bin/mpif90 --with-scalar-type=real --with-blaslapack-include=/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/imkl/2020.1.217/mkl/include --with-blaslapack-lib="-Wl,-rpath,/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/imkl/2020.1.217/mkl/lib/intel64 -L/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/imkl/2020.1.217/mkl/lib/intel64 -lmkl_rt -lmkl_intel_thread -lmkl_core  -liomp5 -lpthread" --download-metis --download-ptscotch --download-hypre --download-parmetis --download-mmg --download-parmmg --download-superlu --download-suitesparse --download-tetgen --download-slepc --download-hpddm --download-scalapack --download-mumps --download-slepc-configure-arguments=--download-arpack=https://github.com/prj-/arpack-ng/archive/b64dccb.tar.gz PETSC_ARCH=fr
-----------------------------------------
Libraries compiled on 2021-05-02 15:39:49 on nia-login03.scinet.local 
Machine characteristics: Linux-3.10.0-957.27.2.el7.x86_64-x86_64-with-centos-7.6.1810-Core
Using PETSc directory: /home/a/asarkar/sudhipv/software/FreeFem-sources/install/ff-petsc/r
Using PETSc arch: 
-----------------------------------------

Using C compiler: /cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Compiler/intel2020/openmpi/4.0.3/bin/mpicc  -fPIC -wd1572 -O3 -mtune=native   
Using Fortran compiler: /cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Compiler/intel2020/openmpi/4.0.3/bin/mpif90  -fPIC -O3 -mtune=native     
-----------------------------------------

Using include paths: -I/home/a/asarkar/sudhipv/software/FreeFem-sources/install/ff-petsc/r/include -I/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/imkl/2020.1.217/mkl/include
-----------------------------------------

Using C linker: /cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Compiler/intel2020/openmpi/4.0.3/bin/mpicc
Using Fortran linker: /cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Compiler/intel2020/openmpi/4.0.3/bin/mpif90
Using libraries: -Wl,-rpath,/home/a/asarkar/sudhipv/software/FreeFem-sources/install/ff-petsc/r/lib -L/home/a/asarkar/sudhipv/software/FreeFem-sources/install/ff-petsc/r/lib -lpetsc -Wl,-rpath,/home/a/asarkar/sudhipv/software/FreeFem-sources/install/ff-petsc/r/lib -L/home/a/asarkar/sudhipv/software/FreeFem-sources/install/ff-petsc/r/lib -Wl,-rpath,/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/imkl/2020.1.217/mkl/lib/intel64 -L/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/imkl/2020.1.217/mkl/lib/intel64 -Wl,-rpath,/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Compiler/intel2020/openmpi/4.0.3/lib -L/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Compiler/intel2020/openmpi/4.0.3/lib -Wl,-rpath,/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/libfabric/1.10.1/lib -L/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/libfabric/1.10.1/lib -Wl,-rpath,/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/ucx/1.8.0/lib -L/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/ucx/1.8.0/lib -Wl,-rpath,/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/imkl/2020.1.217/lib/intel64 -L/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/imkl/2020.1.217/lib/intel64 -Wl,-rpath,/cvmfs/restricted.computecanada.ca/easybuild/software/2020/Core/intel/2020.1.217/compilers_and_libraries_2020.1.217/linux/tbb/lib/intel64/gcc4.8 -L/cvmfs/restricted.computecanada.ca/easybuild/software/2020/Core/intel/2020.1.217/compilers_and_libraries_2020.1.217/linux/tbb/lib/intel64/gcc4.8 -Wl,-rpath,/cvmfs/restricted.computecanada.ca/easybuild/software/2020/Core/intel/2020.1.217/compilers_and_libraries_2020.1.217/linux/compiler/lib/intel64_lin -L/cvmfs/restricted.computecanada.ca/easybuild/software/2020/Core/intel/2020.1.217/compilers_and_libraries_2020.1.217/linux/compiler/lib/intel64_lin -Wl,-rpath,/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Compiler/intel2020/openmpi/4.0.3/lib64 -L/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Compiler/intel2020/openmpi/4.0.3/lib64 -Wl,-rpath,/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/gcccore/9.3.0/lib/gcc/x86_64-pc-linux-gnu/9.3.0 -L/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/gcccore/9.3.0/lib/gcc/x86_64-pc-linux-gnu/9.3.0 -Wl,-rpath,/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/gcccore/9.3.0/lib64 -L/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/gcccore/9.3.0/lib64 -Wl,-rpath,/cvmfs/soft.computecanada.ca/gentoo/2020/lib64 -L/cvmfs/soft.computecanada.ca/gentoo/2020/lib64 -Wl,-rpath,/cvmfs/soft.computecanada.ca/gentoo/2020/usr/lib64 -L/cvmfs/soft.computecanada.ca/gentoo/2020/usr/lib64 -Wl,-rpath,/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/gcccore/9.3.0/lib -L/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/gcccore/9.3.0/lib -Wl,-rpath,/cvmfs/soft.computecanada.ca/gentoo/2020/lib -L/cvmfs/soft.computecanada.ca/gentoo/2020/lib -Wl,-rpath,/cvmfs/soft.computecanada.ca/gentoo/2020/usr/lib -L/cvmfs/soft.computecanada.ca/gentoo/2020/usr/lib -lHYPRE -lcmumps -ldmumps -lsmumps -lzmumps -lmumps_common -lpord -lscalapack -lumfpack -lklu -lcholmod -lbtf -lccolamd -lcolamd -lcamd -lamd -lsuitesparseconfig -lsuperlu -lmkl_rt -lmkl_intel_thread -lmkl_core -liomp5 -lpthread -lparmmg -lmmg -lmmg3d -lptesmumps -lptscotchparmetis -lptscotch -lptscotcherr -lesmumps -lscotch -lscotcherr -lpthread -lparmetis -lmetis -ltet -lstdc++ -ldl -lmpi_usempif08 -lmpi_usempi_ignore_tkr -lmpi_mpifh -lmpi -lifport -lifcoremt_pic -limf -lsvml -lm -lipgo -lirc -lpthread -lgcc_s -lirc_s -lrt -lquadmath -lstdc++ -ldl
-----------------------------------------

#############################################################################################################
#############################################################################################################
########     Finsihed Code with NL 10 Processors             #################################################
#############################################################################################################
#############################################################################################################
Number of Vertices for fine mesh3381921
Linear solve converged due to CONVERGED_RTOL iterations 7
KSP final norm of residual 2.62204e-07
time taken for solve is10.8068
L2 Norm of uerr is1.64529e-06
************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

/home/a/asarkar/sudhipv/software/FreeFem-sources/src/mpi/FreeFem++-mpi on a  named nia0525.scinet.local with 240 processors, by sudhipv Thu Oct  7 06:39:36 2021
Using Petsc Release Version 3.15.0, Mar 30, 2021 

                         Max       Max/Min     Avg       Total
Time (sec):           1.091e+01     1.000   1.091e+01
Objects:              4.000e+01     1.000   4.000e+01
Flop:                 3.733e+06     1.048   3.649e+06  8.757e+08
Flop/sec:             3.421e+05     1.048   3.345e+05  8.027e+07
MPI Messages:         1.040e+02     4.000   7.117e+01  1.708e+04
MPI Message Lengths:  8.292e+04     3.176   8.793e+02  1.502e+07
MPI Reductions:       5.700e+01     1.000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flop
                            and VecAXPY() for complex vectors of length N --> 8N flop

Summary of Stages:   ----- Time ------  ----- Flop ------  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total    Count   %Total     Avg         %Total    Count   %Total
 0:      Main Stage: 1.0909e+01 100.0%  8.7568e+08 100.0%  1.708e+04 100.0%  8.793e+02      100.0%  3.900e+01  68.4%

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flop: Max - maximum over all processors
                  Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   AvgLen: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flop in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flop over all processors)/(max time over all processors)
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flop                              --- Global ---  --- Stage ----  Total
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   AvgLen  Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s
------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

BuildTwoSided          2 1.0 4.3007e-03 7.3 0.00e+00 0.0 1.3e+03 4.0e+00 2.0e+00  0  0  8  0  4   0  0  8  0  5     0
BuildTwoSidedF         1 1.0 2.8904e-03 4.3 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  2   0  0  0  0  3     0
MatMult                8 1.0 2.6692e-02 8.9 1.50e+06 1.0 1.3e+04 5.8e+02 1.0e+00  0 40 77 51  2   0 40 77 51  3 13168
MatConvert             1 1.0 1.1516e-02 1.4 0.00e+00 0.0 0.0e+00 0.0e+00 4.0e+00  0  0  0  0  7   0  0  0  0 10     0
MatAssemblyBegin       3 1.0 3.2511e-03 1.3 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  2   0  0  0  0  3     0
MatAssemblyEnd         3 1.0 6.6691e-02 5.6 0.00e+00 0.0 0.0e+00 0.0e+00 8.0e+00  1  0  0  0 14   1  0  0  0 21     0
MatGetRowIJ            2 1.0 2.8070e-04143.8 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecMDot                7 1.0 1.7083e-02 2.4 8.07e+05 1.0 0.0e+00 0.0e+00 7.0e+00  0 22  0  0 12   0 22  0  0 18 11086
VecNorm                9 1.0 9.6421e-02 1.3 2.59e+05 1.0 0.0e+00 0.0e+00 9.0e+00  1  7  0  0 16   1  7  0  0 23   631
VecScale               8 1.0 6.9909e-04 6.9 1.15e+05 1.0 0.0e+00 0.0e+00 0.0e+00  0  3  0  0  0   0  3  0  0  0 38701
VecCopy                3 1.0 3.9052e-04 7.5 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecSet                11 1.0 2.0692e-04 2.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecAXPY                1 1.0 1.8341e-02365.2 2.88e+04 1.0 0.0e+00 0.0e+00 0.0e+00  0  1  0  0  0   0  1  0  0  0   369
VecAYPX                1 1.0 3.9923e-0426.2 1.44e+04 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  8471
VecMAXPY               8 1.0 8.5525e-04 4.3 1.01e+06 1.0 0.0e+00 0.0e+00 0.0e+00  0 27  0  0  0   0 27  0  0  0 276801
VecScatterBegin        8 1.0 6.1368e-03 4.7 0.00e+00 0.0 1.3e+04 5.8e+02 1.0e+00  0  0 77 51  2   0  0 77 51  3     0
VecScatterEnd          8 1.0 2.1286e-02342.8 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecNormalize           8 1.0 7.7945e-02 1.0 3.46e+05 1.0 0.0e+00 0.0e+00 8.0e+00  1  9  0  0 14   1  9  0  0 21  1041
SFSetGraph             1 1.0 2.3880e-06 8.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
SFSetUp                1 1.0 2.3675e-03 2.2 0.00e+00 0.0 2.6e+03 1.7e+02 1.0e+00  0  0 15  3  2   0  0 15  3  3     0
SFPack                 8 1.0 3.3649e-0423.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
SFUnpack               8 1.0 4.1170e-06 2.8 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
KSPSetUp               1 1.0 1.4076e-0313.9 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
KSPSolve               1 1.0 1.8141e-01 1.1 3.50e+06 1.0 1.2e+04 5.7e+02 1.6e+01  2 94 69 45 28   2 94 69 45 41  4529
KSPGMRESOrthog         7 1.0 1.7354e-02 2.0 1.61e+06 1.0 0.0e+00 0.0e+00 7.0e+00  0 43  0  0 12   0 43  0  0 18 21826
PCSetUp                1 1.0 8.7607e-02 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 4.0e+00  1  0  0  0  7   1  0  0  0 10     0
PCApply                8 1.0 6.7619e-02 1.3 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0
------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     5              5      3655712     0.
              Vector    22             22      2026680     0.
           Index Set     3              3        64016     0.
   Star Forest Graph     3              3         3384     0.
       Krylov Solver     1              1        18856     0.
      Preconditioner     1              1         1496     0.
              Viewer     2              1          848     0.
    Distributed Mesh     1              1         5048     0.
     Discrete System     1              1          904     0.
           Weak Form     1              1          824     0.
========================================================================================================================
Average time to get PetscTime(): 7.4e-08
Average time for MPI_Barrier(): 9.57518e-05
Average time for zero size MPI_Send(): 1.27655e-06
#PETSc Option Table entries:
-ksp_converged_reason
-ksp_pc_side right
-ksp_rtol 1e-8
-ksp_view_final_residual
-log_view
-m 1838
-n 1838
-nw
-pc_type hypre
#End of PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: --prefix=/home/a/asarkar/sudhipv/software/FreeFem-sources/install/ff-petsc/r MAKEFLAGS= --with-debugging=0 COPTFLAGS="-O3 -mtune=native" CXXOPTFLAGS="-O3 -mtune=native" FOPTFLAGS="-O3 -mtune=native" --with-cxx-dialect=C++11 --with-ssl=0 --with-x=0 --with-fortran-bindings=0 --with-cudac=0 --with-cc=/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Compiler/intel2020/openmpi/4.0.3/bin/mpicc --with-cxx=/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Compiler/intel2020/openmpi/4.0.3/bin/mpic++ --with-fc=/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Compiler/intel2020/openmpi/4.0.3/bin/mpif90 --with-scalar-type=real --with-blaslapack-include=/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/imkl/2020.1.217/mkl/include --with-blaslapack-lib="-Wl,-rpath,/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/imkl/2020.1.217/mkl/lib/intel64 -L/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/imkl/2020.1.217/mkl/lib/intel64 -lmkl_rt -lmkl_intel_thread -lmkl_core  -liomp5 -lpthread" --download-metis --download-ptscotch --download-hypre --download-parmetis --download-mmg --download-parmmg --download-superlu --download-suitesparse --download-tetgen --download-slepc --download-hpddm --download-scalapack --download-mumps --download-slepc-configure-arguments=--download-arpack=https://github.com/prj-/arpack-ng/archive/b64dccb.tar.gz PETSC_ARCH=fr
-----------------------------------------
Libraries compiled on 2021-05-02 15:39:49 on nia-login03.scinet.local 
Machine characteristics: Linux-3.10.0-957.27.2.el7.x86_64-x86_64-with-centos-7.6.1810-Core
Using PETSc directory: /home/a/asarkar/sudhipv/software/FreeFem-sources/install/ff-petsc/r
Using PETSc arch: 
-----------------------------------------

Using C compiler: /cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Compiler/intel2020/openmpi/4.0.3/bin/mpicc  -fPIC -wd1572 -O3 -mtune=native   
Using Fortran compiler: /cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Compiler/intel2020/openmpi/4.0.3/bin/mpif90  -fPIC -O3 -mtune=native     
-----------------------------------------

Using include paths: -I/home/a/asarkar/sudhipv/software/FreeFem-sources/install/ff-petsc/r/include -I/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/imkl/2020.1.217/mkl/include
-----------------------------------------

Using C linker: /cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Compiler/intel2020/openmpi/4.0.3/bin/mpicc
Using Fortran linker: /cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Compiler/intel2020/openmpi/4.0.3/bin/mpif90
Using libraries: -Wl,-rpath,/home/a/asarkar/sudhipv/software/FreeFem-sources/install/ff-petsc/r/lib -L/home/a/asarkar/sudhipv/software/FreeFem-sources/install/ff-petsc/r/lib -lpetsc -Wl,-rpath,/home/a/asarkar/sudhipv/software/FreeFem-sources/install/ff-petsc/r/lib -L/home/a/asarkar/sudhipv/software/FreeFem-sources/install/ff-petsc/r/lib -Wl,-rpath,/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/imkl/2020.1.217/mkl/lib/intel64 -L/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/imkl/2020.1.217/mkl/lib/intel64 -Wl,-rpath,/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Compiler/intel2020/openmpi/4.0.3/lib -L/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Compiler/intel2020/openmpi/4.0.3/lib -Wl,-rpath,/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/libfabric/1.10.1/lib -L/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/libfabric/1.10.1/lib -Wl,-rpath,/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/ucx/1.8.0/lib -L/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/ucx/1.8.0/lib -Wl,-rpath,/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/imkl/2020.1.217/lib/intel64 -L/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/imkl/2020.1.217/lib/intel64 -Wl,-rpath,/cvmfs/restricted.computecanada.ca/easybuild/software/2020/Core/intel/2020.1.217/compilers_and_libraries_2020.1.217/linux/tbb/lib/intel64/gcc4.8 -L/cvmfs/restricted.computecanada.ca/easybuild/software/2020/Core/intel/2020.1.217/compilers_and_libraries_2020.1.217/linux/tbb/lib/intel64/gcc4.8 -Wl,-rpath,/cvmfs/restricted.computecanada.ca/easybuild/software/2020/Core/intel/2020.1.217/compilers_and_libraries_2020.1.217/linux/compiler/lib/intel64_lin -L/cvmfs/restricted.computecanada.ca/easybuild/software/2020/Core/intel/2020.1.217/compilers_and_libraries_2020.1.217/linux/compiler/lib/intel64_lin -Wl,-rpath,/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Compiler/intel2020/openmpi/4.0.3/lib64 -L/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Compiler/intel2020/openmpi/4.0.3/lib64 -Wl,-rpath,/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/gcccore/9.3.0/lib/gcc/x86_64-pc-linux-gnu/9.3.0 -L/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/gcccore/9.3.0/lib/gcc/x86_64-pc-linux-gnu/9.3.0 -Wl,-rpath,/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/gcccore/9.3.0/lib64 -L/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/gcccore/9.3.0/lib64 -Wl,-rpath,/cvmfs/soft.computecanada.ca/gentoo/2020/lib64 -L/cvmfs/soft.computecanada.ca/gentoo/2020/lib64 -Wl,-rpath,/cvmfs/soft.computecanada.ca/gentoo/2020/usr/lib64 -L/cvmfs/soft.computecanada.ca/gentoo/2020/usr/lib64 -Wl,-rpath,/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/gcccore/9.3.0/lib -L/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/gcccore/9.3.0/lib -Wl,-rpath,/cvmfs/soft.computecanada.ca/gentoo/2020/lib -L/cvmfs/soft.computecanada.ca/gentoo/2020/lib -Wl,-rpath,/cvmfs/soft.computecanada.ca/gentoo/2020/usr/lib -L/cvmfs/soft.computecanada.ca/gentoo/2020/usr/lib -lHYPRE -lcmumps -ldmumps -lsmumps -lzmumps -lmumps_common -lpord -lscalapack -lumfpack -lklu -lcholmod -lbtf -lccolamd -lcolamd -lcamd -lamd -lsuitesparseconfig -lsuperlu -lmkl_rt -lmkl_intel_thread -lmkl_core -liomp5 -lpthread -lparmmg -lmmg -lmmg3d -lptesmumps -lptscotchparmetis -lptscotch -lptscotcherr -lesmumps -lscotch -lscotcherr -lpthread -lparmetis -lmetis -ltet -lstdc++ -ldl -lmpi_usempif08 -lmpi_usempi_ignore_tkr -lmpi_mpifh -lmpi -lifport -lifcoremt_pic -limf -lsvml -lm -lipgo -lirc -lpthread -lgcc_s -lirc_s -lrt -lquadmath -lstdc++ -ldl
-----------------------------------------

#############################################################################################################
#############################################################################################################
########     Finsihed Code with NL 10 Processors             #################################################
#############################################################################################################
#############################################################################################################
Number of Vertices for fine mesh5635876
Linear solve converged due to CONVERGED_RTOL iterations 7
KSP final norm of residual 2.83044e-07
time taken for solve is18.4177
L2 Norm of uerr is1.36679e-06
************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

/home/a/asarkar/sudhipv/software/FreeFem-sources/src/mpi/FreeFem++-mpi on a  named nia0525.scinet.local with 400 processors, by sudhipv Thu Oct  7 06:39:58 2021
Using Petsc Release Version 3.15.0, Mar 30, 2021 

                         Max       Max/Min     Avg       Total
Time (sec):           1.851e+01     1.000   1.851e+01
Objects:              4.000e+01     1.000   4.000e+01
Flop:                 3.739e+06     1.051   3.648e+06  1.459e+09
Flop/sec:             2.020e+05     1.051   1.971e+05  7.883e+07
MPI Messages:         1.040e+02     4.000   7.261e+01  2.904e+04
MPI Message Lengths:  7.894e+04     3.080   8.806e+02  2.557e+07
MPI Reductions:       5.700e+01     1.000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flop
                            and VecAXPY() for complex vectors of length N --> 8N flop

Summary of Stages:   ----- Time ------  ----- Flop ------  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total    Count   %Total     Avg         %Total    Count   %Total
 0:      Main Stage: 1.8513e+01 100.0%  1.4594e+09 100.0%  2.904e+04 100.0%  8.806e+02      100.0%  3.900e+01  68.4%

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flop: Max - maximum over all processors
                  Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   AvgLen: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flop in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flop over all processors)/(max time over all processors)
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flop                              --- Global ---  --- Stage ----  Total
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   AvgLen  Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s
------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

BuildTwoSided          2 1.0 4.0435e-03 5.1 0.00e+00 0.0 2.2e+03 4.0e+00 2.0e+00  0  0  8  0  4   0  0  8  0  5     0
BuildTwoSidedF         1 1.0 3.4006e-03 4.0 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  2   0  0  0  0  3     0
MatMult                8 1.0 8.4875e-03 3.8 1.50e+06 1.1 2.2e+04 5.8e+02 1.0e+00  0 40 77 51  2   0 40 77 51  3 69023
MatConvert             1 1.0 1.2779e-02 1.4 0.00e+00 0.0 0.0e+00 0.0e+00 4.0e+00  0  0  0  0  7   0  0  0  0 10     0
MatAssemblyBegin       3 1.0 4.0291e-03 1.4 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  2   0  0  0  0  3     0
MatAssemblyEnd         3 1.0 7.2700e-02 5.4 0.00e+00 0.0 0.0e+00 0.0e+00 8.0e+00  0  0  0  0 14   0  0  0  0 21     0
MatGetRowIJ            2 1.0 2.9777e-04143.8 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecMDot                7 1.0 1.2947e-02 6.0 8.08e+05 1.1 0.0e+00 0.0e+00 7.0e+00  0 22  0  0 12   0 22  0  0 18 24377
VecNorm                9 1.0 9.3001e-02 1.0 2.60e+05 1.1 0.0e+00 0.0e+00 9.0e+00  0  7  0  0 16   0  7  0  0 23  1091
VecScale               8 1.0 7.0570e-04 9.9 1.15e+05 1.1 0.0e+00 0.0e+00 0.0e+00  0  3  0  0  0   0  3  0  0  0 63890
VecCopy                3 1.0 3.7544e-04 7.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecSet                11 1.0 2.1262e-04 2.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecAXPY                1 1.0 1.4493e-0328.2 2.89e+04 1.1 0.0e+00 0.0e+00 0.0e+00  0  1  0  0  0   0  1  0  0  0  7777
VecAYPX                1 1.0 3.6551e-0425.0 1.44e+04 1.1 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0 15419
VecMAXPY               8 1.0 8.4854e-04 4.1 1.01e+06 1.1 0.0e+00 0.0e+00 0.0e+00  0 27  0  0  0   0 27  0  0  0 464928
VecScatterBegin        8 1.0 5.1821e-03 9.5 0.00e+00 0.0 2.2e+04 5.8e+02 1.0e+00  0  0 77 51  2   0  0 77 51  3     0
VecScatterEnd          8 1.0 5.0617e-0373.5 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecNormalize           8 1.0 9.0965e-02 1.0 3.46e+05 1.1 0.0e+00 0.0e+00 8.0e+00  0  9  0  0 14   0  9  0  0 21  1487
SFSetGraph             1 1.0 6.7560e-0620.5 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
SFSetUp                1 1.0 1.6051e-03 5.2 0.00e+00 0.0 4.5e+03 1.7e+02 1.0e+00  0  0 15  3  2   0  0 15  3  3     0
SFPack                 8 1.0 3.3494e-0423.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
SFUnpack               8 1.0 2.0648e-0516.8 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
KSPSetUp               1 1.0 1.8152e-0317.7 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
KSPSolve               1 1.0 1.7674e-01 1.0 3.51e+06 1.1 2.0e+04 5.7e+02 1.6e+01  1 94 69 45 28   1 94 69 45 41  7747
KSPGMRESOrthog         7 1.0 1.3209e-02 3.7 1.62e+06 1.1 0.0e+00 0.0e+00 7.0e+00  0 43  0  0 12   0 43  0  0 18 47787
PCSetUp                1 1.0 1.0049e-01 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 4.0e+00  1  0  0  0  7   1  0  0  0 10     0
PCApply                8 1.0 7.1780e-02 1.3 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     5              5      3661816     0.
              Vector    22             22      2030056     0.
           Index Set     3              3        64000     0.
   Star Forest Graph     3              3         3384     0.
       Krylov Solver     1              1        18856     0.
      Preconditioner     1              1         1496     0.
              Viewer     2              1          848     0.
    Distributed Mesh     1              1         5048     0.
     Discrete System     1              1          904     0.
           Weak Form     1              1          824     0.
========================================================================================================================
Average time to get PetscTime(): 3.12e-08
Average time for MPI_Barrier(): 0.000102571
Average time for zero size MPI_Send(): 1.27653e-06
#PETSc Option Table entries:
-ksp_converged_reason
-ksp_pc_side right
-ksp_rtol 1e-8
-ksp_view_final_residual
-log_view
-m 2373
-n 2373
-nw
-pc_type hypre
#End of PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: --prefix=/home/a/asarkar/sudhipv/software/FreeFem-sources/install/ff-petsc/r MAKEFLAGS= --with-debugging=0 COPTFLAGS="-O3 -mtune=native" CXXOPTFLAGS="-O3 -mtune=native" FOPTFLAGS="-O3 -mtune=native" --with-cxx-dialect=C++11 --with-ssl=0 --with-x=0 --with-fortran-bindings=0 --with-cudac=0 --with-cc=/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Compiler/intel2020/openmpi/4.0.3/bin/mpicc --with-cxx=/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Compiler/intel2020/openmpi/4.0.3/bin/mpic++ --with-fc=/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Compiler/intel2020/openmpi/4.0.3/bin/mpif90 --with-scalar-type=real --with-blaslapack-include=/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/imkl/2020.1.217/mkl/include --with-blaslapack-lib="-Wl,-rpath,/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/imkl/2020.1.217/mkl/lib/intel64 -L/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/imkl/2020.1.217/mkl/lib/intel64 -lmkl_rt -lmkl_intel_thread -lmkl_core  -liomp5 -lpthread" --download-metis --download-ptscotch --download-hypre --download-parmetis --download-mmg --download-parmmg --download-superlu --download-suitesparse --download-tetgen --download-slepc --download-hpddm --download-scalapack --download-mumps --download-slepc-configure-arguments=--download-arpack=https://github.com/prj-/arpack-ng/archive/b64dccb.tar.gz PETSC_ARCH=fr
-----------------------------------------
Libraries compiled on 2021-05-02 15:39:49 on nia-login03.scinet.local 
Machine characteristics: Linux-3.10.0-957.27.2.el7.x86_64-x86_64-with-centos-7.6.1810-Core
Using PETSc directory: /home/a/asarkar/sudhipv/software/FreeFem-sources/install/ff-petsc/r
Using PETSc arch: 
-----------------------------------------

Using C compiler: /cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Compiler/intel2020/openmpi/4.0.3/bin/mpicc  -fPIC -wd1572 -O3 -mtune=native   
Using Fortran compiler: /cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Compiler/intel2020/openmpi/4.0.3/bin/mpif90  -fPIC -O3 -mtune=native     
-----------------------------------------

Using include paths: -I/home/a/asarkar/sudhipv/software/FreeFem-sources/install/ff-petsc/r/include -I/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/imkl/2020.1.217/mkl/include
-----------------------------------------

Using C linker: /cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Compiler/intel2020/openmpi/4.0.3/bin/mpicc
Using Fortran linker: /cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Compiler/intel2020/openmpi/4.0.3/bin/mpif90
Using libraries: -Wl,-rpath,/home/a/asarkar/sudhipv/software/FreeFem-sources/install/ff-petsc/r/lib -L/home/a/asarkar/sudhipv/software/FreeFem-sources/install/ff-petsc/r/lib -lpetsc -Wl,-rpath,/home/a/asarkar/sudhipv/software/FreeFem-sources/install/ff-petsc/r/lib -L/home/a/asarkar/sudhipv/software/FreeFem-sources/install/ff-petsc/r/lib -Wl,-rpath,/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/imkl/2020.1.217/mkl/lib/intel64 -L/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/imkl/2020.1.217/mkl/lib/intel64 -Wl,-rpath,/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Compiler/intel2020/openmpi/4.0.3/lib -L/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Compiler/intel2020/openmpi/4.0.3/lib -Wl,-rpath,/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/libfabric/1.10.1/lib -L/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/libfabric/1.10.1/lib -Wl,-rpath,/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/ucx/1.8.0/lib -L/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/ucx/1.8.0/lib -Wl,-rpath,/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/imkl/2020.1.217/lib/intel64 -L/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/imkl/2020.1.217/lib/intel64 -Wl,-rpath,/cvmfs/restricted.computecanada.ca/easybuild/software/2020/Core/intel/2020.1.217/compilers_and_libraries_2020.1.217/linux/tbb/lib/intel64/gcc4.8 -L/cvmfs/restricted.computecanada.ca/easybuild/software/2020/Core/intel/2020.1.217/compilers_and_libraries_2020.1.217/linux/tbb/lib/intel64/gcc4.8 -Wl,-rpath,/cvmfs/restricted.computecanada.ca/easybuild/software/2020/Core/intel/2020.1.217/compilers_and_libraries_2020.1.217/linux/compiler/lib/intel64_lin -L/cvmfs/restricted.computecanada.ca/easybuild/software/2020/Core/intel/2020.1.217/compilers_and_libraries_2020.1.217/linux/compiler/lib/intel64_lin -Wl,-rpath,/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Compiler/intel2020/openmpi/4.0.3/lib64 -L/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Compiler/intel2020/openmpi/4.0.3/lib64 -Wl,-rpath,/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/gcccore/9.3.0/lib/gcc/x86_64-pc-linux-gnu/9.3.0 -L/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/gcccore/9.3.0/lib/gcc/x86_64-pc-linux-gnu/9.3.0 -Wl,-rpath,/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/gcccore/9.3.0/lib64 -L/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/gcccore/9.3.0/lib64 -Wl,-rpath,/cvmfs/soft.computecanada.ca/gentoo/2020/lib64 -L/cvmfs/soft.computecanada.ca/gentoo/2020/lib64 -Wl,-rpath,/cvmfs/soft.computecanada.ca/gentoo/2020/usr/lib64 -L/cvmfs/soft.computecanada.ca/gentoo/2020/usr/lib64 -Wl,-rpath,/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/gcccore/9.3.0/lib -L/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/gcccore/9.3.0/lib -Wl,-rpath,/cvmfs/soft.computecanada.ca/gentoo/2020/lib -L/cvmfs/soft.computecanada.ca/gentoo/2020/lib -Wl,-rpath,/cvmfs/soft.computecanada.ca/gentoo/2020/usr/lib -L/cvmfs/soft.computecanada.ca/gentoo/2020/usr/lib -lHYPRE -lcmumps -ldmumps -lsmumps -lzmumps -lmumps_common -lpord -lscalapack -lumfpack -lklu -lcholmod -lbtf -lccolamd -lcolamd -lcamd -lamd -lsuitesparseconfig -lsuperlu -lmkl_rt -lmkl_intel_thread -lmkl_core -liomp5 -lpthread -lparmmg -lmmg -lmmg3d -lptesmumps -lptscotchparmetis -lptscotch -lptscotcherr -lesmumps -lscotch -lscotcherr -lpthread -lparmetis -lmetis -ltet -lstdc++ -ldl -lmpi_usempif08 -lmpi_usempi_ignore_tkr -lmpi_mpifh -lmpi -lifport -lifcoremt_pic -limf -lsvml -lm -lipgo -lirc -lpthread -lgcc_s -lirc_s -lrt -lquadmath -lstdc++ -ldl
-----------------------------------------

#############################################################################################################
#############################################################################################################
########     Finsihed Code with NL 10 Processors             #################################################
#############################################################################################################
#############################################################################################################

scontrol show jobid 6206367
JobId=6206367 JobName=1L_hypre
   UserId=sudhipv(3061414) GroupId=asarkar(6001189) MCS_label=N/A
   Priority=2420647 Nice=0 Account=def-asarkar QOS=normal
   JobState=COMPLETING Reason=None Dependency=(null)
   Requeue=0 Restarts=0 BatchFlag=1 Reboot=0 ExitCode=0:0
   RunTime=00:09:01 TimeLimit=00:30:00 TimeMin=N/A
   SubmitTime=2021-10-07T06:30:56 EligibleTime=2021-10-07T06:30:56
   AccrueTime=2021-10-07T06:30:56
   StartTime=2021-10-07T06:30:59 EndTime=2021-10-07T06:40:00 Deadline=N/A
   SuspendTime=None SecsPreSuspend=0 LastSchedEval=2021-10-07T06:30:59
   Partition=compute AllocNode:Sid=nia-login05:283551
   ReqNodeList=(null) ExcNodeList=(null)
   NodeList=nia[0571,0598,0638,0725-0728,0730-0731]
   BatchHost=nia0525
   NumNodes=9 NumCPUs=800 NumTasks=400 CPUs/Task=1 ReqB:S:C:T=0:0:*:*
   TRES=cpu=800,mem=1750000M,node=10,billing=400
   Socks/Node=* NtasksPerN:B:S:C=40:0:*:* CoreSpec=*
   MinCPUsNode=40 MinMemoryNode=175000M MinTmpDiskNode=0
   Features=[skylake|cascade] DelayBoot=00:00:00
   OverSubscribe=NO Contiguous=0 Licenses=(null) Network=(null)
   Command=/gpfs/fs0/scratch/a/asarkar/sudhipv/poisson_hpc/weak/runff_niagara_weak.sh
   WorkDir=/gpfs/fs0/scratch/a/asarkar/sudhipv/poisson_hpc/weak
   StdErr=/gpfs/fs0/scratch/a/asarkar/sudhipv/poisson_hpc/weak/1L_hypre-6206367.out
   StdIn=/dev/null
   StdOut=/gpfs/fs0/scratch/a/asarkar/sudhipv/poisson_hpc/weak/1L_hypre-6206367.out
   Power=
   MailUser=sudhipv@cmail.carleton.ca MailType=BEGIN,END,FAIL,REQUEUE,STAGE_OUT

sacct -j 6206367
       JobID    JobName    Account    Elapsed  MaxVMSize     MaxRSS  SystemCPU    UserCPU ExitCode 
------------ ---------- ---------- ---------- ---------- ---------- ---------- ---------- -------- 
6206367        1L_hypre def-asark+   00:09:01                        13:17.985   03:14:25      0:0 
6206367.bat+      batch def-asark+   00:09:01  50435796K  30776028K  02:32.698  30:14.037      0:0 
6206367.ext+     extern def-asark+   00:09:01    138360K      1068K  00:00.003  00:00.005      0:0 
6206367.0         orted def-asark+   00:00:16    395276K      3000K  00:01.391  00:01.097      0:0 
6206367.1         orted def-asark+   00:00:08    395276K      2996K  00:27.526  02:28.919      0:0 
6206367.2         orted def-asark+   00:00:13    395276K      2996K  01:26.307  14:39.643      0:0 
6206367.3         orted def-asark+   00:00:15    395276K      2996K  02:39.290  36:42.932      0:0 
6206367.4         orted def-asark+   00:00:24    395276K      2996K  06:10.768   01:50:18      0:0 

kernel messages produced during job executions:
[Oct 7 05:58] INFO: task cvmfs2:114232 blocked for more than 120 seconds.
[  +0.008089] "echo 0 > /proc/sys/kernel/hung_task_timeout_secs" disables this message.
[  +0.009349] cvmfs2          D ffff8a625833c100     0 114232 114217 0x00000000
[  +0.008479] Call Trace:
[  +0.003210]  [<ffffffffbaed4a0f>] ? ttwu_do_activate+0x6f/0x80
[  +0.006989]  [<ffffffffbb56c9c9>] schedule+0x29/0x70
[  +0.006029]  [<ffffffffc0c9ec91>] cxiWaitEventWait+0x1d1/0x2f0 [mmfslinux]
[  +0.008159]  [<ffffffffbaed81c0>] ? wake_up_state+0x20/0x20
[  +0.006748]  [<ffffffffc0e21790>] _ZN6ThCond12internalWaitEP16KernelSynchStatejPv+0xd0/0x260 [mmfs26]
[  +0.010832]  [<ffffffffc0e22518>] ? _ZN14BaseMutexClass15releaseLockHeldEP16KernelSynchState+0x18/0x130 [mmfs26]
[  +0.011901]  [<ffffffffc0e22e5b>] _ZN6ThCond5kWaitEiPKc+0x1db/0x410 [mmfs26]
[  +0.008415]  [<ffffffffc0d41a15>] _ZN13KernelMailbox21sendToDaemonWithReplyEv+0x375/0x410 [mmfs26]
[  +0.010561]  [<ffffffffc0d6e3ee>] _Z12kShHashFetchP15KernelOperationP8CacheObjtsiPvij+0x1fe/0x230 [mmfs26]
[  +0.011339]  [<ffffffffc0d4e37d>] _Z11lockGetattrP15KernelOperationP13gpfsVfsData_tP10gpfsNode_tR7FileUIDiR13LookupDetailsR10WhatLockedPP8OpenFileP10cxiVattr_t+0xb9d/0x1490 [mmfs26]
[  +0.019099]  [<ffffffffc0d4d0b3>] ? _ZL14lookupFileFastP15KernelOperationP13gpfsVfsData_tPK10gpfsNode_tPvPKcjjP10ext_cred_tP7FileUIDPjP13LookupDetailsSD_PP8OpenFil+0x1043/0x1060 [mmfs26]
[  +0.019630]  [<ffffffffbb01eb00>] ? kmem_cache_alloc_node_trace+0x210/0x210
[  +0.008410]  [<ffffffffc0d502ec>] _ZN10gpfsNode_t6lookupEP15KernelOperationP13gpfsVfsData_tPS_PvPcjjP10ext_cred_tPS5_PS4_PjS9_PiS6_SC_Px+0x65c/0x1400 [mmfs26]
[  +0.017034]  [<ffffffffc0db1d7b>] _Z10gpfsLookupP13gpfsVfsData_tPvP9cxiNode_tS1_S1_PcjPS1_PS3_PyP10cxiVattr_tPjP10ext_cred_tjS5_PiS4_SD_+0x44b/0xad0 [mmfs26]
[  +0.017015]  [<ffffffffc0dd05a2>] ? _Z33gpfsIsCifsBypassTraversalCheckingv+0xe2/0x130 [mmfs26]
[  +0.010334]  [<ffffffffc0c9080e>] ? getCred+0x18e/0x350 [mmfslinux]
[  +0.007720]  [<ffffffffc0c962a6>] gpfs_i_lookup+0x2e6/0x5a0 [mmfslinux]
[  +0.008130]  [<ffffffffc0db1930>] ? _Z8gpfsLinkP13gpfsVfsData_tP9cxiNode_tS2_PvPcjjP10ext_cred_t+0x6e0/0x6e0 [mmfs26]
[  +0.012586]  [<ffffffffbb0202a5>] ? kmem_cache_alloc+0x35/0x1f0
[  +0.007364]  [<ffffffffbb05e025>] ? __d_alloc+0x25/0x180
[  +0.006677]  [<ffffffffbb05e122>] ? __d_alloc+0x122/0x180
[  +0.006783]  [<ffffffffbb05e1d8>] ? d_alloc+0x58/0x70
[  +0.006401]  [<ffffffffbb04e0d3>] lookup_real+0x23/0x60
[  +0.006590]  [<ffffffffbb04eaf2>] __lookup_hash+0x42/0x60
[  +0.006778]  [<ffffffffbb563864>] lookup_slow+0x42/0xa7
[  +0.006592]  [<ffffffffbb0533ff>] link_path_walk+0x80f/0x8b0
[  +0.007085]  [<ffffffffbaff91bc>] ? page_add_file_rmap+0x8c/0xc0
[  +0.007473]  [<ffffffffbb05360a>] path_lookupat+0x7a/0x8b0
[  +0.006898]  [<ffffffffbafeb3e4>] ? handle_pte_fault+0x2f4/0xd10
[  +0.007480]  [<ffffffffbb053e6b>] filename_lookup+0x2b/0xc0
[  +0.007000]  [<ffffffffbb053f42>] filename_create+0x42/0x180
[  +0.007099]  [<ffffffffbb054cbf>] ? getname_flags+0x4f/0x1a0
[  +0.007096]  [<ffffffffbb054d34>] ? getname_flags+0xc4/0x1a0
[  +0.007095]  [<ffffffffbb054fe1>] user_path_create+0x41/0x60
[  +0.007090]  [<ffffffffbb0563a8>] SyS_mkdirat+0x48/0x100
[  +0.006675]  [<ffffffffbb056479>] SyS_mkdir+0x19/0x20
[  +0.006366]  [<ffffffffbb579ddb>] system_call_fastpath+0x22/0x27
[Oct 7 06:00] INFO: task cvmfs2:114232 blocked for more than 120 seconds.
[  +0.008310] "echo 0 > /proc/sys/kernel/hung_task_timeout_secs" disables this message.
[  +0.009617] cvmfs2          D ffff8a625833c100     0 114232 114217 0x00000000
[  +0.008781] Call Trace:
[  +0.003492]  [<ffffffffbaed4a0f>] ? ttwu_do_activate+0x6f/0x80
[  +0.007263]  [<ffffffffbb56c9c9>] schedule+0x29/0x70
[  +0.006289]  [<ffffffffc0c9ec91>] cxiWaitEventWait+0x1d1/0x2f0 [mmfslinux]
[  +0.008416]  [<ffffffffbaed81c0>] ? wake_up_state+0x20/0x20
[  +0.006998]  [<ffffffffc0e21790>] _ZN6ThCond12internalWaitEP16KernelSynchStatejPv+0xd0/0x260 [mmfs26]
[  +0.011070]  [<ffffffffc0e22518>] ? _ZN14BaseMutexClass15releaseLockHeldEP16KernelSynchState+0x18/0x130 [mmfs26]
[  +0.012142]  [<ffffffffc0e22e5b>] _ZN6ThCond5kWaitEiPKc+0x1db/0x410 [mmfs26]
[  +0.008636]  [<ffffffffc0d41a15>] _ZN13KernelMailbox21sendToDaemonWithReplyEv+0x375/0x410 [mmfs26]
[  +0.010767]  [<ffffffffc0d6e3ee>] _Z12kShHashFetchP15KernelOperationP8CacheObjtsiPvij+0x1fe/0x230 [mmfs26]
[  +0.011540]  [<ffffffffc0d4e37d>] _Z11lockGetattrP15KernelOperationP13gpfsVfsData_tP10gpfsNode_tR7FileUIDiR13LookupDetailsR10WhatLockedPP8OpenFileP10cxiVattr_t+0xb9d/0x1490 [mmfs26]
[  +0.019465]  [<ffffffffc0d4d0b3>] ? _ZL14lookupFileFastP15KernelOperationP13gpfsVfsData_tPK10gpfsNode_tPvPKcjjP10ext_cred_tP7FileUIDPjP13LookupDetailsSD_PP8OpenFil+0x1043/0x1060 [mmfs26]
[  +0.019943]  [<ffffffffbb01eb00>] ? kmem_cache_alloc_node_trace+0x210/0x210
[  +0.008538]  [<ffffffffc0d502ec>] _ZN10gpfsNode_t6lookupEP15KernelOperationP13gpfsVfsData_tPS_PvPcjjP10ext_cred_tPS5_PS4_PjS9_PiS6_SC_Px+0x65c/0x1400 [mmfs26]
[  +0.017241]  [<ffffffffc0db1d7b>] _Z10gpfsLookupP13gpfsVfsData_tPvP9cxiNode_tS1_S1_PcjPS1_PS3_PyP10cxiVattr_tPjP10ext_cred_tjS5_PiS4_SD_+0x44b/0xad0 [mmfs26]
[  +0.017152]  [<ffffffffc0dd05a2>] ? _Z33gpfsIsCifsBypassTraversalCheckingv+0xe2/0x130 [mmfs26]
[  +0.010374]  [<ffffffffc0c9080e>] ? getCred+0x18e/0x350 [mmfslinux]
[  +0.007740]  [<ffffffffc0c962a6>] gpfs_i_lookup+0x2e6/0x5a0 [mmfslinux]
[  +0.008135]  [<ffffffffc0db1930>] ? _Z8gpfsLinkP13gpfsVfsData_tP9cxiNode_tS2_PvPcjjP10ext_cred_t+0x6e0/0x6e0 [mmfs26]
[  +0.012575]  [<ffffffffbb0202a5>] ? kmem_cache_alloc+0x35/0x1f0
[  +0.007341]  [<ffffffffbb05e025>] ? __d_alloc+0x25/0x180
[  +0.006654]  [<ffffffffbb05e122>] ? __d_alloc+0x122/0x180
[  +0.006729]  [<ffffffffbb05e1d8>] ? d_alloc+0x58/0x70
[  +0.006342]  [<ffffffffbb04e0d3>] lookup_real+0x23/0x60
[  +0.006537]  [<ffffffffbb04eaf2>] __lookup_hash+0x42/0x60
[  +0.006735]  [<ffffffffbb563864>] lookup_slow+0x42/0xa7
[  +0.006543]  [<ffffffffbb0533ff>] link_path_walk+0x80f/0x8b0
[  +0.007030]  [<ffffffffbaff91bc>] ? page_add_file_rmap+0x8c/0xc0
[  +0.007413]  [<ffffffffbb05360a>] path_lookupat+0x7a/0x8b0
[  +0.006833]  [<ffffffffbafeb3e4>] ? handle_pte_fault+0x2f4/0xd10
[  +0.007416]  [<ffffffffbb053e6b>] filename_lookup+0x2b/0xc0
[  +0.006930]  [<ffffffffbb053f42>] filename_create+0x42/0x180
[  +0.007021]  [<ffffffffbb054cbf>] ? getname_flags+0x4f/0x1a0
[  +0.007020]  [<ffffffffbb054d34>] ? getname_flags+0xc4/0x1a0
[  +0.007016]  [<ffffffffbb054fe1>] user_path_create+0x41/0x60
[  +0.007019]  [<ffffffffbb0563a8>] SyS_mkdirat+0x48/0x100
[  +0.006635]  [<ffffffffbb056479>] SyS_mkdir+0x19/0x20
[  +0.006346]  [<ffffffffbb579ddb>] system_call_fastpath+0x22/0x27
[Oct 7 06:02] INFO: task cvmfs2:114232 blocked for more than 120 seconds.
[  +0.008298] "echo 0 > /proc/sys/kernel/hung_task_timeout_secs" disables this message.
[  +0.009597] cvmfs2          D ffff8a625833c100     0 114232 114217 0x00000000
[  +0.008763] Call Trace:
[  +0.003482]  [<ffffffffbaed4a0f>] ? ttwu_do_activate+0x6f/0x80
[  +0.007262]  [<ffffffffbb56c9c9>] schedule+0x29/0x70
[  +0.006289]  [<ffffffffc0c9ec91>] cxiWaitEventWait+0x1d1/0x2f0 [mmfslinux]
[  +0.008413]  [<ffffffffbaed81c0>] ? wake_up_state+0x20/0x20
[  +0.006995]  [<ffffffffc0e21790>] _ZN6ThCond12internalWaitEP16KernelSynchStatejPv+0xd0/0x260 [mmfs26]
[  +0.011058]  [<ffffffffc0e22518>] ? _ZN14BaseMutexClass15releaseLockHeldEP16KernelSynchState+0x18/0x130 [mmfs26]
[  +0.012133]  [<ffffffffc0e22e5b>] _ZN6ThCond5kWaitEiPKc+0x1db/0x410 [mmfs26]
[  +0.008633]  [<ffffffffc0d41a15>] _ZN13KernelMailbox21sendToDaemonWithReplyEv+0x375/0x410 [mmfs26]
[  +0.010767]  [<ffffffffc0d6e3ee>] _Z12kShHashFetchP15KernelOperationP8CacheObjtsiPvij+0x1fe/0x230 [mmfs26]
[  +0.011540]  [<ffffffffc0d4e37d>] _Z11lockGetattrP15KernelOperationP13gpfsVfsData_tP10gpfsNode_tR7FileUIDiR13LookupDetailsR10WhatLockedPP8OpenFileP10cxiVattr_t+0xb9d/0x1490 [mmfs26]
[  +0.019469]  [<ffffffffc0d4d0b3>] ? _ZL14lookupFileFastP15KernelOperationP13gpfsVfsData_tPK10gpfsNode_tPvPKcjjP10ext_cred_tP7FileUIDPjP13LookupDetailsSD_PP8OpenFil+0x1043/0x1060 [mmfs26]
[  +0.019950]  [<ffffffffbb01eb00>] ? kmem_cache_alloc_node_trace+0x210/0x210
[  +0.008534]  [<ffffffffc0d502ec>] _ZN10gpfsNode_t6lookupEP15KernelOperationP13gpfsVfsData_tPS_PvPcjjP10ext_cred_tPS5_PS4_PjS9_PiS6_SC_Px+0x65c/0x1400 [mmfs26]
[  +0.017242]  [<ffffffffc0db1d7b>] _Z10gpfsLookupP13gpfsVfsData_tPvP9cxiNode_tS1_S1_PcjPS1_PS3_PyP10cxiVattr_tPjP10ext_cred_tjS5_PiS4_SD_+0x44b/0xad0 [mmfs26]
[  +0.017150]  [<ffffffffc0dd05a2>] ? _Z33gpfsIsCifsBypassTraversalCheckingv+0xe2/0x130 [mmfs26]
[  +0.010369]  [<ffffffffc0c9080e>] ? getCred+0x18e/0x350 [mmfslinux]
[  +0.007737]  [<ffffffffc0c962a6>] gpfs_i_lookup+0x2e6/0x5a0 [mmfslinux]
[  +0.008130]  [<ffffffffc0db1930>] ? _Z8gpfsLinkP13gpfsVfsData_tP9cxiNode_tS2_PvPcjjP10ext_cred_t+0x6e0/0x6e0 [mmfs26]
[  +0.012577]  [<ffffffffbb0202a5>] ? kmem_cache_alloc+0x35/0x1f0
[  +0.007344]  [<ffffffffbb05e025>] ? __d_alloc+0x25/0x180
[  +0.006652]  [<ffffffffbb05e122>] ? __d_alloc+0x122/0x180
[  +0.006734]  [<ffffffffbb05e1d8>] ? d_alloc+0x58/0x70
[  +0.006347]  [<ffffffffbb04e0d3>] lookup_real+0x23/0x60
[  +0.006541]  [<ffffffffbb04eaf2>] __lookup_hash+0x42/0x60
[  +0.006736]  [<ffffffffbb563864>] lookup_slow+0x42/0xa7
[  +0.006541]  [<ffffffffbb0533ff>] link_path_walk+0x80f/0x8b0
[  +0.007028]  [<ffffffffbaff91bc>] ? page_add_file_rmap+0x8c/0xc0
[  +0.007416]  [<ffffffffbb05360a>] path_lookupat+0x7a/0x8b0
[  +0.006833]  [<ffffffffbafeb3e4>] ? handle_pte_fault+0x2f4/0xd10
[  +0.007416]  [<ffffffffbb053e6b>] filename_lookup+0x2b/0xc0
[  +0.006931]  [<ffffffffbb053f42>] filename_create+0x42/0x180
[  +0.007025]  [<ffffffffbb054cbf>] ? getname_flags+0x4f/0x1a0
[  +0.007020]  [<ffffffffbb054d34>] ? getname_flags+0xc4/0x1a0
[  +0.007019]  [<ffffffffbb054fe1>] user_path_create+0x41/0x60
[  +0.007021]  [<ffffffffbb0563a8>] SyS_mkdirat+0x48/0x100
[  +0.006636]  [<ffffffffbb056479>] SyS_mkdir+0x19/0x20
[  +0.006346]  [<ffffffffbb579ddb>] system_call_fastpath+0x22/0x27
