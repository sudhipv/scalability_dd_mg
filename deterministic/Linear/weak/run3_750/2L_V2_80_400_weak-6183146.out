/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Compiler/intel2020/openmpi/4.0.3/bin/mpiexec
/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Compiler/intel2020/openmpi/4.0.3/bin/mpirun
Number of Vertices for coarse mesh in process  0  1127844
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 3327
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 7523
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 7479
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 6749
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 5899
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 5546
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 5606
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 7779
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 5797
Linear solve converged due to CONVERGED_RTOL iterations 9
time taken for solve is90.7688
L2 Norm of uerr is6.98165e-08
************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

/home/a/asarkar/sudhipv/software/FreeFem-sources/src/mpi/FreeFem++-mpi on a  named nia0093.scinet.local with 80 processors, by sudhipv Sun Oct  3 13:24:23 2021
Using Petsc Release Version 3.15.0, Mar 30, 2021 

                         Max       Max/Min     Avg       Total
Time (sec):           9.087e+01     1.000   9.087e+01
Objects:              1.820e+02     1.000   1.820e+02
Flop:                 7.680e+10     1.048   7.512e+10  6.009e+12
Flop/sec:             8.452e+08     1.048   8.267e+08  6.613e+10
MPI Messages:         9.221e+05     4.000   5.878e+05  4.703e+07
MPI Message Lengths:  5.162e+08     3.137   7.028e+02  3.305e+10
MPI Reductions:       1.134e+05     1.000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flop
                            and VecAXPY() for complex vectors of length N --> 8N flop

Summary of Stages:   ----- Time ------  ----- Flop ------  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total    Count   %Total     Avg         %Total    Count   %Total
 0:      Main Stage: 9.0868e+01 100.0%  6.0094e+12 100.0%  4.703e+07 100.0%  7.028e+02      100.0%  1.134e+05 100.0%

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flop: Max - maximum over all processors
                  Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   AvgLen: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flop in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flop over all processors)/(max time over all processors)
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flop                              --- Global ---  --- Stage ----  Total
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   AvgLen  Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s
------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

BuildTwoSided         10 1.0 1.5308e-0145.1 0.00e+00 0.0 1.6e+03 4.0e+00 6.0e+00  0  0  0  0  0   0  0  0  0  0     0
BuildTwoSidedF         2 1.0 8.3236e-04 3.2 0.00e+00 0.0 0.0e+00 0.0e+00 2.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatMult            57620 1.0 1.8849e+01 1.1 1.08e+10 1.0 2.4e+07 7.0e+02 2.0e+00 20 14 50 50  0  20 14 50 50  0 44889
MatMultAdd             9 1.0 1.7463e-02 1.3 1.03e+06 1.0 3.7e+03 2.3e+03 0.0e+00  0  0  0  0  0   0  0  0  0  0  4646
MatMultTranspose       9 1.0 2.0625e-02 1.7 0.00e+00 0.0 3.7e+03 7.7e+03 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatSolve           57612 1.0 1.8416e+01 1.1 1.15e+10 1.0 0.0e+00 0.0e+00 0.0e+00 19 15  0  0  0  19 15  0  0  0 48574
MatLUFactorSym         1 1.0 8.1821e-02 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatLUFactorNum         2 1.0 4.2968e-01 1.5 6.71e+08 1.4 0.0e+00 0.0e+00 0.0e+00  0  1  0  0  0   0  1  0  0  0 103417
MatILUFactorSym        1 1.0 7.2386e-04 1.7 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatResidual            9 1.0 1.7948e-02 1.7 7.18e+06 1.0 3.7e+03 1.5e+03 0.0e+00  0  0  0  0  0   0  0  0  0  0 31624
MatAssemblyBegin       6 1.0 8.9872e-04 1.6 0.00e+00 0.0 0.0e+00 0.0e+00 2.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatAssemblyEnd         6 1.0 5.0571e-02 5.8 0.00e+00 0.0 0.0e+00 0.0e+00 8.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatGetRowIJ            2 1.0 2.7837e-03 1.3 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatCreateSubMats       2 1.0 2.7295e-02 1.1 0.00e+00 0.0 4.1e+03 2.6e+03 2.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatGetOrdering         2 1.0 2.7929e-02 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatIncreaseOvrlp       2 1.0 9.3195e-03 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 2.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecMDot            55724 1.0 2.1955e+01 1.2 2.49e+10 1.0 0.0e+00 0.0e+00 5.6e+04 22 32  0  0 49  22 32  0  0 49 88685
VecNorm            57586 1.0 3.9228e+00 1.9 1.66e+09 1.0 0.0e+00 0.0e+00 5.8e+04  3  2  0  0 51   3  2  0  0 51 33150
VecScale           57604 1.0 2.7892e-01 1.4 8.30e+08 1.0 0.0e+00 0.0e+00 0.0e+00  0  1  0  0  0   0  1  0  0  0 233113
VecCopy             1889 1.0 7.3810e-02 1.3 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecSet            174737 1.0 3.9210e+00 1.3 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  4  0  0  0  0   4  0  0  0  0     0
VecAXPY             3722 1.0 7.7925e-02 1.4 1.08e+08 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0 108694
VecAYPX               54 1.0 7.5428e-03 1.4 4.10e+06 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0 43023
VecAXPBYCZ            18 1.0 4.0125e-03 1.2 5.13e+06 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0 101094
VecMAXPY           57586 1.0 1.5798e+01 1.1 2.65e+10 1.0 0.0e+00 0.0e+00 0.0e+00 16 34  0  0  0  16 34  0  0  0 131215
VecScatterBegin   288059 1.0 6.9721e+00 1.3 0.00e+00 0.0 4.7e+07 7.0e+02 4.0e+00  7  0100100  0   7  0100100  0     0
VecScatterEnd     288059 1.0 1.8124e+00 2.3 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0
VecNormalize       57576 1.0 4.1923e+00 1.8 2.49e+09 1.0 0.0e+00 0.0e+00 5.8e+04  4  3  0  0 51   4  3  0  0 51 46496
SFSetGraph             8 1.0 1.6593e-04 1.6 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
SFSetUp                8 1.0 1.5890e-0117.6 0.00e+00 0.0 3.3e+03 2.8e+02 4.0e+00  0  0  0  0  0   0  0  0  0  0     0
SFReduceBegin      57612 1.0 1.2069e+00 1.3 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0
SFReduceEnd        57612 1.0 3.1480e-02 2.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
SFPack            288059 1.0 2.8191e-01 1.8 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
SFUnpack          288059 1.0 1.5215e-01 4.6 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
KSPSetUp               4 1.0 9.8019e-01 1.0 7.94e+08 1.4 1.2e+04 1.7e+03 3.4e+01  1  1  0  0  0   1  1  0  0  0 55063
KSPSolve               1 1.0 8.4489e+01 1.0 7.61e+10 1.0 4.7e+07 7.0e+02 1.1e+05 93 99100100100  93 99100100100 70487
KSPGMRESOrthog     55724 1.0 3.6337e+01 1.1 4.97e+10 1.0 0.0e+00 0.0e+00 5.6e+04 38 65  0  0 49  38 65  0  0 49 107168
PCSetUp                2 1.0 9.9523e-01 1.0 7.94e+08 1.4 1.4e+04 1.7e+03 4.5e+01  1  1  0  0  0   1  1  0  0  0 54248
PCSetUpOnBlocks       28 1.0 5.3367e-01 1.4 6.71e+08 1.4 0.0e+00 0.0e+00 0.0e+00  1  1  0  0  0   1  1  0  0  0 83266
PCApply                9 1.0 8.4456e+01 1.0 7.61e+10 1.0 4.7e+07 7.0e+02 1.1e+05 93 99100100100  93 99100100100 70496
PCApplyOnBlocks    57612 1.0 1.9722e+01 1.1 1.15e+10 1.0 0.0e+00 0.0e+00 0.0e+00 21 15  0  0  0  21 15  0  0  0 45358
------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix    13             13     79302556     0.
              Vector   106            106     30466528     0.
           Index Set    24             24      2030300     0.
   IS L to G Mapping     2              2      2536408     0.
   Star Forest Graph    14             14        16272     0.
       Krylov Solver     6              6        73864     0.
      Preconditioner     6              6         6552     0.
    Distributed Mesh     3              3        15144     0.
     Discrete System     3              3         2712     0.
           Weak Form     3              3         2472     0.
              Viewer     2              1          848     0.
========================================================================================================================
Average time to get PetscTime(): 3.12e-08
Average time for MPI_Barrier(): 5.66106e-05
Average time for zero size MPI_Send(): 3.91886e-06
#PETSc Option Table entries:
-ksp_converged_reason
-ksp_pc_side right
-ksp_rtol 1e-8
-ksp_type fgmres
-log_view
-m 1061
-mg_coarse_ksp_converged_reason
-mg_coarse_ksp_type gmres
-mg_coarse_pc_type asm
-mg_levels_pc_type asm
-mg_levels_sub_pc_type lu
-n 1061
-nw
-pc_mg_type multiplicative
-pc_type mg
#End of PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: --prefix=/home/a/asarkar/sudhipv/software/FreeFem-sources/install/ff-petsc/r MAKEFLAGS= --with-debugging=0 COPTFLAGS="-O3 -mtune=native" CXXOPTFLAGS="-O3 -mtune=native" FOPTFLAGS="-O3 -mtune=native" --with-cxx-dialect=C++11 --with-ssl=0 --with-x=0 --with-fortran-bindings=0 --with-cudac=0 --with-cc=/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Compiler/intel2020/openmpi/4.0.3/bin/mpicc --with-cxx=/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Compiler/intel2020/openmpi/4.0.3/bin/mpic++ --with-fc=/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Compiler/intel2020/openmpi/4.0.3/bin/mpif90 --with-scalar-type=real --with-blaslapack-include=/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/imkl/2020.1.217/mkl/include --with-blaslapack-lib="-Wl,-rpath,/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/imkl/2020.1.217/mkl/lib/intel64 -L/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/imkl/2020.1.217/mkl/lib/intel64 -lmkl_rt -lmkl_intel_thread -lmkl_core  -liomp5 -lpthread" --download-metis --download-ptscotch --download-hypre --download-parmetis --download-mmg --download-parmmg --download-superlu --download-suitesparse --download-tetgen --download-slepc --download-hpddm --download-scalapack --download-mumps --download-slepc-configure-arguments=--download-arpack=https://github.com/prj-/arpack-ng/archive/b64dccb.tar.gz PETSC_ARCH=fr
-----------------------------------------
Libraries compiled on 2021-05-02 15:39:49 on nia-login03.scinet.local 
Machine characteristics: Linux-3.10.0-957.27.2.el7.x86_64-x86_64-with-centos-7.6.1810-Core
Using PETSc directory: /home/a/asarkar/sudhipv/software/FreeFem-sources/install/ff-petsc/r
Using PETSc arch: 
-----------------------------------------

Using C compiler: /cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Compiler/intel2020/openmpi/4.0.3/bin/mpicc  -fPIC -wd1572 -O3 -mtune=native   
Using Fortran compiler: /cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Compiler/intel2020/openmpi/4.0.3/bin/mpif90  -fPIC -O3 -mtune=native     
-----------------------------------------

Using include paths: -I/home/a/asarkar/sudhipv/software/FreeFem-sources/install/ff-petsc/r/include -I/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/imkl/2020.1.217/mkl/include
-----------------------------------------

Using C linker: /cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Compiler/intel2020/openmpi/4.0.3/bin/mpicc
Using Fortran linker: /cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Compiler/intel2020/openmpi/4.0.3/bin/mpif90
Using libraries: -Wl,-rpath,/home/a/asarkar/sudhipv/software/FreeFem-sources/install/ff-petsc/r/lib -L/home/a/asarkar/sudhipv/software/FreeFem-sources/install/ff-petsc/r/lib -lpetsc -Wl,-rpath,/home/a/asarkar/sudhipv/software/FreeFem-sources/install/ff-petsc/r/lib -L/home/a/asarkar/sudhipv/software/FreeFem-sources/install/ff-petsc/r/lib -Wl,-rpath,/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/imkl/2020.1.217/mkl/lib/intel64 -L/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/imkl/2020.1.217/mkl/lib/intel64 -Wl,-rpath,/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Compiler/intel2020/openmpi/4.0.3/lib -L/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Compiler/intel2020/openmpi/4.0.3/lib -Wl,-rpath,/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/libfabric/1.10.1/lib -L/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/libfabric/1.10.1/lib -Wl,-rpath,/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/ucx/1.8.0/lib -L/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/ucx/1.8.0/lib -Wl,-rpath,/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/imkl/2020.1.217/lib/intel64 -L/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/imkl/2020.1.217/lib/intel64 -Wl,-rpath,/cvmfs/restricted.computecanada.ca/easybuild/software/2020/Core/intel/2020.1.217/compilers_and_libraries_2020.1.217/linux/tbb/lib/intel64/gcc4.8 -L/cvmfs/restricted.computecanada.ca/easybuild/software/2020/Core/intel/2020.1.217/compilers_and_libraries_2020.1.217/linux/tbb/lib/intel64/gcc4.8 -Wl,-rpath,/cvmfs/restricted.computecanada.ca/easybuild/software/2020/Core/intel/2020.1.217/compilers_and_libraries_2020.1.217/linux/compiler/lib/intel64_lin -L/cvmfs/restricted.computecanada.ca/easybuild/software/2020/Core/intel/2020.1.217/compilers_and_libraries_2020.1.217/linux/compiler/lib/intel64_lin -Wl,-rpath,/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Compiler/intel2020/openmpi/4.0.3/lib64 -L/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Compiler/intel2020/openmpi/4.0.3/lib64 -Wl,-rpath,/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/gcccore/9.3.0/lib/gcc/x86_64-pc-linux-gnu/9.3.0 -L/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/gcccore/9.3.0/lib/gcc/x86_64-pc-linux-gnu/9.3.0 -Wl,-rpath,/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/gcccore/9.3.0/lib64 -L/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/gcccore/9.3.0/lib64 -Wl,-rpath,/cvmfs/soft.computecanada.ca/gentoo/2020/lib64 -L/cvmfs/soft.computecanada.ca/gentoo/2020/lib64 -Wl,-rpath,/cvmfs/soft.computecanada.ca/gentoo/2020/usr/lib64 -L/cvmfs/soft.computecanada.ca/gentoo/2020/usr/lib64 -Wl,-rpath,/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/gcccore/9.3.0/lib -L/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/gcccore/9.3.0/lib -Wl,-rpath,/cvmfs/soft.computecanada.ca/gentoo/2020/lib -L/cvmfs/soft.computecanada.ca/gentoo/2020/lib -Wl,-rpath,/cvmfs/soft.computecanada.ca/gentoo/2020/usr/lib -L/cvmfs/soft.computecanada.ca/gentoo/2020/usr/lib -lHYPRE -lcmumps -ldmumps -lsmumps -lzmumps -lmumps_common -lpord -lscalapack -lumfpack -lklu -lcholmod -lbtf -lccolamd -lcolamd -lcamd -lamd -lsuitesparseconfig -lsuperlu -lmkl_rt -lmkl_intel_thread -lmkl_core -liomp5 -lpthread -lparmmg -lmmg -lmmg3d -lptesmumps -lptscotchparmetis -lptscotch -lptscotcherr -lesmumps -lscotch -lscotcherr -lpthread -lparmetis -lmetis -ltet -lstdc++ -ldl -lmpi_usempif08 -lmpi_usempi_ignore_tkr -lmpi_mpifh -lmpi -lifport -lifcoremt_pic -limf -lsvml -lm -lipgo -lirc -lpthread -lgcc_s -lirc_s -lrt -lquadmath -lstdc++ -ldl
-----------------------------------------

#############################################################################################################
#############################################################################################################
########     Finsihed Code with NL 10 Processors             #################################################
#############################################################################################################
#############################################################################################################
Number of Vertices for coarse mesh in process  0  2253001
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 5518
    Linear mg_coarse_ solve did not converge due to DIVERGED_ITS iterations 10000
    Linear mg_coarse_ solve did not converge due to DIVERGED_ITS iterations 10000
    Linear mg_coarse_ solve did not converge due to DIVERGED_ITS iterations 10000
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 8969
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 9802
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 9208
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 9269
    Linear mg_coarse_ solve did not converge due to DIVERGED_ITS iterations 10000
Linear solve converged due to CONVERGED_RTOL iterations 9
time taken for solve is138.539
L2 Norm of uerr is9.14998e-07
************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

/home/a/asarkar/sudhipv/software/FreeFem-sources/src/mpi/FreeFem++-mpi on a  named nia0093.scinet.local with 160 processors, by sudhipv Sun Oct  3 13:26:48 2021
Using Petsc Release Version 3.15.0, Mar 30, 2021 

                         Max       Max/Min     Avg       Total
Time (sec):           1.386e+02     1.000   1.386e+02
Objects:              1.820e+02     1.000   1.820e+02
Flop:                 1.135e+11     1.047   1.110e+11  1.776e+13
Flop/sec:             8.188e+08     1.047   8.009e+08  1.281e+11
MPI Messages:         1.370e+06     4.000   9.094e+05  1.455e+08
MPI Message Lengths:  8.649e+08     2.933   6.986e+02  1.016e+11
MPI Reductions:       1.684e+05     1.000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flop
                            and VecAXPY() for complex vectors of length N --> 8N flop

Summary of Stages:   ----- Time ------  ----- Flop ------  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total    Count   %Total     Avg         %Total    Count   %Total
 0:      Main Stage: 1.3861e+02 100.0%  1.7763e+13 100.0%  1.455e+08 100.0%  6.986e+02      100.0%  1.684e+05 100.0%

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flop: Max - maximum over all processors
                  Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   AvgLen: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flop in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flop over all processors)/(max time over all processors)
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flop                              --- Global ---  --- Stage ----  Total
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   AvgLen  Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s
------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

BuildTwoSided         10 1.0 2.7945e-0114.8 0.00e+00 0.0 3.4e+03 4.0e+00 6.0e+00  0  0  0  0  0   0  0  0  0  0     0
BuildTwoSidedF         2 1.0 4.4125e-03 6.4 0.00e+00 0.0 0.0e+00 0.0e+00 2.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatMult            85583 1.0 2.8749e+01 1.2 1.60e+10 1.0 7.3e+07 7.0e+02 2.0e+00 19 14 50 50  0  19 14 50 50  0 87277
MatMultAdd             9 1.0 1.9814e-02 1.5 1.02e+06 1.0 7.6e+03 2.3e+03 0.0e+00  0  0  0  0  0   0  0  0  0  0  8182
MatMultTranspose       9 1.0 1.3720e-0112.1 0.00e+00 0.0 7.6e+03 7.7e+03 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatSolve           85575 1.0 2.7676e+01 1.1 1.68e+10 1.0 0.0e+00 0.0e+00 0.0e+00 19 15  0  0  0  19 15  0  0  0 94888
MatLUFactorSym         1 1.0 8.5122e-02 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatLUFactorNum         2 1.0 4.4659e-01 1.6 6.71e+08 1.4 0.0e+00 0.0e+00 0.0e+00  0  1  0  0  0   0  1  0  0  0 201045
MatILUFactorSym        1 1.0 1.2825e-03 3.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatResidual            9 1.0 1.7501e-02 2.1 7.17e+06 1.0 7.6e+03 1.5e+03 0.0e+00  0  0  0  0  0   0  0  0  0  0 64815
MatAssemblyBegin       6 1.0 4.5040e-03 1.8 0.00e+00 0.0 0.0e+00 0.0e+00 2.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatAssemblyEnd         6 1.0 1.4837e-01 3.2 0.00e+00 0.0 0.0e+00 0.0e+00 8.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatGetRowIJ            2 1.0 2.9399e-03 1.4 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatCreateSubMats       2 1.0 2.9407e-02 1.2 0.00e+00 0.0 8.5e+03 2.6e+03 2.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatGetOrdering         2 1.0 3.0003e-02 1.3 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatIncreaseOvrlp       2 1.0 1.1815e-02 1.3 0.00e+00 0.0 0.0e+00 0.0e+00 2.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecMDot            82785 1.0 3.4925e+01 1.2 3.69e+10 1.0 0.0e+00 0.0e+00 8.3e+04 23 33  0  0 49  23 33  0  0 49 165485
VecNorm            85549 1.0 6.5013e+00 1.7 2.46e+09 1.0 0.0e+00 0.0e+00 8.6e+04  4  2  0  0 51   4  2  0  0 51 59337
VecScale           85567 1.0 4.5153e-01 1.6 1.23e+09 1.0 0.0e+00 0.0e+00 0.0e+00  0  1  0  0  0   0  1  0  0  0 427176
VecCopy             2791 1.0 1.1539e-01 1.3 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecSet            259528 1.0 5.7990e+00 1.3 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  4  0  0  0  0   4  0  0  0  0     0
VecAXPY             5526 1.0 1.4266e-01 1.7 1.60e+08 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0 175580
VecAYPX               54 1.0 7.3605e-03 1.3 4.10e+06 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0 88096
VecAXPBYCZ            18 1.0 3.9761e-03 1.2 5.12e+06 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0 203853
VecMAXPY           85549 1.0 2.3536e+01 1.1 3.93e+10 1.0 0.0e+00 0.0e+00 0.0e+00 16 35  0  0  0  16 35  0  0  0 261435
VecScatterBegin   427874 1.0 1.0772e+01 1.3 0.00e+00 0.0 1.5e+08 7.0e+02 4.0e+00  7  0100100  0   7  0100100  0     0
VecScatterEnd     427874 1.0 4.2052e+00 3.8 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  2  0  0  0  0   2  0  0  0  0     0
VecNormalize       85539 1.0 6.8978e+00 1.6 3.69e+09 1.0 0.0e+00 0.0e+00 8.6e+04  4  3  0  0 51   4  3  0  0 51 83850
SFSetGraph             8 1.0 1.7248e-04 1.7 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
SFSetUp                8 1.0 2.8310e-0111.6 0.00e+00 0.0 6.8e+03 2.8e+02 4.0e+00  0  0  0  0  0   0  0  0  0  0     0
SFReduceBegin      85575 1.0 1.7905e+00 1.4 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0
SFReduceEnd        85575 1.0 5.0438e-02 2.4 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
SFPack            427874 1.0 4.3670e-01 1.7 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
SFUnpack          427874 1.0 2.5171e-01 5.9 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
KSPSetUp               4 1.0 1.3516e+00 1.0 7.94e+08 1.4 2.6e+04 1.7e+03 3.4e+01  1  1  0  0  0   1  1  0  0  0 80569
KSPSolve               1 1.0 1.2821e+02 1.0 1.13e+11 1.0 1.5e+08 7.0e+02 1.7e+05 92 99100100100  92 99100100100 137698
KSPGMRESOrthog     82785 1.0 5.5467e+01 1.1 7.38e+10 1.0 0.0e+00 0.0e+00 8.3e+04 38 65  0  0 49  38 65  0  0 49 208405
PCSetUp                2 1.0 1.3689e+00 1.0 7.94e+08 1.4 3.0e+04 1.7e+03 4.5e+01  1  1  0  0  0   1  1  0  0  0 79576
PCSetUpOnBlocks       28 1.0 5.5719e-01 1.4 6.71e+08 1.4 0.0e+00 0.0e+00 0.0e+00  0  1  0  0  0   0  1  0  0  0 161138
PCApply                9 1.0 1.2817e+02 1.0 1.13e+11 1.0 1.5e+08 7.0e+02 1.7e+05 92 99100100100  92 99100100100 137711
PCApplyOnBlocks    85575 1.0 2.9605e+01 1.1 1.68e+10 1.0 0.0e+00 0.0e+00 0.0e+00 20 15  0  0  0  20 15  0  0  0 88705
------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix    13             13     80235072     0.
              Vector   106            106     30362456     0.
           Index Set    24             24      2027260     0.
   IS L to G Mapping     2              2      2931844     0.
   Star Forest Graph    14             14        16272     0.
       Krylov Solver     6              6        73864     0.
      Preconditioner     6              6         6552     0.
    Distributed Mesh     3              3        15144     0.
     Discrete System     3              3         2712     0.
           Weak Form     3              3         2472     0.
              Viewer     2              1          848     0.
========================================================================================================================
Average time to get PetscTime(): 3.21e-08
Average time for MPI_Barrier(): 7.22944e-05
Average time for zero size MPI_Send(): 2.68309e-06
#PETSc Option Table entries:
-ksp_converged_reason
-ksp_pc_side right
-ksp_rtol 1e-8
-ksp_type fgmres
-log_view
-m 1500
-mg_coarse_ksp_converged_reason
-mg_coarse_ksp_type gmres
-mg_coarse_pc_type asm
-mg_levels_pc_type asm
-mg_levels_sub_pc_type lu
-n 1500
-nw
-pc_mg_type multiplicative
-pc_type mg
#End of PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: --prefix=/home/a/asarkar/sudhipv/software/FreeFem-sources/install/ff-petsc/r MAKEFLAGS= --with-debugging=0 COPTFLAGS="-O3 -mtune=native" CXXOPTFLAGS="-O3 -mtune=native" FOPTFLAGS="-O3 -mtune=native" --with-cxx-dialect=C++11 --with-ssl=0 --with-x=0 --with-fortran-bindings=0 --with-cudac=0 --with-cc=/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Compiler/intel2020/openmpi/4.0.3/bin/mpicc --with-cxx=/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Compiler/intel2020/openmpi/4.0.3/bin/mpic++ --with-fc=/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Compiler/intel2020/openmpi/4.0.3/bin/mpif90 --with-scalar-type=real --with-blaslapack-include=/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/imkl/2020.1.217/mkl/include --with-blaslapack-lib="-Wl,-rpath,/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/imkl/2020.1.217/mkl/lib/intel64 -L/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/imkl/2020.1.217/mkl/lib/intel64 -lmkl_rt -lmkl_intel_thread -lmkl_core  -liomp5 -lpthread" --download-metis --download-ptscotch --download-hypre --download-parmetis --download-mmg --download-parmmg --download-superlu --download-suitesparse --download-tetgen --download-slepc --download-hpddm --download-scalapack --download-mumps --download-slepc-configure-arguments=--download-arpack=https://github.com/prj-/arpack-ng/archive/b64dccb.tar.gz PETSC_ARCH=fr
-----------------------------------------
Libraries compiled on 2021-05-02 15:39:49 on nia-login03.scinet.local 
Machine characteristics: Linux-3.10.0-957.27.2.el7.x86_64-x86_64-with-centos-7.6.1810-Core
Using PETSc directory: /home/a/asarkar/sudhipv/software/FreeFem-sources/install/ff-petsc/r
Using PETSc arch: 
-----------------------------------------

Using C compiler: /cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Compiler/intel2020/openmpi/4.0.3/bin/mpicc  -fPIC -wd1572 -O3 -mtune=native   
Using Fortran compiler: /cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Compiler/intel2020/openmpi/4.0.3/bin/mpif90  -fPIC -O3 -mtune=native     
-----------------------------------------

Using include paths: -I/home/a/asarkar/sudhipv/software/FreeFem-sources/install/ff-petsc/r/include -I/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/imkl/2020.1.217/mkl/include
-----------------------------------------

Using C linker: /cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Compiler/intel2020/openmpi/4.0.3/bin/mpicc
Using Fortran linker: /cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Compiler/intel2020/openmpi/4.0.3/bin/mpif90
Using libraries: -Wl,-rpath,/home/a/asarkar/sudhipv/software/FreeFem-sources/install/ff-petsc/r/lib -L/home/a/asarkar/sudhipv/software/FreeFem-sources/install/ff-petsc/r/lib -lpetsc -Wl,-rpath,/home/a/asarkar/sudhipv/software/FreeFem-sources/install/ff-petsc/r/lib -L/home/a/asarkar/sudhipv/software/FreeFem-sources/install/ff-petsc/r/lib -Wl,-rpath,/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/imkl/2020.1.217/mkl/lib/intel64 -L/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/imkl/2020.1.217/mkl/lib/intel64 -Wl,-rpath,/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Compiler/intel2020/openmpi/4.0.3/lib -L/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Compiler/intel2020/openmpi/4.0.3/lib -Wl,-rpath,/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/libfabric/1.10.1/lib -L/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/libfabric/1.10.1/lib -Wl,-rpath,/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/ucx/1.8.0/lib -L/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/ucx/1.8.0/lib -Wl,-rpath,/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/imkl/2020.1.217/lib/intel64 -L/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/imkl/2020.1.217/lib/intel64 -Wl,-rpath,/cvmfs/restricted.computecanada.ca/easybuild/software/2020/Core/intel/2020.1.217/compilers_and_libraries_2020.1.217/linux/tbb/lib/intel64/gcc4.8 -L/cvmfs/restricted.computecanada.ca/easybuild/software/2020/Core/intel/2020.1.217/compilers_and_libraries_2020.1.217/linux/tbb/lib/intel64/gcc4.8 -Wl,-rpath,/cvmfs/restricted.computecanada.ca/easybuild/software/2020/Core/intel/2020.1.217/compilers_and_libraries_2020.1.217/linux/compiler/lib/intel64_lin -L/cvmfs/restricted.computecanada.ca/easybuild/software/2020/Core/intel/2020.1.217/compilers_and_libraries_2020.1.217/linux/compiler/lib/intel64_lin -Wl,-rpath,/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Compiler/intel2020/openmpi/4.0.3/lib64 -L/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Compiler/intel2020/openmpi/4.0.3/lib64 -Wl,-rpath,/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/gcccore/9.3.0/lib/gcc/x86_64-pc-linux-gnu/9.3.0 -L/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/gcccore/9.3.0/lib/gcc/x86_64-pc-linux-gnu/9.3.0 -Wl,-rpath,/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/gcccore/9.3.0/lib64 -L/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/gcccore/9.3.0/lib64 -Wl,-rpath,/cvmfs/soft.computecanada.ca/gentoo/2020/lib64 -L/cvmfs/soft.computecanada.ca/gentoo/2020/lib64 -Wl,-rpath,/cvmfs/soft.computecanada.ca/gentoo/2020/usr/lib64 -L/cvmfs/soft.computecanada.ca/gentoo/2020/usr/lib64 -Wl,-rpath,/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/gcccore/9.3.0/lib -L/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/gcccore/9.3.0/lib -Wl,-rpath,/cvmfs/soft.computecanada.ca/gentoo/2020/lib -L/cvmfs/soft.computecanada.ca/gentoo/2020/lib -Wl,-rpath,/cvmfs/soft.computecanada.ca/gentoo/2020/usr/lib -L/cvmfs/soft.computecanada.ca/gentoo/2020/usr/lib -lHYPRE -lcmumps -ldmumps -lsmumps -lzmumps -lmumps_common -lpord -lscalapack -lumfpack -lklu -lcholmod -lbtf -lccolamd -lcolamd -lcamd -lamd -lsuitesparseconfig -lsuperlu -lmkl_rt -lmkl_intel_thread -lmkl_core -liomp5 -lpthread -lparmmg -lmmg -lmmg3d -lptesmumps -lptscotchparmetis -lptscotch -lptscotcherr -lesmumps -lscotch -lscotcherr -lpthread -lparmetis -lmetis -ltet -lstdc++ -ldl -lmpi_usempif08 -lmpi_usempi_ignore_tkr -lmpi_mpifh -lmpi -lifport -lifcoremt_pic -limf -lsvml -lm -lipgo -lirc -lpthread -lgcc_s -lirc_s -lrt -lquadmath -lstdc++ -ldl
-----------------------------------------

#############################################################################################################
#############################################################################################################
########     Finsihed Code with NL 10 Processors             #################################################
#############################################################################################################
#############################################################################################################
Number of Vertices for coarse mesh in process  0  3381921
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 6970
    Linear mg_coarse_ solve did not converge due to DIVERGED_ITS iterations 10000
    Linear mg_coarse_ solve did not converge due to DIVERGED_ITS iterations 10000
    Linear mg_coarse_ solve did not converge due to DIVERGED_ITS iterations 10000
    Linear mg_coarse_ solve did not converge due to DIVERGED_ITS iterations 10000
    Linear mg_coarse_ solve did not converge due to DIVERGED_ITS iterations 10000
    Linear mg_coarse_ solve did not converge due to DIVERGED_ITS iterations 10000
    Linear mg_coarse_ solve did not converge due to DIVERGED_ITS iterations 10000
    Linear mg_coarse_ solve did not converge due to DIVERGED_ITS iterations 10000
Linear solve converged due to CONVERGED_RTOL iterations 9
time taken for solve is151.227
L2 Norm of uerr is2.41703e-06
************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

/home/a/asarkar/sudhipv/software/FreeFem-sources/src/mpi/FreeFem++-mpi on a  named nia0093.scinet.local with 240 processors, by sudhipv Sun Oct  3 13:29:25 2021
Using Petsc Release Version 3.15.0, Mar 30, 2021 

                         Max       Max/Min     Avg       Total
Time (sec):           1.513e+02     1.000   1.513e+02
Objects:              1.820e+02     1.000   1.820e+02
Flop:                 1.194e+11     1.048   1.167e+11  2.800e+13
Flop/sec:             7.892e+08     1.048   7.711e+08  1.851e+11
MPI Messages:         1.439e+06     4.000   9.849e+05  2.364e+08
MPI Message Lengths:  8.943e+08     3.103   6.852e+02  1.620e+11
MPI Reductions:       1.770e+05     1.000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flop
                            and VecAXPY() for complex vectors of length N --> 8N flop

Summary of Stages:   ----- Time ------  ----- Flop ------  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total    Count   %Total     Avg         %Total    Count   %Total
 0:      Main Stage: 1.5132e+02 100.0%  2.8003e+13 100.0%  2.364e+08 100.0%  6.852e+02      100.0%  1.769e+05 100.0%

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flop: Max - maximum over all processors
                  Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   AvgLen: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flop in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flop over all processors)/(max time over all processors)
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flop                              --- Global ---  --- Stage ----  Total
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   AvgLen  Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s
------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

BuildTwoSided         10 1.0 2.0698e-01 8.7 0.00e+00 0.0 5.3e+03 4.0e+00 6.0e+00  0  0  0  0  0   0  0  0  0  0     0
BuildTwoSidedF         2 1.0 3.2122e-03 3.8 0.00e+00 0.0 0.0e+00 0.0e+00 2.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatMult            89930 1.0 3.0653e+01 1.2 1.69e+10 1.0 1.2e+08 6.8e+02 2.0e+00 19 14 50 50  0  19 14 50 50  0 129121
MatMultAdd             9 1.0 1.7616e-02 1.3 1.03e+06 1.0 1.2e+04 2.2e+03 0.0e+00  0  0  0  0  0   0  0  0  0  0 13815
MatMultTranspose       9 1.0 2.8208e-02 2.4 0.00e+00 0.0 1.2e+04 7.5e+03 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatSolve           89922 1.0 2.9911e+01 1.1 1.77e+10 1.0 0.0e+00 0.0e+00 0.0e+00 18 15  0  0  0  18 15  0  0  0 138371
MatLUFactorSym         1 1.0 1.0428e-01 1.5 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatLUFactorNum         2 1.0 4.4701e-01 1.7 7.11e+08 1.6 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0 301341
MatILUFactorSym        1 1.0 1.0305e-03 2.4 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatResidual            9 1.0 1.9639e-02 2.3 7.18e+06 1.0 1.2e+04 1.5e+03 0.0e+00  0  0  0  0  0   0  0  0  0  0 86719
MatAssemblyBegin       6 1.0 3.6602e-03 1.3 0.00e+00 0.0 0.0e+00 0.0e+00 2.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatAssemblyEnd         6 1.0 1.4540e-01 5.7 0.00e+00 0.0 0.0e+00 0.0e+00 8.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatGetRowIJ            2 1.0 4.5719e-03 2.3 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatCreateSubMats       2 1.0 3.7504e-02 1.4 0.00e+00 0.0 1.3e+04 2.5e+03 2.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatGetOrdering         2 1.0 4.1541e-02 1.7 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatIncreaseOvrlp       2 1.0 1.3901e-02 1.5 0.00e+00 0.0 0.0e+00 0.0e+00 2.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecMDot            86989 1.0 3.8620e+01 1.3 3.88e+10 1.0 0.0e+00 0.0e+00 8.7e+04 24 33  0  0 49  24 33  0  0 49 235998
VecNorm            89896 1.0 8.0236e+00 1.5 2.59e+09 1.0 0.0e+00 0.0e+00 9.0e+04  4  2  0  0 51   4  2  0  0 51 75834
VecScale           89914 1.0 4.6402e-01 1.6 1.30e+09 1.0 0.0e+00 0.0e+00 0.0e+00  0  1  0  0  0   0  1  0  0  0 655655
VecCopy             2934 1.0 1.2240e-01 1.3 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecSet            272712 1.0 6.0500e+00 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  4  0  0  0  0   4  0  0  0  0     0
VecAXPY             5812 1.0 1.1846e-01 1.4 1.68e+08 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0 333733
VecAYPX               54 1.0 7.5773e-03 1.4 4.10e+06 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0 128472
VecAXPBYCZ            18 1.0 4.1172e-03 1.3 5.13e+06 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0 295545
VecMAXPY           89896 1.0 2.4318e+01 1.1 4.14e+10 1.0 0.0e+00 0.0e+00 0.0e+00 15 35  0  0  0  15 35  0  0  0 399018
VecScatterBegin   449609 1.0 1.1866e+01 1.4 0.00e+00 0.0 2.4e+08 6.8e+02 4.0e+00  6  0100100  0   6  0100100  0     0
VecScatterEnd     449609 1.0 3.7149e+00 2.7 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0
VecNormalize       89886 1.0 8.4226e+00 1.4 3.89e+09 1.0 0.0e+00 0.0e+00 9.0e+04  5  3  0  0 51   5  3  0  0 51 108315
SFSetGraph             8 1.0 2.1107e-04 2.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
SFSetUp                8 1.0 2.1362e-01 7.8 0.00e+00 0.0 1.1e+04 2.8e+02 4.0e+00  0  0  0  0  0   0  0  0  0  0     0
SFReduceBegin      89922 1.0 2.1202e+00 1.5 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0
SFReduceEnd        89922 1.0 5.0811e-02 2.4 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
SFPack            449609 1.0 4.4577e-01 1.8 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
SFUnpack          449609 1.0 2.4842e-01 5.3 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
KSPSetUp               4 1.0 9.9377e-01 1.0 8.38e+08 1.5 3.9e+04 1.7e+03 3.4e+01  1  1  0  0  0   1  1  0  0  0 164437
KSPSolve               1 1.0 1.3787e+02 1.0 1.19e+11 1.0 2.4e+08 6.8e+02 1.8e+05 91 99100100100  91 99100100100 201926
KSPGMRESOrthog     86989 1.0 6.0571e+01 1.1 7.77e+10 1.0 0.0e+00 0.0e+00 8.7e+04 38 65  0  0 49  38 65  0  0 49 300946
PCSetUp                2 1.0 1.0119e+00 1.0 8.38e+08 1.5 4.6e+04 1.7e+03 4.5e+01  1  1  0  0  0   1  1  0  0  0 161542
PCSetUpOnBlocks       28 1.0 5.6180e-01 1.6 7.11e+08 1.6 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0 239772
PCApply                9 1.0 1.3784e+02 1.0 1.19e+11 1.0 2.4e+08 6.8e+02 1.8e+05 91 99100100100  91 99100100100 201942
PCApplyOnBlocks    89922 1.0 3.2112e+01 1.1 1.77e+10 1.0 0.0e+00 0.0e+00 0.0e+00 20 15  0  0  0  20 15  0  0  0 128886
------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix    13             13     80790832     0.
              Vector   106            106     30390912     0.
           Index Set    24             24      2027680     0.
   IS L to G Mapping     2              2       853744     0.
   Star Forest Graph    14             14        16272     0.
       Krylov Solver     6              6        73864     0.
      Preconditioner     6              6         6552     0.
    Distributed Mesh     3              3        15144     0.
     Discrete System     3              3         2712     0.
           Weak Form     3              3         2472     0.
              Viewer     2              1          848     0.
========================================================================================================================
Average time to get PetscTime(): 3.43e-08
Average time for MPI_Barrier(): 6.9534e-05
Average time for zero size MPI_Send(): 1.37734e-06
#PETSc Option Table entries:
-ksp_converged_reason
-ksp_pc_side right
-ksp_rtol 1e-8
-ksp_type fgmres
-log_view
-m 1838
-mg_coarse_ksp_converged_reason
-mg_coarse_ksp_type gmres
-mg_coarse_pc_type asm
-mg_levels_pc_type asm
-mg_levels_sub_pc_type lu
-n 1838
-nw
-pc_mg_type multiplicative
-pc_type mg
#End of PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: --prefix=/home/a/asarkar/sudhipv/software/FreeFem-sources/install/ff-petsc/r MAKEFLAGS= --with-debugging=0 COPTFLAGS="-O3 -mtune=native" CXXOPTFLAGS="-O3 -mtune=native" FOPTFLAGS="-O3 -mtune=native" --with-cxx-dialect=C++11 --with-ssl=0 --with-x=0 --with-fortran-bindings=0 --with-cudac=0 --with-cc=/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Compiler/intel2020/openmpi/4.0.3/bin/mpicc --with-cxx=/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Compiler/intel2020/openmpi/4.0.3/bin/mpic++ --with-fc=/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Compiler/intel2020/openmpi/4.0.3/bin/mpif90 --with-scalar-type=real --with-blaslapack-include=/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/imkl/2020.1.217/mkl/include --with-blaslapack-lib="-Wl,-rpath,/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/imkl/2020.1.217/mkl/lib/intel64 -L/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/imkl/2020.1.217/mkl/lib/intel64 -lmkl_rt -lmkl_intel_thread -lmkl_core  -liomp5 -lpthread" --download-metis --download-ptscotch --download-hypre --download-parmetis --download-mmg --download-parmmg --download-superlu --download-suitesparse --download-tetgen --download-slepc --download-hpddm --download-scalapack --download-mumps --download-slepc-configure-arguments=--download-arpack=https://github.com/prj-/arpack-ng/archive/b64dccb.tar.gz PETSC_ARCH=fr
-----------------------------------------
Libraries compiled on 2021-05-02 15:39:49 on nia-login03.scinet.local 
Machine characteristics: Linux-3.10.0-957.27.2.el7.x86_64-x86_64-with-centos-7.6.1810-Core
Using PETSc directory: /home/a/asarkar/sudhipv/software/FreeFem-sources/install/ff-petsc/r
Using PETSc arch: 
-----------------------------------------

Using C compiler: /cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Compiler/intel2020/openmpi/4.0.3/bin/mpicc  -fPIC -wd1572 -O3 -mtune=native   
Using Fortran compiler: /cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Compiler/intel2020/openmpi/4.0.3/bin/mpif90  -fPIC -O3 -mtune=native     
-----------------------------------------

Using include paths: -I/home/a/asarkar/sudhipv/software/FreeFem-sources/install/ff-petsc/r/include -I/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/imkl/2020.1.217/mkl/include
-----------------------------------------

Using C linker: /cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Compiler/intel2020/openmpi/4.0.3/bin/mpicc
Using Fortran linker: /cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Compiler/intel2020/openmpi/4.0.3/bin/mpif90
Using libraries: -Wl,-rpath,/home/a/asarkar/sudhipv/software/FreeFem-sources/install/ff-petsc/r/lib -L/home/a/asarkar/sudhipv/software/FreeFem-sources/install/ff-petsc/r/lib -lpetsc -Wl,-rpath,/home/a/asarkar/sudhipv/software/FreeFem-sources/install/ff-petsc/r/lib -L/home/a/asarkar/sudhipv/software/FreeFem-sources/install/ff-petsc/r/lib -Wl,-rpath,/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/imkl/2020.1.217/mkl/lib/intel64 -L/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/imkl/2020.1.217/mkl/lib/intel64 -Wl,-rpath,/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Compiler/intel2020/openmpi/4.0.3/lib -L/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Compiler/intel2020/openmpi/4.0.3/lib -Wl,-rpath,/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/libfabric/1.10.1/lib -L/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/libfabric/1.10.1/lib -Wl,-rpath,/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/ucx/1.8.0/lib -L/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/ucx/1.8.0/lib -Wl,-rpath,/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/imkl/2020.1.217/lib/intel64 -L/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/imkl/2020.1.217/lib/intel64 -Wl,-rpath,/cvmfs/restricted.computecanada.ca/easybuild/software/2020/Core/intel/2020.1.217/compilers_and_libraries_2020.1.217/linux/tbb/lib/intel64/gcc4.8 -L/cvmfs/restricted.computecanada.ca/easybuild/software/2020/Core/intel/2020.1.217/compilers_and_libraries_2020.1.217/linux/tbb/lib/intel64/gcc4.8 -Wl,-rpath,/cvmfs/restricted.computecanada.ca/easybuild/software/2020/Core/intel/2020.1.217/compilers_and_libraries_2020.1.217/linux/compiler/lib/intel64_lin -L/cvmfs/restricted.computecanada.ca/easybuild/software/2020/Core/intel/2020.1.217/compilers_and_libraries_2020.1.217/linux/compiler/lib/intel64_lin -Wl,-rpath,/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Compiler/intel2020/openmpi/4.0.3/lib64 -L/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Compiler/intel2020/openmpi/4.0.3/lib64 -Wl,-rpath,/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/gcccore/9.3.0/lib/gcc/x86_64-pc-linux-gnu/9.3.0 -L/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/gcccore/9.3.0/lib/gcc/x86_64-pc-linux-gnu/9.3.0 -Wl,-rpath,/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/gcccore/9.3.0/lib64 -L/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/gcccore/9.3.0/lib64 -Wl,-rpath,/cvmfs/soft.computecanada.ca/gentoo/2020/lib64 -L/cvmfs/soft.computecanada.ca/gentoo/2020/lib64 -Wl,-rpath,/cvmfs/soft.computecanada.ca/gentoo/2020/usr/lib64 -L/cvmfs/soft.computecanada.ca/gentoo/2020/usr/lib64 -Wl,-rpath,/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/gcccore/9.3.0/lib -L/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/gcccore/9.3.0/lib -Wl,-rpath,/cvmfs/soft.computecanada.ca/gentoo/2020/lib -L/cvmfs/soft.computecanada.ca/gentoo/2020/lib -Wl,-rpath,/cvmfs/soft.computecanada.ca/gentoo/2020/usr/lib -L/cvmfs/soft.computecanada.ca/gentoo/2020/usr/lib -lHYPRE -lcmumps -ldmumps -lsmumps -lzmumps -lmumps_common -lpord -lscalapack -lumfpack -lklu -lcholmod -lbtf -lccolamd -lcolamd -lcamd -lamd -lsuitesparseconfig -lsuperlu -lmkl_rt -lmkl_intel_thread -lmkl_core -liomp5 -lpthread -lparmmg -lmmg -lmmg3d -lptesmumps -lptscotchparmetis -lptscotch -lptscotcherr -lesmumps -lscotch -lscotcherr -lpthread -lparmetis -lmetis -ltet -lstdc++ -ldl -lmpi_usempif08 -lmpi_usempi_ignore_tkr -lmpi_mpifh -lmpi -lifport -lifcoremt_pic -limf -lsvml -lm -lipgo -lirc -lpthread -lgcc_s -lirc_s -lrt -lquadmath -lstdc++ -ldl
-----------------------------------------

#############################################################################################################
#############################################################################################################
########     Finsihed Code with NL 10 Processors             #################################################
#############################################################################################################
#############################################################################################################
Number of Vertices for coarse mesh in process  0  5635876
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 9258
    Linear mg_coarse_ solve did not converge due to DIVERGED_ITS iterations 10000
    Linear mg_coarse_ solve did not converge due to DIVERGED_ITS iterations 10000
    Linear mg_coarse_ solve did not converge due to DIVERGED_ITS iterations 10000
    Linear mg_coarse_ solve did not converge due to DIVERGED_ITS iterations 10000
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4945
    Linear mg_coarse_ solve did not converge due to DIVERGED_ITS iterations 10000
    Linear mg_coarse_ solve did not converge due to DIVERGED_ITS iterations 10000
    Linear mg_coarse_ solve did not converge due to DIVERGED_ITS iterations 10000
Linear solve converged due to CONVERGED_RTOL iterations 9
time taken for solve is159.955
L2 Norm of uerr is1.86064e-06
************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

/home/a/asarkar/sudhipv/software/FreeFem-sources/src/mpi/FreeFem++-mpi on a  named nia0093.scinet.local with 400 processors, by sudhipv Sun Oct  3 13:32:11 2021
Using Petsc Release Version 3.15.0, Mar 30, 2021 

                         Max       Max/Min     Avg       Total
Time (sec):           1.601e+02     1.000   1.601e+02
Objects:              1.820e+02     1.000   1.820e+02
Flop:                 1.158e+11     1.051   1.130e+11  4.520e+13
Flop/sec:             7.237e+08     1.051   7.059e+08  2.824e+11
MPI Messages:         1.393e+06     4.000   9.727e+05  3.891e+08
MPI Message Lengths:  8.289e+08     3.126   6.864e+02  2.671e+11
MPI Reductions:       1.713e+05     1.000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flop
                            and VecAXPY() for complex vectors of length N --> 8N flop

Summary of Stages:   ----- Time ------  ----- Flop ------  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total    Count   %Total     Avg         %Total    Count   %Total
 0:      Main Stage: 1.6008e+02 100.0%  4.5199e+13 100.0%  3.891e+08 100.0%  6.864e+02      100.0%  1.713e+05 100.0%

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flop: Max - maximum over all processors
                  Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   AvgLen: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flop in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flop over all processors)/(max time over all processors)
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flop                              --- Global ---  --- Stage ----  Total
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   AvgLen  Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s
------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

BuildTwoSided         10 1.0 2.7917e-01 8.2 0.00e+00 0.0 8.9e+03 4.0e+00 6.0e+00  0  0  0  0  0   0  0  0  0  0     0
BuildTwoSidedF         2 1.0 3.7860e-03 4.1 0.00e+00 0.0 0.0e+00 0.0e+00 2.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatMult            87070 1.0 2.8980e+01 1.1 1.64e+10 1.1 1.9e+08 6.9e+02 2.0e+00 17 14 50 50  0  17 14 50 50  0 220408
MatMultAdd             9 1.0 2.1656e-02 1.4 1.03e+06 1.0 2.0e+04 2.2e+03 0.0e+00  0  0  0  0  0   0  0  0  0  0 18730
MatMultTranspose       9 1.0 1.2264e-01 9.7 0.00e+00 0.0 2.0e+04 7.6e+03 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatSolve           87062 1.0 2.8383e+01 1.1 1.71e+10 1.0 0.0e+00 0.0e+00 0.0e+00 17 15  0  0  0  17 15  0  0  0 235602
MatLUFactorSym         1 1.0 1.0345e-01 1.4 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatLUFactorNum         2 1.0 4.5915e-01 1.5 6.80e+08 1.5 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0 485013
MatILUFactorSym        1 1.0 1.4219e-03 3.4 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatResidual            9 1.0 1.9921e-02 2.7 7.19e+06 1.0 2.0e+04 1.5e+03 0.0e+00  0  0  0  0  0   0  0  0  0  0 142491
MatAssemblyBegin       6 1.0 3.9296e-03 1.3 0.00e+00 0.0 0.0e+00 0.0e+00 2.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatAssemblyEnd         6 1.0 1.3591e-01 7.5 0.00e+00 0.0 0.0e+00 0.0e+00 8.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatGetRowIJ            2 1.0 4.4203e-03 2.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatCreateSubMats       2 1.0 3.8156e-02 1.4 0.00e+00 0.0 2.2e+04 2.5e+03 2.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatGetOrdering         2 1.0 4.1680e-02 1.7 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatIncreaseOvrlp       2 1.0 1.6516e-02 1.4 0.00e+00 0.0 0.0e+00 0.0e+00 2.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecMDot            84222 1.0 3.9747e+01 1.2 3.77e+10 1.1 0.0e+00 0.0e+00 8.4e+04 23 33  0  0 49  23 33  0  0 49 369975
VecNorm            87036 1.0 9.5745e+00 1.3 2.51e+09 1.1 0.0e+00 0.0e+00 8.7e+04  5  2  0  0 51   5  2  0  0 51 102539
VecScale           87054 1.0 4.3132e-01 1.5 1.26e+09 1.1 0.0e+00 0.0e+00 0.0e+00  0  1  0  0  0   0  1  0  0  0 1138090
VecCopy             2841 1.0 1.2336e-01 1.4 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecSet            264039 1.0 5.9007e+00 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  3  0  0  0  0   3  0  0  0  0     0
VecAXPY             5626 1.0 1.2718e-01 1.5 1.63e+08 1.1 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0 501555
VecAYPX               54 1.0 7.7168e-03 1.4 4.11e+06 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0 210248
VecAXPBYCZ            18 1.0 4.1055e-03 1.4 5.13e+06 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0 493989
VecMAXPY           87036 1.0 2.3868e+01 1.1 4.01e+10 1.1 0.0e+00 0.0e+00 0.0e+00 14 35  0  0  0  14 35  0  0  0 655947
VecScatterBegin   435309 1.0 1.0780e+01 1.3 0.00e+00 0.0 3.9e+08 6.9e+02 4.0e+00  6  0100100  0   6  0100100  0     0
VecScatterEnd     435309 1.0 3.4938e+00 2.5 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0
VecNormalize       87026 1.0 9.9605e+00 1.3 3.77e+09 1.1 0.0e+00 0.0e+00 8.7e+04  5  3  0  0 51   5  3  0  0 51 147779
SFSetGraph             8 1.0 1.8377e-04 1.8 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
SFSetUp                8 1.0 2.8234e-01 6.8 0.00e+00 0.0 1.8e+04 2.8e+02 4.0e+00  0  0  0  0  0   0  0  0  0  0     0
SFReduceBegin      87062 1.0 1.8332e+00 1.4 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0
SFReduceEnd        87062 1.0 4.8610e-02 2.4 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
SFPack            435309 1.0 4.3996e-01 1.6 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
SFUnpack          435309 1.0 2.5031e-01 5.3 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
KSPSetUp               4 1.0 1.0658e+00 1.0 8.03e+08 1.4 6.7e+04 1.7e+03 3.4e+01  1  1  0  0  0   1  1  0  0  0 253787
KSPSolve               1 1.0 1.3894e+02 1.0 1.15e+11 1.1 3.9e+08 6.9e+02 1.7e+05 87 99100100100  87 99100100100 323353
KSPGMRESOrthog     84222 1.0 6.1254e+01 1.1 7.53e+10 1.1 0.0e+00 0.0e+00 8.4e+04 37 65  0  0 49  37 65  0  0 49 480154
PCSetUp                2 1.0 1.0853e+00 1.0 8.03e+08 1.4 7.8e+04 1.7e+03 4.5e+01  1  1  0  0  0   1  1  0  0  0 249305
PCSetUpOnBlocks       28 1.0 5.7615e-01 1.4 6.80e+08 1.5 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0 386516
PCApply                9 1.0 1.3890e+02 1.0 1.15e+11 1.1 3.9e+08 6.9e+02 1.7e+05 87 99100100100  87 99100100100 323394
PCApplyOnBlocks    87062 1.0 3.0446e+01 1.1 1.71e+10 1.0 0.0e+00 0.0e+00 0.0e+00 18 15  0  0  0  18 15  0  0  0 219639
------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix    13             13     79432496     0.
              Vector   106            106     30420088     0.
           Index Set    24             24      2028380     0.
   IS L to G Mapping     2              2      1746744     0.
   Star Forest Graph    14             14        16272     0.
       Krylov Solver     6              6        73864     0.
      Preconditioner     6              6         6552     0.
    Distributed Mesh     3              3        15144     0.
     Discrete System     3              3         2712     0.
           Weak Form     3              3         2472     0.
              Viewer     2              1          848     0.
========================================================================================================================
Average time to get PetscTime(): 3.2e-08
Average time for MPI_Barrier(): 8.00208e-05
Average time for zero size MPI_Send(): 1.40388e-06
#PETSc Option Table entries:
-ksp_converged_reason
-ksp_pc_side right
-ksp_rtol 1e-8
-ksp_type fgmres
-log_view
-m 2373
-mg_coarse_ksp_converged_reason
-mg_coarse_ksp_type gmres
-mg_coarse_pc_type asm
-mg_levels_pc_type asm
-mg_levels_sub_pc_type lu
-n 2373
-nw
-pc_mg_type multiplicative
-pc_type mg
#End of PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: --prefix=/home/a/asarkar/sudhipv/software/FreeFem-sources/install/ff-petsc/r MAKEFLAGS= --with-debugging=0 COPTFLAGS="-O3 -mtune=native" CXXOPTFLAGS="-O3 -mtune=native" FOPTFLAGS="-O3 -mtune=native" --with-cxx-dialect=C++11 --with-ssl=0 --with-x=0 --with-fortran-bindings=0 --with-cudac=0 --with-cc=/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Compiler/intel2020/openmpi/4.0.3/bin/mpicc --with-cxx=/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Compiler/intel2020/openmpi/4.0.3/bin/mpic++ --with-fc=/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Compiler/intel2020/openmpi/4.0.3/bin/mpif90 --with-scalar-type=real --with-blaslapack-include=/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/imkl/2020.1.217/mkl/include --with-blaslapack-lib="-Wl,-rpath,/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/imkl/2020.1.217/mkl/lib/intel64 -L/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/imkl/2020.1.217/mkl/lib/intel64 -lmkl_rt -lmkl_intel_thread -lmkl_core  -liomp5 -lpthread" --download-metis --download-ptscotch --download-hypre --download-parmetis --download-mmg --download-parmmg --download-superlu --download-suitesparse --download-tetgen --download-slepc --download-hpddm --download-scalapack --download-mumps --download-slepc-configure-arguments=--download-arpack=https://github.com/prj-/arpack-ng/archive/b64dccb.tar.gz PETSC_ARCH=fr
-----------------------------------------
Libraries compiled on 2021-05-02 15:39:49 on nia-login03.scinet.local 
Machine characteristics: Linux-3.10.0-957.27.2.el7.x86_64-x86_64-with-centos-7.6.1810-Core
Using PETSc directory: /home/a/asarkar/sudhipv/software/FreeFem-sources/install/ff-petsc/r
Using PETSc arch: 
-----------------------------------------

Using C compiler: /cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Compiler/intel2020/openmpi/4.0.3/bin/mpicc  -fPIC -wd1572 -O3 -mtune=native   
Using Fortran compiler: /cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Compiler/intel2020/openmpi/4.0.3/bin/mpif90  -fPIC -O3 -mtune=native     
-----------------------------------------

Using include paths: -I/home/a/asarkar/sudhipv/software/FreeFem-sources/install/ff-petsc/r/include -I/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/imkl/2020.1.217/mkl/include
-----------------------------------------

Using C linker: /cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Compiler/intel2020/openmpi/4.0.3/bin/mpicc
Using Fortran linker: /cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Compiler/intel2020/openmpi/4.0.3/bin/mpif90
Using libraries: -Wl,-rpath,/home/a/asarkar/sudhipv/software/FreeFem-sources/install/ff-petsc/r/lib -L/home/a/asarkar/sudhipv/software/FreeFem-sources/install/ff-petsc/r/lib -lpetsc -Wl,-rpath,/home/a/asarkar/sudhipv/software/FreeFem-sources/install/ff-petsc/r/lib -L/home/a/asarkar/sudhipv/software/FreeFem-sources/install/ff-petsc/r/lib -Wl,-rpath,/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/imkl/2020.1.217/mkl/lib/intel64 -L/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/imkl/2020.1.217/mkl/lib/intel64 -Wl,-rpath,/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Compiler/intel2020/openmpi/4.0.3/lib -L/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Compiler/intel2020/openmpi/4.0.3/lib -Wl,-rpath,/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/libfabric/1.10.1/lib -L/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/libfabric/1.10.1/lib -Wl,-rpath,/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/ucx/1.8.0/lib -L/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/ucx/1.8.0/lib -Wl,-rpath,/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/imkl/2020.1.217/lib/intel64 -L/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/imkl/2020.1.217/lib/intel64 -Wl,-rpath,/cvmfs/restricted.computecanada.ca/easybuild/software/2020/Core/intel/2020.1.217/compilers_and_libraries_2020.1.217/linux/tbb/lib/intel64/gcc4.8 -L/cvmfs/restricted.computecanada.ca/easybuild/software/2020/Core/intel/2020.1.217/compilers_and_libraries_2020.1.217/linux/tbb/lib/intel64/gcc4.8 -Wl,-rpath,/cvmfs/restricted.computecanada.ca/easybuild/software/2020/Core/intel/2020.1.217/compilers_and_libraries_2020.1.217/linux/compiler/lib/intel64_lin -L/cvmfs/restricted.computecanada.ca/easybuild/software/2020/Core/intel/2020.1.217/compilers_and_libraries_2020.1.217/linux/compiler/lib/intel64_lin -Wl,-rpath,/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Compiler/intel2020/openmpi/4.0.3/lib64 -L/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Compiler/intel2020/openmpi/4.0.3/lib64 -Wl,-rpath,/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/gcccore/9.3.0/lib/gcc/x86_64-pc-linux-gnu/9.3.0 -L/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/gcccore/9.3.0/lib/gcc/x86_64-pc-linux-gnu/9.3.0 -Wl,-rpath,/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/gcccore/9.3.0/lib64 -L/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/gcccore/9.3.0/lib64 -Wl,-rpath,/cvmfs/soft.computecanada.ca/gentoo/2020/lib64 -L/cvmfs/soft.computecanada.ca/gentoo/2020/lib64 -Wl,-rpath,/cvmfs/soft.computecanada.ca/gentoo/2020/usr/lib64 -L/cvmfs/soft.computecanada.ca/gentoo/2020/usr/lib64 -Wl,-rpath,/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/gcccore/9.3.0/lib -L/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/gcccore/9.3.0/lib -Wl,-rpath,/cvmfs/soft.computecanada.ca/gentoo/2020/lib -L/cvmfs/soft.computecanada.ca/gentoo/2020/lib -Wl,-rpath,/cvmfs/soft.computecanada.ca/gentoo/2020/usr/lib -L/cvmfs/soft.computecanada.ca/gentoo/2020/usr/lib -lHYPRE -lcmumps -ldmumps -lsmumps -lzmumps -lmumps_common -lpord -lscalapack -lumfpack -lklu -lcholmod -lbtf -lccolamd -lcolamd -lcamd -lamd -lsuitesparseconfig -lsuperlu -lmkl_rt -lmkl_intel_thread -lmkl_core -liomp5 -lpthread -lparmmg -lmmg -lmmg3d -lptesmumps -lptscotchparmetis -lptscotch -lptscotcherr -lesmumps -lscotch -lscotcherr -lpthread -lparmetis -lmetis -ltet -lstdc++ -ldl -lmpi_usempif08 -lmpi_usempi_ignore_tkr -lmpi_mpifh -lmpi -lifport -lifcoremt_pic -limf -lsvml -lm -lipgo -lirc -lpthread -lgcc_s -lirc_s -lrt -lquadmath -lstdc++ -ldl
-----------------------------------------

#############################################################################################################
#############################################################################################################
########     Finsihed Code with NL 10 Processors             #################################################
#############################################################################################################
#############################################################################################################

scontrol show jobid 6183146
JobId=6183146 JobName=2L_V2_80_400_weak
   UserId=sudhipv(3061414) GroupId=asarkar(6001189) MCS_label=N/A
   Priority=2412146 Nice=0 Account=def-asarkar QOS=normal
   JobState=COMPLETING Reason=None Dependency=(null)
   Requeue=0 Restarts=0 BatchFlag=1 Reboot=0 ExitCode=0:0
   RunTime=00:09:37 TimeLimit=00:30:00 TimeMin=N/A
   SubmitTime=2021-10-03T13:22:32 EligibleTime=2021-10-03T13:22:32
   AccrueTime=2021-10-03T13:22:32
   StartTime=2021-10-03T13:22:35 EndTime=2021-10-03T13:32:12 Deadline=N/A
   SuspendTime=None SecsPreSuspend=0 LastSchedEval=2021-10-03T13:22:35
   Partition=compute AllocNode:Sid=nia-login02:4245
   ReqNodeList=(null) ExcNodeList=(null)
   NodeList=nia[0094,0218,0242,0262,0401,0563,0578,0621,0635]
   BatchHost=nia0093
   NumNodes=9 NumCPUs=800 NumTasks=400 CPUs/Task=1 ReqB:S:C:T=0:0:*:*
   TRES=cpu=800,mem=1750000M,node=10,billing=400
   Socks/Node=* NtasksPerN:B:S:C=40:0:*:* CoreSpec=*
   MinCPUsNode=40 MinMemoryNode=175000M MinTmpDiskNode=0
   Features=[skylake|cascade] DelayBoot=00:00:00
   OverSubscribe=NO Contiguous=0 Licenses=(null) Network=(null)
   Command=/gpfs/fs0/scratch/a/asarkar/sudhipv/poisson_hpc/weak/runff_niagara_weak.sh
   WorkDir=/gpfs/fs0/scratch/a/asarkar/sudhipv/poisson_hpc/weak
   StdErr=/gpfs/fs0/scratch/a/asarkar/sudhipv/poisson_hpc/weak/2L_V2_80_400_weak-6183146.out
   StdIn=/dev/null
   StdOut=/gpfs/fs0/scratch/a/asarkar/sudhipv/poisson_hpc/weak/2L_V2_80_400_weak-6183146.out
   Power=
   MailUser=sudhipv@cmail.carleton.ca MailType=BEGIN,END,FAIL,REQUEUE,STAGE_OUT

sacct -j 6183146
       JobID    JobName    Account    Elapsed  MaxVMSize     MaxRSS  SystemCPU    UserCPU ExitCode 
------------ ---------- ---------- ---------- ---------- ---------- ---------- ---------- -------- 
6183146      2L_V2_80_+ def-asark+   00:09:37                        07:57.488   20:02:54      0:0 
6183146.bat+      batch def-asark+   00:09:37  69356632K  50098188K  02:35.076   06:01:07      0:0 
6183146.ext+     extern def-asark+   00:09:37    138360K      1068K  00:00.003  00:00.004      0:0 
6183146.0         orted def-asark+   00:01:38  31648688K  10080400K  00:29.169   01:00:19      0:0 
6183146.1         orted def-asark+   00:02:27  31997356K  10216284K  01:48.413   04:37:46      0:0 
6183146.2         orted def-asark+   00:02:35  33081896K  10443448K  03:04.825   08:23:42      0:0 
6183146.3         orted def-asark+   00:02:45                         00:00:00   00:00:00      0:0 
