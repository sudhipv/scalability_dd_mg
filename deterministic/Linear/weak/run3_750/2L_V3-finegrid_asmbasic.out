/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Compiler/intel2020/openmpi/4.0.3/bin/mpiexec
/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Compiler/intel2020/openmpi/4.0.3/bin/mpirun
Number of Vertices for coarse mesh in process  0  564001
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 5
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 5
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 5
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 4
Linear solve converged due to CONVERGED_RTOL iterations 9
time taken for solve is6.69893
L2 Norm of uerr is1.01646e-08
************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

/home/a/asarkar/sudhipv/software/FreeFem-sources/src/mpi/FreeFem++-mpi on a  named nia0074.scinet.local with 40 processors, by sudhipv Thu Oct  7 09:58:30 2021
Using Petsc Release Version 3.15.0, Mar 30, 2021 

                         Max       Max/Min     Avg       Total
Time (sec):           7.031e+00     1.000   7.031e+00
Objects:              1.420e+02     1.000   1.420e+02
Flop:                 1.201e+09     1.208   1.083e+09  4.331e+10
Flop/sec:             1.708e+08     1.208   1.540e+08  6.160e+09
MPI Messages:         1.776e+03     4.000   1.032e+03  4.129e+04
MPI Message Lengths:  2.378e+06     2.527   1.810e+03  7.473e+07
MPI Reductions:       1.950e+02     1.000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flop
                            and VecAXPY() for complex vectors of length N --> 8N flop

Summary of Stages:   ----- Time ------  ----- Flop ------  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total    Count   %Total     Avg         %Total    Count   %Total
 0:      Main Stage: 7.0311e+00 100.0%  4.3315e+10 100.0%  4.129e+04 100.0%  1.810e+03      100.0%  1.770e+02  90.8%

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flop: Max - maximum over all processors
                  Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   AvgLen: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flop in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flop over all processors)/(max time over all processors)
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flop                              --- Global ---  --- Stage ----  Total
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   AvgLen  Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s
------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

BuildTwoSided          6 1.0 1.1618e-01131.0 0.00e+00 0.0 5.6e+02 4.0e+00 5.0e+00  1  0  1  0  3   1  0  1  0  3     0
BuildTwoSidedF         2 1.0 1.1103e-03 1.4 0.00e+00 0.0 0.0e+00 0.0e+00 2.0e+00  0  0  0  0  1   0  0  0  0  1     0
MatMult              103 1.0 1.1305e-01 1.3 4.79e+07 1.0 2.0e+04 1.3e+03 2.0e+00  1  4 48 34  1   1  4 48 34  1 16762
MatMultAdd             9 1.0 1.8608e-02 1.4 1.02e+06 1.0 1.7e+03 2.4e+03 0.0e+00  0  0  4  5  0   0  0  4  5  0  2179
MatMultTranspose       9 1.0 1.0646e-01 1.1 0.00e+00 0.0 1.7e+03 7.9e+03 0.0e+00  1  0  4 18  0   1  0  4 18  0     0
MatSolve              47 1.0 5.6699e-01 1.1 4.31e+08 1.1 0.0e+00 0.0e+00 0.0e+00  8 38  0  0  0   8 38  0  0  0 29350
MatLUFactorSym         1 1.0 1.0219e-01 1.4 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0
MatLUFactorNum         1 1.0 4.2738e-01 1.3 6.76e+08 1.4 0.0e+00 0.0e+00 0.0e+00  5 53  0  0  0   5 53  0  0  0 53492
MatConvert             1 1.0 5.5970e-02 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 4.0e+00  1  0  0  0  2   1  0  0  0  2     0
MatResidual            9 1.0 1.7246e-02 1.7 7.16e+06 1.0 1.7e+03 1.6e+03 0.0e+00  0  1  4  4  0   0  1  4  4  0 16448
MatAssemblyBegin       6 1.0 3.0674e-03 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 2.0e+00  0  0  0  0  1   0  0  0  0  1     0
MatAssemblyEnd         6 1.0 5.8378e-02 1.7 0.00e+00 0.0 0.0e+00 0.0e+00 1.2e+01  1  0  0  0  6   1  0  0  0  7     0
MatGetRowIJ            3 1.0 4.7014e-03 2.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatCreateSubMats       1 1.0 2.9970e-02 1.4 0.00e+00 0.0 9.3e+02 3.7e+03 1.0e+00  0  0  2  5  1   0  0  2  5  1     0
MatGetOrdering         1 1.0 4.3904e-02 1.9 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatIncreaseOvrlp       1 1.0 8.3413e-03 1.5 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  1   0  0  0  0  1     0
VecMDot               58 1.0 6.7842e-02 3.0 1.44e+07 1.0 0.0e+00 0.0e+00 5.8e+01  1  1  0  0 30   1  1  0  0 33  8388
VecNorm               69 1.0 2.5769e-01 1.0 3.77e+06 1.0 0.0e+00 0.0e+00 6.9e+01  4  0  0  0 35   4  0  0  0 39   577
VecScale              87 1.0 1.6629e-03 2.9 1.88e+06 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0 44733
VecCopy               38 1.0 2.7342e-03 1.5 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecSet               235 1.0 1.6600e-02 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecAXPY               20 1.0 3.2393e-02 1.0 1.51e+06 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  1844
VecAYPX               54 1.0 7.6193e-03 1.3 4.09e+06 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0 21290
VecAXPBYCZ            18 1.0 4.0528e-03 1.8 5.12e+06 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0 50032
VecMAXPY              69 1.0 1.3066e-02 1.1 1.77e+07 1.0 0.0e+00 0.0e+00 0.0e+00  0  2  0  0  0   0  2  0  0  0 53470
VecScatterBegin      282 1.0 1.4895e-01 4.7 0.00e+00 0.0 3.6e+04 1.4e+03 3.0e+00  1  0 87 67  2   1  0 87 67  2     0
VecScatterEnd        282 1.0 8.6241e-02 3.6 5.56e+04 2.5 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0    20
VecNormalize          59 1.0 2.5629e-01 1.0 3.94e+06 1.0 0.0e+00 0.0e+00 5.9e+01  4  0  0  0 30   4  0  0  0 33   607
SFSetGraph             4 1.0 1.3805e-04 2.7 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
SFSetUp                4 1.0 1.1956e-0130.2 0.00e+00 0.0 1.1e+03 3.3e+02 3.0e+00  1  0  3  0  2   1  0  3  0  2     0
SFPack               282 1.0 1.3719e-03 1.9 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
SFUnpack             282 1.0 6.3704e-04 3.5 5.56e+04 2.5 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  2730
KSPSetUp               3 1.0 1.5232e+00 1.0 7.99e+08 1.3 7.6e+03 1.7e+03 3.4e+01 22 64 18 18 17  22 64 18 18 19 18165
KSPSolve               1 1.0 9.3771e-01 1.0 4.03e+08 1.1 3.3e+04 1.7e+03 1.1e+02 13 36 79 77 56  13 36 79 77 62 16685
KSPGMRESOrthog        58 1.0 7.8247e-02 2.4 2.88e+07 1.0 0.0e+00 0.0e+00 5.8e+01  1  3  0  0 30   1  3  0  0 33 14545
PCSetUp                1 1.0 1.6384e+00 1.0 7.99e+08 1.3 7.6e+03 1.7e+03 4.2e+01 23 64 18 18 22  23 64 18 18 24 16888
PCSetUpOnBlocks       19 1.0 5.3589e-01 1.3 6.76e+08 1.4 0.0e+00 0.0e+00 0.0e+00  7 53  0  0  0   7 53  0  0  0 42660
PCApply                9 1.0 9.1095e-01 1.0 3.83e+08 1.1 3.1e+04 1.8e+03 9.0e+01 13 34 75 73 46  13 34 75 73 51 16318
PCApplyOnBlocks       47 1.0 5.7172e-01 1.1 4.31e+08 1.1 0.0e+00 0.0e+00 0.0e+00  8 38  0  0  0   8 38  0  0  0 29107
------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix    12             12     76414380     0.
              Vector    83             83     27629056     0.
           Index Set    15             15      1676852     0.
   IS L to G Mapping     1              1      2805172     0.
   Star Forest Graph    10             10        11376     0.
       Krylov Solver     5              5        72264     0.
      Preconditioner     5              5         6024     0.
    Distributed Mesh     3              3        15144     0.
     Discrete System     3              3         2712     0.
           Weak Form     3              3         2472     0.
              Viewer     2              1          848     0.
========================================================================================================================
Average time to get PetscTime(): 3.09e-08
Average time for MPI_Barrier(): 4.5026e-05
Average time for zero size MPI_Send(): 1.0947e-06
#PETSc Option Table entries:
-ksp_converged_reason
-ksp_pc_side right
-ksp_rtol 1e-8
-ksp_type fgmres
-log_view
-m 750
-mg_coarse_ksp_converged_reason
-mg_coarse_ksp_type gmres
-mg_coarse_pc_type hypre
-mg_levels_pc_asm_type basic
-mg_levels_pc_type asm
-mg_levels_sub_pc_type lu
-n 750
-nw
-pc_mg_type multiplicative
-pc_type mg
#End of PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: --prefix=/home/a/asarkar/sudhipv/software/FreeFem-sources/install/ff-petsc/r MAKEFLAGS= --with-debugging=0 COPTFLAGS="-O3 -mtune=native" CXXOPTFLAGS="-O3 -mtune=native" FOPTFLAGS="-O3 -mtune=native" --with-cxx-dialect=C++11 --with-ssl=0 --with-x=0 --with-fortran-bindings=0 --with-cudac=0 --with-cc=/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Compiler/intel2020/openmpi/4.0.3/bin/mpicc --with-cxx=/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Compiler/intel2020/openmpi/4.0.3/bin/mpic++ --with-fc=/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Compiler/intel2020/openmpi/4.0.3/bin/mpif90 --with-scalar-type=real --with-blaslapack-include=/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/imkl/2020.1.217/mkl/include --with-blaslapack-lib="-Wl,-rpath,/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/imkl/2020.1.217/mkl/lib/intel64 -L/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/imkl/2020.1.217/mkl/lib/intel64 -lmkl_rt -lmkl_intel_thread -lmkl_core  -liomp5 -lpthread" --download-metis --download-ptscotch --download-hypre --download-parmetis --download-mmg --download-parmmg --download-superlu --download-suitesparse --download-tetgen --download-slepc --download-hpddm --download-scalapack --download-mumps --download-slepc-configure-arguments=--download-arpack=https://github.com/prj-/arpack-ng/archive/b64dccb.tar.gz PETSC_ARCH=fr
-----------------------------------------
Libraries compiled on 2021-05-02 15:39:49 on nia-login03.scinet.local 
Machine characteristics: Linux-3.10.0-957.27.2.el7.x86_64-x86_64-with-centos-7.6.1810-Core
Using PETSc directory: /home/a/asarkar/sudhipv/software/FreeFem-sources/install/ff-petsc/r
Using PETSc arch: 
-----------------------------------------

Using C compiler: /cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Compiler/intel2020/openmpi/4.0.3/bin/mpicc  -fPIC -wd1572 -O3 -mtune=native   
Using Fortran compiler: /cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Compiler/intel2020/openmpi/4.0.3/bin/mpif90  -fPIC -O3 -mtune=native     
-----------------------------------------

Using include paths: -I/home/a/asarkar/sudhipv/software/FreeFem-sources/install/ff-petsc/r/include -I/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/imkl/2020.1.217/mkl/include
-----------------------------------------

Using C linker: /cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Compiler/intel2020/openmpi/4.0.3/bin/mpicc
Using Fortran linker: /cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Compiler/intel2020/openmpi/4.0.3/bin/mpif90
Using libraries: -Wl,-rpath,/home/a/asarkar/sudhipv/software/FreeFem-sources/install/ff-petsc/r/lib -L/home/a/asarkar/sudhipv/software/FreeFem-sources/install/ff-petsc/r/lib -lpetsc -Wl,-rpath,/home/a/asarkar/sudhipv/software/FreeFem-sources/install/ff-petsc/r/lib -L/home/a/asarkar/sudhipv/software/FreeFem-sources/install/ff-petsc/r/lib -Wl,-rpath,/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/imkl/2020.1.217/mkl/lib/intel64 -L/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/imkl/2020.1.217/mkl/lib/intel64 -Wl,-rpath,/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Compiler/intel2020/openmpi/4.0.3/lib -L/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Compiler/intel2020/openmpi/4.0.3/lib -Wl,-rpath,/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/libfabric/1.10.1/lib -L/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/libfabric/1.10.1/lib -Wl,-rpath,/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/ucx/1.8.0/lib -L/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/ucx/1.8.0/lib -Wl,-rpath,/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/imkl/2020.1.217/lib/intel64 -L/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/imkl/2020.1.217/lib/intel64 -Wl,-rpath,/cvmfs/restricted.computecanada.ca/easybuild/software/2020/Core/intel/2020.1.217/compilers_and_libraries_2020.1.217/linux/tbb/lib/intel64/gcc4.8 -L/cvmfs/restricted.computecanada.ca/easybuild/software/2020/Core/intel/2020.1.217/compilers_and_libraries_2020.1.217/linux/tbb/lib/intel64/gcc4.8 -Wl,-rpath,/cvmfs/restricted.computecanada.ca/easybuild/software/2020/Core/intel/2020.1.217/compilers_and_libraries_2020.1.217/linux/compiler/lib/intel64_lin -L/cvmfs/restricted.computecanada.ca/easybuild/software/2020/Core/intel/2020.1.217/compilers_and_libraries_2020.1.217/linux/compiler/lib/intel64_lin -Wl,-rpath,/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Compiler/intel2020/openmpi/4.0.3/lib64 -L/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Compiler/intel2020/openmpi/4.0.3/lib64 -Wl,-rpath,/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/gcccore/9.3.0/lib/gcc/x86_64-pc-linux-gnu/9.3.0 -L/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/gcccore/9.3.0/lib/gcc/x86_64-pc-linux-gnu/9.3.0 -Wl,-rpath,/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/gcccore/9.3.0/lib64 -L/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/gcccore/9.3.0/lib64 -Wl,-rpath,/cvmfs/soft.computecanada.ca/gentoo/2020/lib64 -L/cvmfs/soft.computecanada.ca/gentoo/2020/lib64 -Wl,-rpath,/cvmfs/soft.computecanada.ca/gentoo/2020/usr/lib64 -L/cvmfs/soft.computecanada.ca/gentoo/2020/usr/lib64 -Wl,-rpath,/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/gcccore/9.3.0/lib -L/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/gcccore/9.3.0/lib -Wl,-rpath,/cvmfs/soft.computecanada.ca/gentoo/2020/lib -L/cvmfs/soft.computecanada.ca/gentoo/2020/lib -Wl,-rpath,/cvmfs/soft.computecanada.ca/gentoo/2020/usr/lib -L/cvmfs/soft.computecanada.ca/gentoo/2020/usr/lib -lHYPRE -lcmumps -ldmumps -lsmumps -lzmumps -lmumps_common -lpord -lscalapack -lumfpack -lklu -lcholmod -lbtf -lccolamd -lcolamd -lcamd -lamd -lsuitesparseconfig -lsuperlu -lmkl_rt -lmkl_intel_thread -lmkl_core -liomp5 -lpthread -lparmmg -lmmg -lmmg3d -lptesmumps -lptscotchparmetis -lptscotch -lptscotcherr -lesmumps -lscotch -lscotcherr -lpthread -lparmetis -lmetis -ltet -lstdc++ -ldl -lmpi_usempif08 -lmpi_usempi_ignore_tkr -lmpi_mpifh -lmpi -lifport -lifcoremt_pic -limf -lsvml -lm -lipgo -lirc -lpthread -lgcc_s -lirc_s -lrt -lquadmath -lstdc++ -ldl
-----------------------------------------

#############################################################################################################
#############################################################################################################
########     Finsihed Code with 40 Processors             #################################################
#############################################################################################################
#############################################################################################################
Number of Vertices for coarse mesh in process  0  1127844
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 5
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 5
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 5
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 5
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 5
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 5
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 5
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 5
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 5
Linear solve converged due to CONVERGED_RTOL iterations 9
time taken for solve is7.3068
L2 Norm of uerr is1.82684e-08
************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

/home/a/asarkar/sudhipv/software/FreeFem-sources/src/mpi/FreeFem++-mpi on a  named nia0074.scinet.local with 80 processors, by sudhipv Thu Oct  7 09:58:41 2021
Using Petsc Release Version 3.15.0, Mar 30, 2021 

                         Max       Max/Min     Avg       Total
Time (sec):           7.396e+00     1.000   7.396e+00
Objects:              1.420e+02     1.000   1.420e+02
Flop:                 1.199e+09     1.248   1.067e+09  8.532e+10
Flop/sec:             1.621e+08     1.248   1.442e+08  1.154e+10
MPI Messages:         1.824e+03     4.000   1.163e+03  9.302e+04
MPI Message Lengths:  2.505e+06     3.089   1.725e+03  1.605e+08
MPI Reductions:       2.070e+02     1.000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flop
                            and VecAXPY() for complex vectors of length N --> 8N flop

Summary of Stages:   ----- Time ------  ----- Flop ------  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total    Count   %Total     Avg         %Total    Count   %Total
 0:      Main Stage: 7.3957e+00 100.0%  8.5324e+10 100.0%  9.302e+04 100.0%  1.725e+03      100.0%  1.890e+02  91.3%

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flop: Max - maximum over all processors
                  Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   AvgLen: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flop in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flop over all processors)/(max time over all processors)
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flop                              --- Global ---  --- Stage ----  Total
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   AvgLen  Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s
------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

BuildTwoSided          6 1.0 1.6248e-01150.3 0.00e+00 0.0 1.2e+03 4.0e+00 5.0e+00  1  0  1  0  2   1  0  1  0  3     0
BuildTwoSidedF         2 1.0 4.5523e-03 5.8 0.00e+00 0.0 0.0e+00 0.0e+00 2.0e+00  0  0  0  0  1   0  0  0  0  1     0
MatMult              109 1.0 1.2120e-01 1.3 4.91e+07 1.0 4.6e+04 1.2e+03 2.0e+00  1  5 50 35  1   1  5 50 35  1 32011
MatMultAdd             9 1.0 1.7238e-02 1.4 1.03e+06 1.0 3.7e+03 2.3e+03 0.0e+00  0  0  4  5  0   0  0  4  5  0  4706
MatMultTranspose       9 1.0 4.3882e-02 3.6 0.00e+00 0.0 3.7e+03 7.7e+03 0.0e+00  0  0  4 18  0   0  0  4 18  0     0
MatSolve              47 1.0 5.7471e-01 1.1 4.30e+08 1.1 0.0e+00 0.0e+00 0.0e+00  7 39  0  0  0   7 39  0  0  0 57456
MatLUFactorSym         1 1.0 1.0149e-01 1.4 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0
MatLUFactorNum         1 1.0 4.2415e-01 1.5 6.71e+08 1.4 0.0e+00 0.0e+00 0.0e+00  5 52  0  0  0   5 52  0  0  0 104724
MatConvert             1 1.0 3.8722e-03 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 4.0e+00  0  0  0  0  2   0  0  0  0  2     0
MatResidual            9 1.0 1.8422e-02 1.9 7.18e+06 1.0 3.7e+03 1.5e+03 0.0e+00  0  1  4  4  0   0  1  4  4  0 30810
MatAssemblyBegin       6 1.0 4.6193e-03 1.7 0.00e+00 0.0 0.0e+00 0.0e+00 2.0e+00  0  0  0  0  1   0  0  0  0  1     0
MatAssemblyEnd         6 1.0 1.2775e-0110.4 0.00e+00 0.0 0.0e+00 0.0e+00 1.2e+01  2  0  0  0  6   2  0  0  0  6     0
MatGetRowIJ            3 1.0 4.5482e-03 2.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatCreateSubMats       1 1.0 3.0736e-02 1.5 0.00e+00 0.0 2.0e+03 3.5e+03 1.0e+00  0  0  2  5  0   0  0  2  5  1     0
MatGetOrdering         1 1.0 4.3476e-02 1.8 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatIncreaseOvrlp       1 1.0 9.7617e-03 1.5 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  1     0
VecMDot               64 1.0 7.3336e-02 3.4 1.53e+07 1.0 0.0e+00 0.0e+00 6.4e+01  1  1  0  0 31   1  1  0  0 34 16444
VecNorm               75 1.0 7.2151e-02 1.1 3.95e+06 1.0 0.0e+00 0.0e+00 7.5e+01  1  0  0  0 36   1  0  0  0 40  4312
VecScale              93 1.0 9.0450e-04 1.7 1.97e+06 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0 171977
VecCopy               38 1.0 2.6528e-03 1.4 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecSet               241 1.0 1.7485e-02 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecAXPY               20 1.0 2.7304e-03 2.3 1.51e+06 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0 43751
VecAYPX               54 1.0 7.3262e-03 1.2 4.10e+06 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0 44295
VecAXPBYCZ            18 1.0 3.9008e-03 1.7 5.13e+06 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0 103988
VecMAXPY              75 1.0 1.4443e-02 1.2 1.87e+07 1.0 0.0e+00 0.0e+00 0.0e+00  0  2  0  0  0   0  2  0  0  0 102383
VecScatterBegin      288 1.0 1.9264e-01 6.2 0.00e+00 0.0 8.2e+04 1.3e+03 3.0e+00  1  0 88 67  1   1  0 88 67  2     0
VecScatterEnd        288 1.0 9.1172e-02 9.1 5.78e+04 3.1 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0    41
VecNormalize          65 1.0 6.9369e-02 1.1 4.21e+06 1.0 0.0e+00 0.0e+00 6.5e+01  1  0  0  0 31   1  0  0  0 34  4778
SFSetGraph             4 1.0 1.2902e-04 2.5 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
SFSetUp                4 1.0 1.6392e-0135.0 0.00e+00 0.0 2.4e+03 3.2e+02 3.0e+00  1  0  3  0  1   1  0  3  0  2     0
SFPack               288 1.0 1.3960e-03 3.4 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
SFUnpack             288 1.0 1.0315e-03 6.9 5.78e+04 3.1 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  3581
KSPSetUp               3 1.0 9.2019e-01 1.0 7.94e+08 1.4 1.7e+04 1.7e+03 3.4e+01 12 63 18 17 16  12 63 18 17 18 58654
KSPSolve               1 1.0 9.1004e-01 1.0 4.05e+08 1.1 7.4e+04 1.7e+03 1.2e+02 12 37 80 77 58  12 37 80 77 64 34450
KSPGMRESOrthog        64 1.0 8.3545e-02 2.6 3.06e+07 1.0 0.0e+00 0.0e+00 6.4e+01  1  3  0  0 31   1  3  0  0 34 28869
PCSetUp                1 1.0 9.9630e-01 1.0 7.94e+08 1.4 1.7e+04 1.7e+03 4.2e+01 13 63 18 17 20  13 63 18 17 22 54174
PCSetUpOnBlocks       19 1.0 5.4454e-01 1.4 6.71e+08 1.4 0.0e+00 0.0e+00 0.0e+00  6 52  0  0  0   6 52  0  0  0 81572
PCApply                9 1.0 8.8102e-01 1.0 3.85e+08 1.1 7.1e+04 1.7e+03 1.0e+02 12 35 76 73 49  12 35 76 73 54 33810
PCApplyOnBlocks       47 1.0 5.7993e-01 1.1 4.30e+08 1.1 0.0e+00 0.0e+00 0.0e+00  7 39  0  0  0   7 39  0  0  0 56940
------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix    12             12     76405960     0.
              Vector    83             83     27754872     0.
           Index Set    15             15      1678844     0.
   IS L to G Mapping     1              1      2028588     0.
   Star Forest Graph    10             10        11376     0.
       Krylov Solver     5              5        72264     0.
      Preconditioner     5              5         6024     0.
    Distributed Mesh     3              3        15144     0.
     Discrete System     3              3         2712     0.
           Weak Form     3              3         2472     0.
              Viewer     2              1          848     0.
========================================================================================================================
Average time to get PetscTime(): 2.99e-08
Average time for MPI_Barrier(): 4.8018e-05
Average time for zero size MPI_Send(): 1.11051e-06
#PETSc Option Table entries:
-ksp_converged_reason
-ksp_pc_side right
-ksp_rtol 1e-8
-ksp_type fgmres
-log_view
-m 1061
-mg_coarse_ksp_converged_reason
-mg_coarse_ksp_type gmres
-mg_coarse_pc_type hypre
-mg_levels_pc_asm_type basic
-mg_levels_pc_type asm
-mg_levels_sub_pc_type lu
-n 1061
-nw
-pc_mg_type multiplicative
-pc_type mg
#End of PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: --prefix=/home/a/asarkar/sudhipv/software/FreeFem-sources/install/ff-petsc/r MAKEFLAGS= --with-debugging=0 COPTFLAGS="-O3 -mtune=native" CXXOPTFLAGS="-O3 -mtune=native" FOPTFLAGS="-O3 -mtune=native" --with-cxx-dialect=C++11 --with-ssl=0 --with-x=0 --with-fortran-bindings=0 --with-cudac=0 --with-cc=/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Compiler/intel2020/openmpi/4.0.3/bin/mpicc --with-cxx=/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Compiler/intel2020/openmpi/4.0.3/bin/mpic++ --with-fc=/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Compiler/intel2020/openmpi/4.0.3/bin/mpif90 --with-scalar-type=real --with-blaslapack-include=/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/imkl/2020.1.217/mkl/include --with-blaslapack-lib="-Wl,-rpath,/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/imkl/2020.1.217/mkl/lib/intel64 -L/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/imkl/2020.1.217/mkl/lib/intel64 -lmkl_rt -lmkl_intel_thread -lmkl_core  -liomp5 -lpthread" --download-metis --download-ptscotch --download-hypre --download-parmetis --download-mmg --download-parmmg --download-superlu --download-suitesparse --download-tetgen --download-slepc --download-hpddm --download-scalapack --download-mumps --download-slepc-configure-arguments=--download-arpack=https://github.com/prj-/arpack-ng/archive/b64dccb.tar.gz PETSC_ARCH=fr
-----------------------------------------
Libraries compiled on 2021-05-02 15:39:49 on nia-login03.scinet.local 
Machine characteristics: Linux-3.10.0-957.27.2.el7.x86_64-x86_64-with-centos-7.6.1810-Core
Using PETSc directory: /home/a/asarkar/sudhipv/software/FreeFem-sources/install/ff-petsc/r
Using PETSc arch: 
-----------------------------------------

Using C compiler: /cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Compiler/intel2020/openmpi/4.0.3/bin/mpicc  -fPIC -wd1572 -O3 -mtune=native   
Using Fortran compiler: /cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Compiler/intel2020/openmpi/4.0.3/bin/mpif90  -fPIC -O3 -mtune=native     
-----------------------------------------

Using include paths: -I/home/a/asarkar/sudhipv/software/FreeFem-sources/install/ff-petsc/r/include -I/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/imkl/2020.1.217/mkl/include
-----------------------------------------

Using C linker: /cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Compiler/intel2020/openmpi/4.0.3/bin/mpicc
Using Fortran linker: /cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Compiler/intel2020/openmpi/4.0.3/bin/mpif90
Using libraries: -Wl,-rpath,/home/a/asarkar/sudhipv/software/FreeFem-sources/install/ff-petsc/r/lib -L/home/a/asarkar/sudhipv/software/FreeFem-sources/install/ff-petsc/r/lib -lpetsc -Wl,-rpath,/home/a/asarkar/sudhipv/software/FreeFem-sources/install/ff-petsc/r/lib -L/home/a/asarkar/sudhipv/software/FreeFem-sources/install/ff-petsc/r/lib -Wl,-rpath,/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/imkl/2020.1.217/mkl/lib/intel64 -L/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/imkl/2020.1.217/mkl/lib/intel64 -Wl,-rpath,/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Compiler/intel2020/openmpi/4.0.3/lib -L/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Compiler/intel2020/openmpi/4.0.3/lib -Wl,-rpath,/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/libfabric/1.10.1/lib -L/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/libfabric/1.10.1/lib -Wl,-rpath,/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/ucx/1.8.0/lib -L/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/ucx/1.8.0/lib -Wl,-rpath,/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/imkl/2020.1.217/lib/intel64 -L/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/imkl/2020.1.217/lib/intel64 -Wl,-rpath,/cvmfs/restricted.computecanada.ca/easybuild/software/2020/Core/intel/2020.1.217/compilers_and_libraries_2020.1.217/linux/tbb/lib/intel64/gcc4.8 -L/cvmfs/restricted.computecanada.ca/easybuild/software/2020/Core/intel/2020.1.217/compilers_and_libraries_2020.1.217/linux/tbb/lib/intel64/gcc4.8 -Wl,-rpath,/cvmfs/restricted.computecanada.ca/easybuild/software/2020/Core/intel/2020.1.217/compilers_and_libraries_2020.1.217/linux/compiler/lib/intel64_lin -L/cvmfs/restricted.computecanada.ca/easybuild/software/2020/Core/intel/2020.1.217/compilers_and_libraries_2020.1.217/linux/compiler/lib/intel64_lin -Wl,-rpath,/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Compiler/intel2020/openmpi/4.0.3/lib64 -L/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Compiler/intel2020/openmpi/4.0.3/lib64 -Wl,-rpath,/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/gcccore/9.3.0/lib/gcc/x86_64-pc-linux-gnu/9.3.0 -L/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/gcccore/9.3.0/lib/gcc/x86_64-pc-linux-gnu/9.3.0 -Wl,-rpath,/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/gcccore/9.3.0/lib64 -L/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/gcccore/9.3.0/lib64 -Wl,-rpath,/cvmfs/soft.computecanada.ca/gentoo/2020/lib64 -L/cvmfs/soft.computecanada.ca/gentoo/2020/lib64 -Wl,-rpath,/cvmfs/soft.computecanada.ca/gentoo/2020/usr/lib64 -L/cvmfs/soft.computecanada.ca/gentoo/2020/usr/lib64 -Wl,-rpath,/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/gcccore/9.3.0/lib -L/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/gcccore/9.3.0/lib -Wl,-rpath,/cvmfs/soft.computecanada.ca/gentoo/2020/lib -L/cvmfs/soft.computecanada.ca/gentoo/2020/lib -Wl,-rpath,/cvmfs/soft.computecanada.ca/gentoo/2020/usr/lib -L/cvmfs/soft.computecanada.ca/gentoo/2020/usr/lib -lHYPRE -lcmumps -ldmumps -lsmumps -lzmumps -lmumps_common -lpord -lscalapack -lumfpack -lklu -lcholmod -lbtf -lccolamd -lcolamd -lcamd -lamd -lsuitesparseconfig -lsuperlu -lmkl_rt -lmkl_intel_thread -lmkl_core -liomp5 -lpthread -lparmmg -lmmg -lmmg3d -lptesmumps -lptscotchparmetis -lptscotch -lptscotcherr -lesmumps -lscotch -lscotcherr -lpthread -lparmetis -lmetis -ltet -lstdc++ -ldl -lmpi_usempif08 -lmpi_usempi_ignore_tkr -lmpi_mpifh -lmpi -lifport -lifcoremt_pic -limf -lsvml -lm -lipgo -lirc -lpthread -lgcc_s -lirc_s -lrt -lquadmath -lstdc++ -ldl
-----------------------------------------

#############################################################################################################
#############################################################################################################
########     Finsihed Code with NL 80 Processors             #################################################
#############################################################################################################
#############################################################################################################
Number of Vertices for coarse mesh in process  0  2253001
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 5
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 5
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 5
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 5
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 5
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 5
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 5
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 5
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 5
Linear solve converged due to CONVERGED_RTOL iterations 9
time taken for solve is10.8511
L2 Norm of uerr is1.76494e-08
************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

/home/a/asarkar/sudhipv/software/FreeFem-sources/src/mpi/FreeFem++-mpi on a  named nia0074.scinet.local with 160 processors, by sudhipv Thu Oct  7 09:58:55 2021
Using Petsc Release Version 3.15.0, Mar 30, 2021 

                         Max       Max/Min     Avg       Total
Time (sec):           1.094e+01     1.000   1.094e+01
Objects:              1.420e+02     1.000   1.420e+02
Flop:                 1.198e+09     1.251   1.073e+09  1.717e+11
Flop/sec:             1.096e+08     1.251   9.815e+07  1.570e+10
MPI Messages:         1.824e+03     4.000   1.211e+03  1.938e+05
MPI Message Lengths:  2.807e+06     3.100   1.731e+03  3.354e+08
MPI Reductions:       2.070e+02     1.000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flop
                            and VecAXPY() for complex vectors of length N --> 8N flop

Summary of Stages:   ----- Time ------  ----- Flop ------  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total    Count   %Total     Avg         %Total    Count   %Total
 0:      Main Stage: 1.0936e+01 100.0%  1.7175e+11 100.0%  1.938e+05 100.0%  1.731e+03      100.0%  1.890e+02  91.3%

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flop: Max - maximum over all processors
                  Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   AvgLen: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flop in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flop over all processors)/(max time over all processors)
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flop                              --- Global ---  --- Stage ----  Total
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   AvgLen  Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s
------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

BuildTwoSided          6 1.0 1.8397e-0114.1 0.00e+00 0.0 2.6e+03 4.0e+00 5.0e+00  1  0  1  0  2   1  0  1  0  3     0
BuildTwoSidedF         2 1.0 4.3888e-03 6.2 0.00e+00 0.0 0.0e+00 0.0e+00 2.0e+00  0  0  0  0  1   0  0  0  0  1     0
MatMult              109 1.0 1.2731e-01 1.5 4.91e+07 1.0 9.6e+04 1.2e+03 2.0e+00  1  5 50 35  1   1  5 50 35  1 60903
MatMultAdd             9 1.0 1.8893e-02 1.5 1.02e+06 1.0 7.6e+03 2.3e+03 0.0e+00  0  0  4  5  0   0  0  4  5  0  8580
MatMultTranspose       9 1.0 2.7245e-02 2.2 0.00e+00 0.0 7.6e+03 7.7e+03 0.0e+00  0  0  4 18  0   0  0  4 18  0     0
MatSolve              47 1.0 5.8023e-01 1.2 4.31e+08 1.1 0.0e+00 0.0e+00 0.0e+00  5 39  0  0  0   5 39  0  0  0 114167
MatLUFactorSym         1 1.0 1.1076e-01 1.6 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0
MatLUFactorNum         1 1.0 4.4887e-01 1.6 6.71e+08 1.4 0.0e+00 0.0e+00 0.0e+00  3 52  0  0  0   3 52  0  0  0 199947
MatConvert             1 1.0 8.4066e-03 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 4.0e+00  0  0  0  0  2   0  0  0  0  2     0
MatResidual            9 1.0 1.8532e-02 2.2 7.17e+06 1.0 7.6e+03 1.5e+03 0.0e+00  0  1  4  4  0   0  1  4  4  0 61210
MatAssemblyBegin       6 1.0 4.4569e-03 1.8 0.00e+00 0.0 0.0e+00 0.0e+00 2.0e+00  0  0  0  0  1   0  0  0  0  1     0
MatAssemblyEnd         6 1.0 1.7856e-01 2.4 0.00e+00 0.0 0.0e+00 0.0e+00 1.2e+01  1  0  0  0  6   1  0  0  0  6     0
MatGetRowIJ            3 1.0 4.3734e-03 2.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatCreateSubMats       1 1.0 3.1119e-02 1.5 0.00e+00 0.0 4.2e+03 3.6e+03 1.0e+00  0  0  2  5  0   0  0  2  5  1     0
MatGetOrdering         1 1.0 4.3298e-02 1.8 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatIncreaseOvrlp       1 1.0 1.0284e-02 1.5 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  1     0
VecMDot               64 1.0 1.0631e-01 2.6 1.53e+07 1.0 0.0e+00 0.0e+00 6.4e+01  1  1  0  0 31   1  1  0  0 34 22665
VecNorm               75 1.0 8.8031e-02 1.1 3.94e+06 1.0 0.0e+00 0.0e+00 7.5e+01  1  0  0  0 36   1  0  0  0 40  7061
VecScale              93 1.0 9.6152e-04 1.9 1.97e+06 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0 323224
VecCopy               38 1.0 2.5604e-03 1.4 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecSet               241 1.0 1.9249e-02 1.4 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecAXPY               20 1.0 2.4531e-03 1.8 1.51e+06 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0 97298
VecAYPX               54 1.0 7.9461e-03 1.4 4.10e+06 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0 81603
VecAXPBYCZ            18 1.0 4.1353e-03 2.0 5.12e+06 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0 196006
VecMAXPY              75 1.0 1.4844e-02 1.3 1.87e+07 1.0 0.0e+00 0.0e+00 0.0e+00  0  2  0  0  0   0  2  0  0  0 199033
VecScatterBegin      288 1.0 2.1323e-01 5.7 0.00e+00 0.0 1.7e+05 1.3e+03 3.0e+00  1  0 88 67  1   1  0 88 67  2     0
VecScatterEnd        288 1.0 1.1131e-0134.5 6.46e+04 3.1 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0    69
VecNormalize          65 1.0 8.6271e-02 1.1 4.21e+06 1.0 0.0e+00 0.0e+00 6.5e+01  1  0  0  0 31   1  0  0  0 34  7676
SFSetGraph             4 1.0 1.4959e-04 2.9 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
SFSetUp                4 1.0 1.8480e-0113.1 0.00e+00 0.0 5.1e+03 3.2e+02 3.0e+00  1  0  3  0  1   1  0  3  0  2     0
SFPack               288 1.0 1.4675e-03 2.9 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
SFUnpack             288 1.0 6.4541e-04 3.3 6.46e+04 3.1 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0 11973
KSPSetUp               3 1.0 9.3098e-01 1.0 7.94e+08 1.4 3.5e+04 1.7e+03 3.4e+01  9 63 18 17 16   9 63 18 17 18 116975
KSPSolve               1 1.0 9.7389e-01 1.0 4.07e+08 1.1 1.5e+05 1.7e+03 1.2e+02  9 37 80 77 58   9 37 80 77 64 64531
KSPGMRESOrthog        64 1.0 1.1665e-01 2.3 3.05e+07 1.0 0.0e+00 0.0e+00 6.4e+01  1  3  0  0 31   1  3  0  0 34 41312
PCSetUp                1 1.0 1.0065e+00 1.0 7.94e+08 1.4 3.5e+04 1.7e+03 4.2e+01  9 63 18 17 20   9 63 18 17 22 108196
PCSetUpOnBlocks       19 1.0 5.6263e-01 1.5 6.71e+08 1.4 0.0e+00 0.0e+00 0.0e+00  4 52  0  0  0   4 52  0  0  0 159517
PCApply                9 1.0 9.3557e-01 1.0 3.87e+08 1.1 1.5e+05 1.7e+03 1.0e+02  8 35 76 73 49   8 35 76 73 54 63834
PCApplyOnBlocks       47 1.0 5.8555e-01 1.2 4.31e+08 1.1 0.0e+00 0.0e+00 0.0e+00  5 39  0  0  0   5 39  0  0  0 113130
------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix    12             12     77347872     0.
              Vector    83             83     27664400     0.
           Index Set    15             15      1677124     0.
   IS L to G Mapping     1              1       681680     0.
   Star Forest Graph    10             10        11376     0.
       Krylov Solver     5              5        72264     0.
      Preconditioner     5              5         6024     0.
    Distributed Mesh     3              3        15144     0.
     Discrete System     3              3         2712     0.
           Weak Form     3              3         2472     0.
              Viewer     2              1          848     0.
========================================================================================================================
Average time to get PetscTime(): 3.22e-08
Average time for MPI_Barrier(): 0.000124032
Average time for zero size MPI_Send(): 1.23227e-06
#PETSc Option Table entries:
-ksp_converged_reason
-ksp_pc_side right
-ksp_rtol 1e-8
-ksp_type fgmres
-log_view
-m 1500
-mg_coarse_ksp_converged_reason
-mg_coarse_ksp_type gmres
-mg_coarse_pc_type hypre
-mg_levels_pc_asm_type basic
-mg_levels_pc_type asm
-mg_levels_sub_pc_type lu
-n 1500
-nw
-pc_mg_type multiplicative
-pc_type mg
#End of PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: --prefix=/home/a/asarkar/sudhipv/software/FreeFem-sources/install/ff-petsc/r MAKEFLAGS= --with-debugging=0 COPTFLAGS="-O3 -mtune=native" CXXOPTFLAGS="-O3 -mtune=native" FOPTFLAGS="-O3 -mtune=native" --with-cxx-dialect=C++11 --with-ssl=0 --with-x=0 --with-fortran-bindings=0 --with-cudac=0 --with-cc=/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Compiler/intel2020/openmpi/4.0.3/bin/mpicc --with-cxx=/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Compiler/intel2020/openmpi/4.0.3/bin/mpic++ --with-fc=/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Compiler/intel2020/openmpi/4.0.3/bin/mpif90 --with-scalar-type=real --with-blaslapack-include=/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/imkl/2020.1.217/mkl/include --with-blaslapack-lib="-Wl,-rpath,/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/imkl/2020.1.217/mkl/lib/intel64 -L/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/imkl/2020.1.217/mkl/lib/intel64 -lmkl_rt -lmkl_intel_thread -lmkl_core  -liomp5 -lpthread" --download-metis --download-ptscotch --download-hypre --download-parmetis --download-mmg --download-parmmg --download-superlu --download-suitesparse --download-tetgen --download-slepc --download-hpddm --download-scalapack --download-mumps --download-slepc-configure-arguments=--download-arpack=https://github.com/prj-/arpack-ng/archive/b64dccb.tar.gz PETSC_ARCH=fr
-----------------------------------------
Libraries compiled on 2021-05-02 15:39:49 on nia-login03.scinet.local 
Machine characteristics: Linux-3.10.0-957.27.2.el7.x86_64-x86_64-with-centos-7.6.1810-Core
Using PETSc directory: /home/a/asarkar/sudhipv/software/FreeFem-sources/install/ff-petsc/r
Using PETSc arch: 
-----------------------------------------

Using C compiler: /cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Compiler/intel2020/openmpi/4.0.3/bin/mpicc  -fPIC -wd1572 -O3 -mtune=native   
Using Fortran compiler: /cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Compiler/intel2020/openmpi/4.0.3/bin/mpif90  -fPIC -O3 -mtune=native     
-----------------------------------------

Using include paths: -I/home/a/asarkar/sudhipv/software/FreeFem-sources/install/ff-petsc/r/include -I/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/imkl/2020.1.217/mkl/include
-----------------------------------------

Using C linker: /cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Compiler/intel2020/openmpi/4.0.3/bin/mpicc
Using Fortran linker: /cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Compiler/intel2020/openmpi/4.0.3/bin/mpif90
Using libraries: -Wl,-rpath,/home/a/asarkar/sudhipv/software/FreeFem-sources/install/ff-petsc/r/lib -L/home/a/asarkar/sudhipv/software/FreeFem-sources/install/ff-petsc/r/lib -lpetsc -Wl,-rpath,/home/a/asarkar/sudhipv/software/FreeFem-sources/install/ff-petsc/r/lib -L/home/a/asarkar/sudhipv/software/FreeFem-sources/install/ff-petsc/r/lib -Wl,-rpath,/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/imkl/2020.1.217/mkl/lib/intel64 -L/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/imkl/2020.1.217/mkl/lib/intel64 -Wl,-rpath,/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Compiler/intel2020/openmpi/4.0.3/lib -L/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Compiler/intel2020/openmpi/4.0.3/lib -Wl,-rpath,/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/libfabric/1.10.1/lib -L/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/libfabric/1.10.1/lib -Wl,-rpath,/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/ucx/1.8.0/lib -L/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/ucx/1.8.0/lib -Wl,-rpath,/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/imkl/2020.1.217/lib/intel64 -L/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/imkl/2020.1.217/lib/intel64 -Wl,-rpath,/cvmfs/restricted.computecanada.ca/easybuild/software/2020/Core/intel/2020.1.217/compilers_and_libraries_2020.1.217/linux/tbb/lib/intel64/gcc4.8 -L/cvmfs/restricted.computecanada.ca/easybuild/software/2020/Core/intel/2020.1.217/compilers_and_libraries_2020.1.217/linux/tbb/lib/intel64/gcc4.8 -Wl,-rpath,/cvmfs/restricted.computecanada.ca/easybuild/software/2020/Core/intel/2020.1.217/compilers_and_libraries_2020.1.217/linux/compiler/lib/intel64_lin -L/cvmfs/restricted.computecanada.ca/easybuild/software/2020/Core/intel/2020.1.217/compilers_and_libraries_2020.1.217/linux/compiler/lib/intel64_lin -Wl,-rpath,/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Compiler/intel2020/openmpi/4.0.3/lib64 -L/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Compiler/intel2020/openmpi/4.0.3/lib64 -Wl,-rpath,/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/gcccore/9.3.0/lib/gcc/x86_64-pc-linux-gnu/9.3.0 -L/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/gcccore/9.3.0/lib/gcc/x86_64-pc-linux-gnu/9.3.0 -Wl,-rpath,/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/gcccore/9.3.0/lib64 -L/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/gcccore/9.3.0/lib64 -Wl,-rpath,/cvmfs/soft.computecanada.ca/gentoo/2020/lib64 -L/cvmfs/soft.computecanada.ca/gentoo/2020/lib64 -Wl,-rpath,/cvmfs/soft.computecanada.ca/gentoo/2020/usr/lib64 -L/cvmfs/soft.computecanada.ca/gentoo/2020/usr/lib64 -Wl,-rpath,/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/gcccore/9.3.0/lib -L/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/gcccore/9.3.0/lib -Wl,-rpath,/cvmfs/soft.computecanada.ca/gentoo/2020/lib -L/cvmfs/soft.computecanada.ca/gentoo/2020/lib -Wl,-rpath,/cvmfs/soft.computecanada.ca/gentoo/2020/usr/lib -L/cvmfs/soft.computecanada.ca/gentoo/2020/usr/lib -lHYPRE -lcmumps -ldmumps -lsmumps -lzmumps -lmumps_common -lpord -lscalapack -lumfpack -lklu -lcholmod -lbtf -lccolamd -lcolamd -lcamd -lamd -lsuitesparseconfig -lsuperlu -lmkl_rt -lmkl_intel_thread -lmkl_core -liomp5 -lpthread -lparmmg -lmmg -lmmg3d -lptesmumps -lptscotchparmetis -lptscotch -lptscotcherr -lesmumps -lscotch -lscotcherr -lpthread -lparmetis -lmetis -ltet -lstdc++ -ldl -lmpi_usempif08 -lmpi_usempi_ignore_tkr -lmpi_mpifh -lmpi -lifport -lifcoremt_pic -limf -lsvml -lm -lipgo -lirc -lpthread -lgcc_s -lirc_s -lrt -lquadmath -lstdc++ -ldl
-----------------------------------------

#############################################################################################################
#############################################################################################################
########     Finsihed Code with NL 160 Processors             #################################################
#############################################################################################################
#############################################################################################################
Number of Vertices for coarse mesh in process  0  3381921
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 5
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 5
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 5
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 5
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 5
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 5
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 5
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 5
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 5
Linear solve converged due to CONVERGED_RTOL iterations 9
time taken for solve is14.5641
L2 Norm of uerr is1.60735e-08
************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

/home/a/asarkar/sudhipv/software/FreeFem-sources/src/mpi/FreeFem++-mpi on a  named nia0074.scinet.local with 240 processors, by sudhipv Thu Oct  7 09:59:13 2021
Using Petsc Release Version 3.15.0, Mar 30, 2021 

                         Max       Max/Min     Avg       Total
Time (sec):           1.465e+01     1.000   1.465e+01
Objects:              1.420e+02     1.000   1.420e+02
Flop:                 1.256e+09     1.340   1.074e+09  2.578e+11
Flop/sec:             8.571e+07     1.340   7.332e+07  1.760e+10
MPI Messages:         1.824e+03     4.000   1.248e+03  2.996e+05
MPI Message Lengths:  2.793e+06     3.268   1.689e+03  5.061e+08
MPI Reductions:       2.070e+02     1.000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flop
                            and VecAXPY() for complex vectors of length N --> 8N flop

Summary of Stages:   ----- Time ------  ----- Flop ------  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total    Count   %Total     Avg         %Total    Count   %Total
 0:      Main Stage: 1.4650e+01 100.0%  2.5780e+11 100.0%  2.996e+05 100.0%  1.689e+03      100.0%  1.890e+02  91.3%

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flop: Max - maximum over all processors
                  Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   AvgLen: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flop in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flop over all processors)/(max time over all processors)
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flop                              --- Global ---  --- Stage ----  Total
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   AvgLen  Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s
------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

BuildTwoSided          6 1.0 2.0030e-0122.6 0.00e+00 0.0 3.9e+03 4.0e+00 5.0e+00  1  0  1  0  2   1  0  1  0  3     0
BuildTwoSidedF         2 1.0 3.4760e-03 4.7 0.00e+00 0.0 0.0e+00 0.0e+00 2.0e+00  0  0  0  0  1   0  0  0  0  1     0
MatMult              109 1.0 1.2818e-01 1.5 4.92e+07 1.0 1.5e+05 1.2e+03 2.0e+00  1  5 50 35  1   1  5 50 35  1 90819
MatMultAdd             9 1.0 1.7531e-02 1.4 1.03e+06 1.0 1.2e+04 2.2e+03 0.0e+00  0  0  4  5  0   0  0  4  5  0 13882
MatMultTranspose       9 1.0 2.4563e-02 2.0 0.00e+00 0.0 1.2e+04 7.5e+03 0.0e+00  0  0  4 18  0   0  0  4 18  0     0
MatSolve              47 1.0 5.8822e-01 1.1 4.45e+08 1.1 0.0e+00 0.0e+00 0.0e+00  4 39  0  0  0   4 39  0  0  0 169144
MatLUFactorSym         1 1.0 1.0966e-01 1.6 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0
MatLUFactorNum         1 1.0 4.4600e-01 1.7 7.11e+08 1.6 0.0e+00 0.0e+00 0.0e+00  3 52  0  0  0   3 52  0  0  0 301910
MatConvert             1 1.0 6.5563e-03 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 4.0e+00  0  0  0  0  2   0  0  0  0  2     0
MatResidual            9 1.0 1.7290e-02 2.2 7.18e+06 1.0 1.2e+04 1.5e+03 0.0e+00  0  1  4  4  0   0  1  4  4  0 98497
MatAssemblyBegin       6 1.0 3.5500e-03 1.3 0.00e+00 0.0 0.0e+00 0.0e+00 2.0e+00  0  0  0  0  1   0  0  0  0  1     0
MatAssemblyEnd         6 1.0 1.1592e-01 4.2 0.00e+00 0.0 0.0e+00 0.0e+00 1.2e+01  0  0  0  0  6   0  0  0  0  6     0
MatGetRowIJ            3 1.0 4.2216e-03 2.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatCreateSubMats       1 1.0 3.0740e-02 1.4 0.00e+00 0.0 6.6e+03 3.5e+03 1.0e+00  0  0  2  5  0   0  0  2  5  1     0
MatGetOrdering         1 1.0 4.2496e-02 1.8 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatIncreaseOvrlp       1 1.0 1.0855e-02 1.4 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  1     0
VecMDot               64 1.0 9.1503e-02 2.0 1.53e+07 1.0 0.0e+00 0.0e+00 6.4e+01  0  1  0  0 31   0  1  0  0 34 39530
VecNorm               75 1.0 9.7018e-02 1.1 3.95e+06 1.0 0.0e+00 0.0e+00 7.5e+01  1  0  0  0 36   1  0  0  0 40  9618
VecScale              93 1.0 1.0334e-03 2.0 1.97e+06 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0 451485
VecCopy               38 1.0 3.2565e-03 1.8 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecSet               241 1.0 1.9649e-02 1.4 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecAXPY               20 1.0 2.4196e-03 1.9 1.51e+06 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0 148089
VecAYPX               54 1.0 8.1275e-03 1.4 4.10e+06 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0 119774
VecAXPBYCZ            18 1.0 4.4736e-03 2.3 5.13e+06 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0 272002
VecMAXPY              75 1.0 1.4729e-02 1.2 1.87e+07 1.0 0.0e+00 0.0e+00 0.0e+00  0  2  0  0  0   0  2  0  0  0 301121
VecScatterBegin      288 1.0 2.3417e-01 7.2 0.00e+00 0.0 2.6e+05 1.3e+03 3.0e+00  1  0 88 67  1   1  0 88 67  2     0
VecScatterEnd        288 1.0 8.5289e-0220.4 6.43e+04 3.3 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0   137
VecNormalize          65 1.0 9.6337e-02 1.1 4.21e+06 1.0 0.0e+00 0.0e+00 6.5e+01  1  0  0  0 31   1  0  0  0 34 10318
SFSetGraph             4 1.0 1.3663e-04 2.7 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
SFSetUp                4 1.0 2.0379e-0121.5 0.00e+00 0.0 7.9e+03 3.1e+02 3.0e+00  1  0  3  0  1   1  0  3  0  2     0
SFPack               288 1.0 1.4428e-03 2.6 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
SFUnpack             288 1.0 7.1886e-04 3.7 6.43e+04 3.3 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0 16206
KSPSetUp               3 1.0 9.9996e-01 1.0 8.38e+08 1.5 5.4e+04 1.6e+03 3.4e+01  7 63 18 17 16   7 63 18 17 18 163420
KSPSolve               1 1.0 9.5051e-01 1.0 4.18e+08 1.1 2.4e+05 1.6e+03 1.2e+02  6 37 80 77 58   6 37 80 77 64 99298
KSPGMRESOrthog        64 1.0 1.0244e-01 1.8 3.06e+07 1.0 0.0e+00 0.0e+00 6.4e+01  1  3  0  0 31   1  3  0  0 34 70620
PCSetUp                1 1.0 1.0895e+00 1.0 8.38e+08 1.5 5.4e+04 1.6e+03 4.2e+01  7 63 18 17 20   7 63 18 17 22 149986
PCSetUpOnBlocks       19 1.0 5.5868e-01 1.6 7.11e+08 1.6 0.0e+00 0.0e+00 0.0e+00  3 52  0  0  0   3 52  0  0  0 241017
PCApply                9 1.0 9.2004e-01 1.0 3.98e+08 1.1 2.3e+05 1.6e+03 1.0e+02  6 35 76 73 49   6 35 76 73 54 97488
PCApplyOnBlocks       47 1.0 5.9387e-01 1.1 4.45e+08 1.1 0.0e+00 0.0e+00 0.0e+00  4 39  0  0  0   4 39  0  0  0 167535
------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix    12             12     77902036     0.
              Vector    83             83     27689976     0.
           Index Set    15             15      1677424     0.
   IS L to G Mapping     1              1       681992     0.
   Star Forest Graph    10             10        11376     0.
       Krylov Solver     5              5        72264     0.
      Preconditioner     5              5         6024     0.
    Distributed Mesh     3              3        15144     0.
     Discrete System     3              3         2712     0.
           Weak Form     3              3         2472     0.
              Viewer     2              1          848     0.
========================================================================================================================
Average time to get PetscTime(): 2.89e-08
Average time for MPI_Barrier(): 6.26324e-05
Average time for zero size MPI_Send(): 1.35408e-06
#PETSc Option Table entries:
-ksp_converged_reason
-ksp_pc_side right
-ksp_rtol 1e-8
-ksp_type fgmres
-log_view
-m 1838
-mg_coarse_ksp_converged_reason
-mg_coarse_ksp_type gmres
-mg_coarse_pc_type hypre
-mg_levels_pc_asm_type basic
-mg_levels_pc_type asm
-mg_levels_sub_pc_type lu
-n 1838
-nw
-pc_mg_type multiplicative
-pc_type mg
#End of PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: --prefix=/home/a/asarkar/sudhipv/software/FreeFem-sources/install/ff-petsc/r MAKEFLAGS= --with-debugging=0 COPTFLAGS="-O3 -mtune=native" CXXOPTFLAGS="-O3 -mtune=native" FOPTFLAGS="-O3 -mtune=native" --with-cxx-dialect=C++11 --with-ssl=0 --with-x=0 --with-fortran-bindings=0 --with-cudac=0 --with-cc=/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Compiler/intel2020/openmpi/4.0.3/bin/mpicc --with-cxx=/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Compiler/intel2020/openmpi/4.0.3/bin/mpic++ --with-fc=/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Compiler/intel2020/openmpi/4.0.3/bin/mpif90 --with-scalar-type=real --with-blaslapack-include=/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/imkl/2020.1.217/mkl/include --with-blaslapack-lib="-Wl,-rpath,/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/imkl/2020.1.217/mkl/lib/intel64 -L/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/imkl/2020.1.217/mkl/lib/intel64 -lmkl_rt -lmkl_intel_thread -lmkl_core  -liomp5 -lpthread" --download-metis --download-ptscotch --download-hypre --download-parmetis --download-mmg --download-parmmg --download-superlu --download-suitesparse --download-tetgen --download-slepc --download-hpddm --download-scalapack --download-mumps --download-slepc-configure-arguments=--download-arpack=https://github.com/prj-/arpack-ng/archive/b64dccb.tar.gz PETSC_ARCH=fr
-----------------------------------------
Libraries compiled on 2021-05-02 15:39:49 on nia-login03.scinet.local 
Machine characteristics: Linux-3.10.0-957.27.2.el7.x86_64-x86_64-with-centos-7.6.1810-Core
Using PETSc directory: /home/a/asarkar/sudhipv/software/FreeFem-sources/install/ff-petsc/r
Using PETSc arch: 
-----------------------------------------

Using C compiler: /cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Compiler/intel2020/openmpi/4.0.3/bin/mpicc  -fPIC -wd1572 -O3 -mtune=native   
Using Fortran compiler: /cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Compiler/intel2020/openmpi/4.0.3/bin/mpif90  -fPIC -O3 -mtune=native     
-----------------------------------------

Using include paths: -I/home/a/asarkar/sudhipv/software/FreeFem-sources/install/ff-petsc/r/include -I/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/imkl/2020.1.217/mkl/include
-----------------------------------------

Using C linker: /cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Compiler/intel2020/openmpi/4.0.3/bin/mpicc
Using Fortran linker: /cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Compiler/intel2020/openmpi/4.0.3/bin/mpif90
Using libraries: -Wl,-rpath,/home/a/asarkar/sudhipv/software/FreeFem-sources/install/ff-petsc/r/lib -L/home/a/asarkar/sudhipv/software/FreeFem-sources/install/ff-petsc/r/lib -lpetsc -Wl,-rpath,/home/a/asarkar/sudhipv/software/FreeFem-sources/install/ff-petsc/r/lib -L/home/a/asarkar/sudhipv/software/FreeFem-sources/install/ff-petsc/r/lib -Wl,-rpath,/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/imkl/2020.1.217/mkl/lib/intel64 -L/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/imkl/2020.1.217/mkl/lib/intel64 -Wl,-rpath,/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Compiler/intel2020/openmpi/4.0.3/lib -L/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Compiler/intel2020/openmpi/4.0.3/lib -Wl,-rpath,/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/libfabric/1.10.1/lib -L/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/libfabric/1.10.1/lib -Wl,-rpath,/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/ucx/1.8.0/lib -L/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/ucx/1.8.0/lib -Wl,-rpath,/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/imkl/2020.1.217/lib/intel64 -L/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/imkl/2020.1.217/lib/intel64 -Wl,-rpath,/cvmfs/restricted.computecanada.ca/easybuild/software/2020/Core/intel/2020.1.217/compilers_and_libraries_2020.1.217/linux/tbb/lib/intel64/gcc4.8 -L/cvmfs/restricted.computecanada.ca/easybuild/software/2020/Core/intel/2020.1.217/compilers_and_libraries_2020.1.217/linux/tbb/lib/intel64/gcc4.8 -Wl,-rpath,/cvmfs/restricted.computecanada.ca/easybuild/software/2020/Core/intel/2020.1.217/compilers_and_libraries_2020.1.217/linux/compiler/lib/intel64_lin -L/cvmfs/restricted.computecanada.ca/easybuild/software/2020/Core/intel/2020.1.217/compilers_and_libraries_2020.1.217/linux/compiler/lib/intel64_lin -Wl,-rpath,/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Compiler/intel2020/openmpi/4.0.3/lib64 -L/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Compiler/intel2020/openmpi/4.0.3/lib64 -Wl,-rpath,/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/gcccore/9.3.0/lib/gcc/x86_64-pc-linux-gnu/9.3.0 -L/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/gcccore/9.3.0/lib/gcc/x86_64-pc-linux-gnu/9.3.0 -Wl,-rpath,/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/gcccore/9.3.0/lib64 -L/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/gcccore/9.3.0/lib64 -Wl,-rpath,/cvmfs/soft.computecanada.ca/gentoo/2020/lib64 -L/cvmfs/soft.computecanada.ca/gentoo/2020/lib64 -Wl,-rpath,/cvmfs/soft.computecanada.ca/gentoo/2020/usr/lib64 -L/cvmfs/soft.computecanada.ca/gentoo/2020/usr/lib64 -Wl,-rpath,/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/gcccore/9.3.0/lib -L/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/gcccore/9.3.0/lib -Wl,-rpath,/cvmfs/soft.computecanada.ca/gentoo/2020/lib -L/cvmfs/soft.computecanada.ca/gentoo/2020/lib -Wl,-rpath,/cvmfs/soft.computecanada.ca/gentoo/2020/usr/lib -L/cvmfs/soft.computecanada.ca/gentoo/2020/usr/lib -lHYPRE -lcmumps -ldmumps -lsmumps -lzmumps -lmumps_common -lpord -lscalapack -lumfpack -lklu -lcholmod -lbtf -lccolamd -lcolamd -lcamd -lamd -lsuitesparseconfig -lsuperlu -lmkl_rt -lmkl_intel_thread -lmkl_core -liomp5 -lpthread -lparmmg -lmmg -lmmg3d -lptesmumps -lptscotchparmetis -lptscotch -lptscotcherr -lesmumps -lscotch -lscotcherr -lpthread -lparmetis -lmetis -ltet -lstdc++ -ldl -lmpi_usempif08 -lmpi_usempi_ignore_tkr -lmpi_mpifh -lmpi -lifport -lifcoremt_pic -limf -lsvml -lm -lipgo -lirc -lpthread -lgcc_s -lirc_s -lrt -lquadmath -lstdc++ -ldl
-----------------------------------------

#############################################################################################################
#############################################################################################################
########     Finsihed Code with NL 240 Processors             #################################################
#############################################################################################################
#############################################################################################################
Number of Vertices for coarse mesh in process  0  5635876
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 5
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 5
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 5
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 5
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 5
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 5
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 5
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 5
    Linear mg_coarse_ solve converged due to CONVERGED_RTOL iterations 5
Linear solve converged due to CONVERGED_RTOL iterations 9
time taken for solve is21.9713
L2 Norm of uerr is1.87359e-08
************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

/home/a/asarkar/sudhipv/software/FreeFem-sources/src/mpi/FreeFem++-mpi on a  named nia0074.scinet.local with 400 processors, by sudhipv Thu Oct  7 09:59:40 2021
Using Petsc Release Version 3.15.0, Mar 30, 2021 

                         Max       Max/Min     Avg       Total
Time (sec):           2.207e+01     1.000   2.207e+01
Objects:              1.420e+02     1.000   1.420e+02
Flop:                 1.207e+09     1.252   1.069e+09  4.276e+11
Flop/sec:             5.470e+07     1.252   4.845e+07  1.938e+10
MPI Messages:         1.824e+03     4.000   1.273e+03  5.094e+05
MPI Message Lengths:  2.628e+06     2.980   1.691e+03  8.613e+08
MPI Reductions:       2.070e+02     1.000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flop
                            and VecAXPY() for complex vectors of length N --> 8N flop

Summary of Stages:   ----- Time ------  ----- Flop ------  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total    Count   %Total     Avg         %Total    Count   %Total
 0:      Main Stage: 2.2065e+01 100.0%  4.2760e+11 100.0%  5.094e+05 100.0%  1.691e+03      100.0%  1.890e+02  91.3%

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flop: Max - maximum over all processors
                  Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   AvgLen: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flop in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flop over all processors)/(max time over all processors)
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flop                              --- Global ---  --- Stage ----  Total
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   AvgLen  Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s
------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

BuildTwoSided          6 1.0 1.8198e-0115.0 0.00e+00 0.0 6.7e+03 4.0e+00 5.0e+00  0  0  1  0  2   0  0  1  0  3     0
BuildTwoSidedF         2 1.0 3.8318e-03 3.8 0.00e+00 0.0 0.0e+00 0.0e+00 2.0e+00  0  0  0  0  1   0  0  0  0  1     0
MatMult              109 1.0 1.2695e-01 1.5 4.92e+07 1.0 2.5e+05 1.2e+03 2.0e+00  0  5 50 35  1   0  5 50 35  1 152841
MatMultAdd             9 1.0 1.9520e-02 1.5 1.03e+06 1.0 2.0e+04 2.2e+03 0.0e+00  0  0  4  5  0   0  0  4  5  0 20780
MatMultTranspose       9 1.0 6.4993e-02 5.7 0.00e+00 0.0 2.0e+04 7.6e+03 0.0e+00  0  0  4 18  0   0  0  4 18  0     0
MatSolve              47 1.0 6.0084e-01 1.2 4.36e+08 1.1 0.0e+00 0.0e+00 0.0e+00  2 39  0  0  0   2 39  0  0  0 275564
MatLUFactorSym         1 1.0 1.1700e-01 1.6 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatLUFactorNum         1 1.0 4.5578e-01 1.6 6.80e+08 1.5 0.0e+00 0.0e+00 0.0e+00  2 52  0  0  0   2 52  0  0  0 488405
MatConvert             1 1.0 8.3889e-03 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 4.0e+00  0  0  0  0  2   0  0  0  0  2     0
MatResidual            9 1.0 2.0405e-02 2.4 7.19e+06 1.0 2.0e+04 1.5e+03 0.0e+00  0  1  4  4  0   0  1  4  4  0 139115
MatAssemblyBegin       6 1.0 4.2168e-03 1.3 0.00e+00 0.0 0.0e+00 0.0e+00 2.0e+00  0  0  0  0  1   0  0  0  0  1     0
MatAssemblyEnd         6 1.0 1.9336e-01 2.2 0.00e+00 0.0 0.0e+00 0.0e+00 1.2e+01  1  0  0  0  6   1  0  0  0  6     0
MatGetRowIJ            3 1.0 4.6050e-03 2.3 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatCreateSubMats       1 1.0 3.2172e-02 1.5 0.00e+00 0.0 1.1e+04 3.5e+03 1.0e+00  0  0  2  5  0   0  0  2  5  1     0
MatGetOrdering         1 1.0 4.2899e-02 1.8 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatIncreaseOvrlp       1 1.0 1.2667e-02 1.5 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  1     0
VecMDot               64 1.0 1.2817e-01 2.5 1.53e+07 1.0 0.0e+00 0.0e+00 6.4e+01  0  1  0  0 31   0  1  0  0 34 47035
VecNorm               75 1.0 8.0978e-02 1.2 3.95e+06 1.0 0.0e+00 0.0e+00 7.5e+01  0  0  0  0 36   0  0  0  0 40 19204
VecScale              93 1.0 1.1321e-03 2.2 1.98e+06 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0 686830
VecCopy               38 1.0 2.8650e-03 1.5 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecSet               241 1.0 1.9235e-02 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecAXPY               20 1.0 2.8801e-03 2.1 1.51e+06 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0 207351
VecAYPX               54 1.0 8.1706e-03 1.4 4.11e+06 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0 198572
VecAXPBYCZ            18 1.0 4.5029e-03 2.3 5.13e+06 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0 450386
VecMAXPY              75 1.0 1.5286e-02 1.3 1.88e+07 1.0 0.0e+00 0.0e+00 0.0e+00  0  2  0  0  0   0  2  0  0  0 483587
VecScatterBegin      288 1.0 2.1262e-01 5.1 0.00e+00 0.0 4.5e+05 1.3e+03 3.0e+00  1  0 88 67  1   1  0 88 67  2     0
VecScatterEnd        288 1.0 1.1697e-0133.0 6.03e+04 2.9 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0   169
VecNormalize          65 1.0 7.9002e-02 1.2 4.22e+06 1.0 0.0e+00 0.0e+00 6.5e+01  0  0  0  0 31   0  0  0  0 34 20970
SFSetGraph             4 1.0 1.2930e-04 2.5 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
SFSetUp                4 1.0 1.8252e-0112.8 0.00e+00 0.0 1.3e+04 3.1e+02 3.0e+00  0  0  3  0  1   0  0  3  0  2     0
SFPack               288 1.0 1.5789e-03 3.8 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
SFUnpack             288 1.0 7.6489e-04 4.3 6.03e+04 2.9 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0 25914
KSPSetUp               3 1.0 9.8559e-01 1.0 8.03e+08 1.4 9.2e+04 1.6e+03 3.4e+01  4 63 18 17 16   4 63 18 17 18 274442
KSPSolve               1 1.0 1.0879e+00 1.0 4.11e+08 1.1 4.1e+05 1.6e+03 1.2e+02  5 37 80 77 58   5 37 80 77 64 144414
KSPGMRESOrthog        64 1.0 1.3901e-01 2.2 3.06e+07 1.0 0.0e+00 0.0e+00 6.4e+01  0  3  0  0 31   0  3  0  0 34 86734
PCSetUp                1 1.0 1.0870e+00 1.0 8.03e+08 1.4 9.2e+04 1.6e+03 4.2e+01  5 63 18 17 20   5 63 18 17 22 248828
PCSetUpOnBlocks       19 1.0 5.7098e-01 1.4 6.80e+08 1.5 0.0e+00 0.0e+00 0.0e+00  2 52  0  0  0   2 52  0  0  0 389866
PCApply                9 1.0 1.0506e+00 1.0 3.91e+08 1.1 3.9e+05 1.6e+03 1.0e+02  5 35 76 73 49   5 35 76 73 54 142110
PCApplyOnBlocks       47 1.0 6.0658e-01 1.2 4.36e+08 1.1 0.0e+00 0.0e+00 0.0e+00  3 39  0  0  0   3 39  0  0  0 272960
------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix    12             12     76537760     0.
              Vector    83             83     27714480     0.
           Index Set    15             15      1677380     0.
   IS L to G Mapping     1              1       682160     0.
   Star Forest Graph    10             10        11376     0.
       Krylov Solver     5              5        72264     0.
      Preconditioner     5              5         6024     0.
    Distributed Mesh     3              3        15144     0.
     Discrete System     3              3         2712     0.
           Weak Form     3              3         2472     0.
              Viewer     2              1          848     0.
========================================================================================================================
Average time to get PetscTime(): 3.25e-08
Average time for MPI_Barrier(): 7.73154e-05
Average time for zero size MPI_Send(): 1.37353e-06
#PETSc Option Table entries:
-ksp_converged_reason
-ksp_pc_side right
-ksp_rtol 1e-8
-ksp_type fgmres
-log_view
-m 2373
-mg_coarse_ksp_converged_reason
-mg_coarse_ksp_type gmres
-mg_coarse_pc_type hypre
-mg_levels_pc_asm_type basic
-mg_levels_pc_type asm
-mg_levels_sub_pc_type lu
-n 2373
-nw
-pc_mg_type multiplicative
-pc_type mg
#End of PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: --prefix=/home/a/asarkar/sudhipv/software/FreeFem-sources/install/ff-petsc/r MAKEFLAGS= --with-debugging=0 COPTFLAGS="-O3 -mtune=native" CXXOPTFLAGS="-O3 -mtune=native" FOPTFLAGS="-O3 -mtune=native" --with-cxx-dialect=C++11 --with-ssl=0 --with-x=0 --with-fortran-bindings=0 --with-cudac=0 --with-cc=/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Compiler/intel2020/openmpi/4.0.3/bin/mpicc --with-cxx=/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Compiler/intel2020/openmpi/4.0.3/bin/mpic++ --with-fc=/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Compiler/intel2020/openmpi/4.0.3/bin/mpif90 --with-scalar-type=real --with-blaslapack-include=/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/imkl/2020.1.217/mkl/include --with-blaslapack-lib="-Wl,-rpath,/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/imkl/2020.1.217/mkl/lib/intel64 -L/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/imkl/2020.1.217/mkl/lib/intel64 -lmkl_rt -lmkl_intel_thread -lmkl_core  -liomp5 -lpthread" --download-metis --download-ptscotch --download-hypre --download-parmetis --download-mmg --download-parmmg --download-superlu --download-suitesparse --download-tetgen --download-slepc --download-hpddm --download-scalapack --download-mumps --download-slepc-configure-arguments=--download-arpack=https://github.com/prj-/arpack-ng/archive/b64dccb.tar.gz PETSC_ARCH=fr
-----------------------------------------
Libraries compiled on 2021-05-02 15:39:49 on nia-login03.scinet.local 
Machine characteristics: Linux-3.10.0-957.27.2.el7.x86_64-x86_64-with-centos-7.6.1810-Core
Using PETSc directory: /home/a/asarkar/sudhipv/software/FreeFem-sources/install/ff-petsc/r
Using PETSc arch: 
-----------------------------------------

Using C compiler: /cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Compiler/intel2020/openmpi/4.0.3/bin/mpicc  -fPIC -wd1572 -O3 -mtune=native   
Using Fortran compiler: /cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Compiler/intel2020/openmpi/4.0.3/bin/mpif90  -fPIC -O3 -mtune=native     
-----------------------------------------

Using include paths: -I/home/a/asarkar/sudhipv/software/FreeFem-sources/install/ff-petsc/r/include -I/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/imkl/2020.1.217/mkl/include
-----------------------------------------

Using C linker: /cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Compiler/intel2020/openmpi/4.0.3/bin/mpicc
Using Fortran linker: /cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Compiler/intel2020/openmpi/4.0.3/bin/mpif90
Using libraries: -Wl,-rpath,/home/a/asarkar/sudhipv/software/FreeFem-sources/install/ff-petsc/r/lib -L/home/a/asarkar/sudhipv/software/FreeFem-sources/install/ff-petsc/r/lib -lpetsc -Wl,-rpath,/home/a/asarkar/sudhipv/software/FreeFem-sources/install/ff-petsc/r/lib -L/home/a/asarkar/sudhipv/software/FreeFem-sources/install/ff-petsc/r/lib -Wl,-rpath,/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/imkl/2020.1.217/mkl/lib/intel64 -L/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/imkl/2020.1.217/mkl/lib/intel64 -Wl,-rpath,/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Compiler/intel2020/openmpi/4.0.3/lib -L/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Compiler/intel2020/openmpi/4.0.3/lib -Wl,-rpath,/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/libfabric/1.10.1/lib -L/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/libfabric/1.10.1/lib -Wl,-rpath,/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/ucx/1.8.0/lib -L/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/ucx/1.8.0/lib -Wl,-rpath,/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/imkl/2020.1.217/lib/intel64 -L/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/imkl/2020.1.217/lib/intel64 -Wl,-rpath,/cvmfs/restricted.computecanada.ca/easybuild/software/2020/Core/intel/2020.1.217/compilers_and_libraries_2020.1.217/linux/tbb/lib/intel64/gcc4.8 -L/cvmfs/restricted.computecanada.ca/easybuild/software/2020/Core/intel/2020.1.217/compilers_and_libraries_2020.1.217/linux/tbb/lib/intel64/gcc4.8 -Wl,-rpath,/cvmfs/restricted.computecanada.ca/easybuild/software/2020/Core/intel/2020.1.217/compilers_and_libraries_2020.1.217/linux/compiler/lib/intel64_lin -L/cvmfs/restricted.computecanada.ca/easybuild/software/2020/Core/intel/2020.1.217/compilers_and_libraries_2020.1.217/linux/compiler/lib/intel64_lin -Wl,-rpath,/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Compiler/intel2020/openmpi/4.0.3/lib64 -L/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Compiler/intel2020/openmpi/4.0.3/lib64 -Wl,-rpath,/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/gcccore/9.3.0/lib/gcc/x86_64-pc-linux-gnu/9.3.0 -L/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/gcccore/9.3.0/lib/gcc/x86_64-pc-linux-gnu/9.3.0 -Wl,-rpath,/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/gcccore/9.3.0/lib64 -L/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/gcccore/9.3.0/lib64 -Wl,-rpath,/cvmfs/soft.computecanada.ca/gentoo/2020/lib64 -L/cvmfs/soft.computecanada.ca/gentoo/2020/lib64 -Wl,-rpath,/cvmfs/soft.computecanada.ca/gentoo/2020/usr/lib64 -L/cvmfs/soft.computecanada.ca/gentoo/2020/usr/lib64 -Wl,-rpath,/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/gcccore/9.3.0/lib -L/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/gcccore/9.3.0/lib -Wl,-rpath,/cvmfs/soft.computecanada.ca/gentoo/2020/lib -L/cvmfs/soft.computecanada.ca/gentoo/2020/lib -Wl,-rpath,/cvmfs/soft.computecanada.ca/gentoo/2020/usr/lib -L/cvmfs/soft.computecanada.ca/gentoo/2020/usr/lib -lHYPRE -lcmumps -ldmumps -lsmumps -lzmumps -lmumps_common -lpord -lscalapack -lumfpack -lklu -lcholmod -lbtf -lccolamd -lcolamd -lcamd -lamd -lsuitesparseconfig -lsuperlu -lmkl_rt -lmkl_intel_thread -lmkl_core -liomp5 -lpthread -lparmmg -lmmg -lmmg3d -lptesmumps -lptscotchparmetis -lptscotch -lptscotcherr -lesmumps -lscotch -lscotcherr -lpthread -lparmetis -lmetis -ltet -lstdc++ -ldl -lmpi_usempif08 -lmpi_usempi_ignore_tkr -lmpi_mpifh -lmpi -lifport -lifcoremt_pic -limf -lsvml -lm -lipgo -lirc -lpthread -lgcc_s -lirc_s -lrt -lquadmath -lstdc++ -ldl
-----------------------------------------

#############################################################################################################
#############################################################################################################
########     Finsihed Code with NL 400 Processors             #################################################
#############################################################################################################
#############################################################################################################

scontrol show jobid 6207467
JobId=6207467 JobName=2L_V3
   UserId=sudhipv(3061414) GroupId=asarkar(6001189) MCS_label=N/A
   Priority=2358041 Nice=0 Account=def-asarkar QOS=normal
   JobState=COMPLETED Reason=None Dependency=(null)
   Requeue=0 Restarts=0 BatchFlag=1 Reboot=0 ExitCode=0:0
   RunTime=00:01:40 TimeLimit=00:30:00 TimeMin=N/A
   SubmitTime=2021-10-07T09:58:00 EligibleTime=2021-10-07T09:58:00
   AccrueTime=2021-10-07T09:58:00
   StartTime=2021-10-07T09:58:03 EndTime=2021-10-07T09:59:43 Deadline=N/A
   SuspendTime=None SecsPreSuspend=0 LastSchedEval=2021-10-07T09:58:03
   Partition=compute AllocNode:Sid=nia-login01:116089
   ReqNodeList=(null) ExcNodeList=(null)
   NodeList=nia[0074,0081,0111,0122-0123,0129,0141,0153,0193-0194]
   BatchHost=nia0074
   NumNodes=10 NumCPUs=800 NumTasks=400 CPUs/Task=1 ReqB:S:C:T=0:0:*:*
   TRES=cpu=800,mem=1750000M,node=10,billing=400
   Socks/Node=* NtasksPerN:B:S:C=40:0:*:* CoreSpec=*
   MinCPUsNode=40 MinMemoryNode=175000M MinTmpDiskNode=0
   Features=[skylake|cascade] DelayBoot=00:00:00
   OverSubscribe=NO Contiguous=0 Licenses=(null) Network=(null)
   Command=/gpfs/fs0/scratch/a/asarkar/sudhipv/poisson_hpc/weak/runff_niagara_weak.sh
   WorkDir=/gpfs/fs0/scratch/a/asarkar/sudhipv/poisson_hpc/weak
   StdErr=/gpfs/fs0/scratch/a/asarkar/sudhipv/poisson_hpc/weak/2L_V3-6207467.out
   StdIn=/dev/null
   StdOut=/gpfs/fs0/scratch/a/asarkar/sudhipv/poisson_hpc/weak/2L_V3-6207467.out
   Power=
   MailUser=sudhipv@cmail.carleton.ca MailType=BEGIN,END,FAIL,REQUEUE,STAGE_OUT

sacct -j 6207467
       JobID    JobName    Account    Elapsed  MaxVMSize     MaxRSS  SystemCPU    UserCPU ExitCode 
------------ ---------- ---------- ---------- ---------- ---------- ---------- ---------- -------- 
6207467           2L_V3 def-asark+   00:01:40                        13:45.912   04:10:11      0:0 
6207467.bat+      batch def-asark+   00:01:40  71568664K  52378956K  02:44.400  41:34.118      0:0 
6207467.ext+     extern def-asark+   00:01:40    138360K      1068K  00:00.001  00:00.006      0:0 
6207467.0         orted def-asark+   00:00:17    395276K      2996K  00:01.375  00:01.121      0:0 
6207467.1         orted def-asark+   00:00:11    395276K      2996K  00:28.438  04:44.309      0:0 
6207467.2         orted def-asark+   00:00:14    395276K      2992K  01:30.086  22:16.803      0:0 
6207467.3         orted def-asark+   00:00:18    395276K      2992K  02:42.657  49:12.170      0:0 
6207467.4         orted def-asark+   00:00:28    395276K      2992K  06:18.951   02:12:23      0:0 
